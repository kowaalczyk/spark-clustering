# install hadoop and spark
- hosts: all
  become: yes
  vars_files:
    - variables.yml
  vars:
    master_host: "{{ groups['master'][0] }}"
    slaves: "{{ groups['slaves'] }}"
  environment:
    JAVA_HOME: "{{ paths.java }}"
    HADOOP_HOME: "{{ paths.hadoop }}"
    HADOOP_PREFIX: "{{ paths.hadoop }}"
    HADOOP_INSTALL: "{{ paths.hadoop }}"
    HADOOP_MAPRED_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_HOME: "{{ paths.hadoop }}"
    HADOOP_HDFS_HOME: "{{ paths.hadoop }}"
    YARN_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_LIB_NATIVE_DIR: "{{ paths.hadoop }}/lib/native"
    HADOOP_OPTS: "-Djava.library.path={{ paths.hadoop }}/lib/native"
    PATH: "{{ ansible_env.PATH }}:{{ paths.hadoop }}/sbin:{{ paths.hadoop }}/bin"
  tasks:
    - name: Install ssh server
      apt:
        name: openssh-server
        state: present
        update_cache: yes
    - name: Copy ssh config
      copy:
        src: "all/ssh/config"
        dest: "~/.ssh/config"
    - name: Install java
      apt:
        name: openjdk-8-jdk
        state: present
        update_cache: yes
    - name: Download & extract Hadoop
      unarchive:
        remote_src: yes
        src: "{{ apache_download_proxy }}/hadoop/common/{{ hadoop_version }}/{{ hadoop_version }}.tar.gz"
        dest: "{{ prefix }}"
        creates: "{{ paths.hadoop }}"
    - name: Create directories for NameNode and DataNode
      file:
        path: "{{ paths.hdfs }}/{{ item }}"
        state: directory
        mode: "0755"
      with_items:
        - "namenode"
        - "datanode"
    - name: Create hadoop config files
      template:
        src: "all/hadoop/{{ item }}.j2"
        dest: "{{ paths.hadoop }}/etc/hadoop/{{ item }}"
      with_items:
        - "hadoop-env.sh"
        - "core-site.xml"
        - "hdfs-site.xml"
        - "mapred-site.xml"
        - "yarn-site.xml"
        - "slaves"
    - name: Download & extract Spark
      unarchive:
        remote_src: yes
        src: "{{ apache_download_proxy }}/spark/{{ spark_version }}/{{ spark_version }}-bin-without-hadoop.tgz"
        dest: "{{ prefix }}"
        creates: "{{ paths.spark }}"
        

# generate ssh key for hadoop
- hosts: master
  become: yes
  vars_files:
    - variables.yml
  tasks:
    - name: Generate ssh key pair
      openssh_keypair:
        path: "~/.ssh/id_rsa"
      register: ssh_key

# configure access between nodes
- hosts: all
  become: yes
  vars:
    master_host: "{{ groups['master'][0] }}"
  vars_files:
    - variables.yml
  tasks:
    - name: Add master key to authorized keys
      authorized_key:
        user: root
        key: "{{ hostvars[master_host]['ssh_key']['public_key'] }}"
    - name: Enable firewall
      ufw:
        state: enabled
    - name: Open ports in firewall
      ufw:
        rule: allow
        port: "{{ item }}"
        proto: tcp
        state: reloaded
      with_items:
        - '22'
        - '9000'
        - '50070'
    - name: Restart ssh server with new config
      service:
        name: ssh
        state: restarted

# start hdfs
- hosts: master
  become: yes
  vars_files:
    - variables.yml
  environment:
    JAVA_HOME: "{{ paths.java }}"
    HADOOP_HOME: "{{ paths.hadoop }}"
    HADOOP_PREFIX: "{{ paths.hadoop }}"
    HADOOP_INSTALL: "{{ paths.hadoop }}"
    HADOOP_MAPRED_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_HOME: "{{ paths.hadoop }}"
    HADOOP_HDFS_HOME: "{{ paths.hadoop }}"
    YARN_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_LIB_NATIVE_DIR: "{{ paths.hadoop }}/lib/native"
    HADOOP_OPTS: "-Djava.library.path={{ paths.hadoop }}/lib/native"
    PATH: "{{ ansible_env.PATH }}:{{ paths.hadoop }}/sbin:{{ paths.hadoop }}/bin"
  tasks:
    - name: Debug printenv
      command: printenv
      register: printenv
    - debug:
        var: printenv
    - name: Format HDFS Namenode
      command: "echo 'Y' | hdfs namenode -format"
    - name: Start DFS
      command: start-dfs.sh
      register: dfs_log
    - debug:
        var: dfs_log.stdout_lines
    - name: Start Yarn
      command: start-yarn.sh
      register: yarn_log
    - debug:
        var: yarn_log.stdout_lines
    - name: Generate DFS Admin report
      command: "sleep 30; hdfs dfsadmin -report"
      register: dfsadmin_log
    - debug:
        var: dfsadmin_log.stdout_lines
