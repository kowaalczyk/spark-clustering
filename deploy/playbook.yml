# install hadoop and spark
- hosts: all
  become: yes
  vars_files:
    - variables.yml
  vars:
    master_host: "{{ groups['master'][0] }}"
    slaves: "{{ groups['slaves'] }}"
  environment:
    JAVA_HOME: "{{ paths.java }}"
    HADOOP_HOME: "{{ paths.hadoop }}"
    HADOOP_PREFIX: "{{ paths.hadoop }}"
    HADOOP_INSTALL: "{{ paths.hadoop }}"
    HADOOP_MAPRED_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_HOME: "{{ paths.hadoop }}"
    HADOOP_HDFS_HOME: "{{ paths.hadoop }}"
    YARN_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_LIB_NATIVE_DIR: "{{ paths.hadoop }}/lib/native"
    HADOOP_OPTS: "-Djava.library.path={{ paths.hadoop }}/lib/native"
    PATH: "{{ ansible_env.PATH }}:{{ paths.hadoop }}/sbin:{{ paths.hadoop }}/bin"
  tasks:
    - name: Install ssh server
      apt:
        name: openssh-server
        state: present
        update_cache: yes
    - name: Copy ssh config
      copy:
        src: "all/ssh/config"
        dest: "~/.ssh/config"
    - name: Install java
      apt:
        name: openjdk-8-jdk
        state: present
        update_cache: yes
    - name: Download & extract Hadoop
      unarchive:
        remote_src: yes
        src: "{{ apache_download_proxy }}/hadoop/common/{{ hadoop_version }}/{{ hadoop_version }}.tar.gz"
        dest: "{{ prefix }}"
        creates: "{{ paths.hadoop }}"
    - name: Create directories for NameNode and DataNode
      file:
        path: "{{ paths.hdfs }}/{{ item }}"
        state: directory
        mode: "0755"
      with_items:
        - "namenode"
        - "datanode"
    - name: Create hadoop config files
      template:
        src: "all/hadoop/{{ item }}.j2"
        dest: "{{ paths.hadoop }}/etc/hadoop/{{ item }}"
      with_items:
        - "hadoop-env.sh"
        - "core-site.xml"
        - "hdfs-site.xml"
        - "mapred-site.xml"
        - "yarn-site.xml"
        - "slaves"

# generate ssh key for hadoop
- hosts: master
  become: yes
  vars_files:
    - variables.yml
  tasks:
    - name: Generate ssh key pair
      openssh_keypair:
        path: "~/.ssh/id_rsa"
      register: ssh_key

# configure access between nodes
- hosts: all
  become: yes
  vars:
    master_host: "{{ groups['master'][0] }}"
  vars_files:
    - variables.yml
  tasks:
    - name: Add master key to authorized keys
      authorized_key:
        user: root
        key: "{{ hostvars[master_host]['ssh_key']['public_key'] }}"
    - name: Enable firewall with ssh
      ufw:
        rule: allow
        port: ssh
        state: enabled
    - name: Restart ssh server with new config
      service:
        name: ssh
        state: restarted
- hosts: master
  become: yes
  vars:
    master_host: "{{ groups['master'][0] }}"
    slave_hosts: "{{ groups['slaves'] }}"
  tasks:
    - name: Open GUI ports to the public
      ufw:
        rule: allow
        port: "{{ item }}"
        proto: tcp
        state: reloaded
      with_items:
        - "8088"
        - "50070"
        - "50090"
    - name: Open all ports to slaves and master
      ufw:
        rule: allow
        src: "{{ item }}"
        state: reloaded
      with_items: "{{ slave_hosts + [master_host] }}"
    - name: Debug ufw status
      command: "ufw status verbose"
      register: ufw_status
    - debug:
        var: ufw_status
- hosts: slaves
  become: yes
  vars:
    master_host: "{{ groups['master'][0] }}"
    slave_hosts: "{{ groups['slaves'] }}"
  tasks:
    - name: Open GUI ports to the public
      ufw:
        rule: allow
        port: "{{ item }}"
        proto: tcp
        state: reloaded
      with_items:
        - "50075"
    - name: Open all ports to slaves and master
      ufw:
        rule: allow
        src: "{{ item }}"
        state: reloaded
      with_items: "{{ slave_hosts + [master_host] }}"
# start hdfs
- hosts: master
  become: yes
  vars_files:
    - variables.yml
  environment:
    JAVA_HOME: "{{ paths.java }}"
    HADOOP_HOME: "{{ paths.hadoop }}"
    HADOOP_PREFIX: "{{ paths.hadoop }}"
    HADOOP_INSTALL: "{{ paths.hadoop }}"
    HADOOP_MAPRED_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_HOME: "{{ paths.hadoop }}"
    HADOOP_HDFS_HOME: "{{ paths.hadoop }}"
    YARN_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_LIB_NATIVE_DIR: "{{ paths.hadoop }}/lib/native"
    HADOOP_OPTS: "-Djava.library.path={{ paths.hadoop }}/lib/native"
    PATH: "{{ ansible_env.PATH }}:{{ paths.hadoop }}/sbin:{{ paths.hadoop }}/bin"
  tasks:
    - name: Debug printenv
      command: printenv
      register: printenv
    - debug:
        var: printenv
    - name: Start HDFS if not running
      block:
        - name: Generate DFS Admin report
          command: "hdfs dfsadmin -report"
          register: dfsadmin_log
        - debug:
            var: dfsadmin_log.stdout_lines
      rescue:
        - name: Clear tmp
          command:
            cmd: "rm -rf /tmp/*"
            removes: "/tmp/*"
        - name: Format HDFS Namenode
          command: "hdfs namenode -format"
        - name: Start DFS
          command: start-dfs.sh
          register: dfs_log
        - debug:
            var: dfs_log.stdout_lines
        - name: Start Yarn
          command: start-yarn.sh
          register: yarn_log
        - debug:
            var: yarn_log.stdout_lines
        - pause:
            seconds: 30
        - name: Generate DFS Admin report
          command: "hdfs dfsadmin -report"
          register: dfsadmin_log
        - debug:
            var: dfsadmin_log.stdout_lines

# start spark
- hosts: master
  become: yes
  vars_files:
    - variables.yml
  environment:
    JAVA_HOME: "{{ paths.java }}"
    HADOOP_HOME: "{{ paths.hadoop }}"
    HADOOP_PREFIX: "{{ paths.hadoop }}"
    HADOOP_INSTALL: "{{ paths.hadoop }}"
    HADOOP_MAPRED_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_HOME: "{{ paths.hadoop }}"
    HADOOP_HDFS_HOME: "{{ paths.hadoop }}"
    YARN_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_LIB_NATIVE_DIR: "{{ paths.hadoop }}/lib/native"
    HADOOP_OPTS: "-Djava.library.path={{ paths.hadoop }}/lib/native"
    PATH: "{{ paths.spark }}/bin:{{ ansible_env.PATH }}:{{ paths.hadoop }}/sbin:{{ paths.hadoop }}/bin"
    YARN_CONF_DIR: "{{ paths.hadoop }}/etc/hadoop"
    SPARK_HOME: "{{ paths.spark }}"
  tasks:
    - name: Debug printenv
      command: printenv
      register: printenv
    - debug:
        var: printenv.stdout_lines
    - name: Download & extract Spark
      unarchive:
        remote_src: yes
        src: "{{ apache_download_proxy }}/spark/{{ spark_version }}/{{ spark_version }}-bin-without-hadoop.tgz"
        dest: "{{ prefix }}"
        creates: "{{ paths.spark }}"
    - name: Get hadoop classpath
      command: "hdfs classpath"
      register: hdfs_classpath
    # TODO: Deploy and run the application instead
    - name: Run spark example
      environment:
        SPARK_DIST_CLASSPATH: "{{ hdfs_classpath.stdout }}"
      script: "master/run-spark-example.sh"
      register: example
    - debug:
        var: example.stdout_lines
