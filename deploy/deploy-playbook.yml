# Sets up hadoop, spark running on yarn, and python configured for pyspark.

# 1. install hadoop
- hosts: all
  become: yes
  vars_files:
    - variables.yml
  vars:
    master_host: "{{ groups['master'][0] }}"
    slaves: "{{ groups['slaves'] }}"
  environment:
    JAVA_HOME: "{{ paths.java }}"
    HADOOP_HOME: "{{ paths.hadoop }}"
    HADOOP_PREFIX: "{{ paths.hadoop }}"
    HADOOP_INSTALL: "{{ paths.hadoop }}"
    HADOOP_MAPRED_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_HOME: "{{ paths.hadoop }}"
    HADOOP_HDFS_HOME: "{{ paths.hadoop }}"
    YARN_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_LIB_NATIVE_DIR: "{{ paths.hadoop }}/lib/native"
    HADOOP_OPTS: "-Djava.library.path={{ paths.hadoop }}/lib/native"
    PATH: "{{ ansible_env.PATH }}:{{ paths.hadoop }}/sbin:{{ paths.hadoop }}/bin"
  tasks:
    - name: Set hostname to prevent yarn nodemanager connection errors
      hostname:
        name: "{{ inventory_hostname }}"
    - name: Install ssh server
      apt:
        name: openssh-server
        state: present
        update_cache: yes
    - name: Copy ssh config
      copy:
        src: "all/ssh/config"
        dest: "~/.ssh/config"
    - name: Install java
      apt:
        name: openjdk-8-jdk
        state: present
        update_cache: yes
    - name: Download & extract Hadoop
      unarchive:
        remote_src: yes
        src: "{{ apache_download_proxy }}/hadoop/common/{{ hadoop_version }}/{{ hadoop_version }}.tar.gz"
        dest: "{{ prefix }}"
        creates: "{{ paths.hadoop }}"
    - name: Create directories for NameNode and DataNode
      file:
        path: "{{ paths.hdfs }}/{{ item }}"
        state: directory
        mode: "0755"
      with_items:
        - "namenode"
        - "datanode"
    - name: Create hadoop config files
      template:
        src: "all/hadoop/{{ item }}.j2"
        dest: "{{ paths.hadoop }}/etc/hadoop/{{ item }}"
      with_items:
        - "hadoop-env.sh"
        - "core-site.xml"
        - "hdfs-site.xml"
        - "mapred-site.xml"
        - "yarn-site.xml"
        - "slaves"

# generate ssh key for hadoop
- hosts: master
  become: yes
  vars_files:
    - variables.yml
  tasks:
    - name: Generate ssh key pair
      openssh_keypair:
        path: "~/.ssh/id_rsa"
      register: ssh_key

# 2. configure access between nodes
- hosts: all
  become: yes
  vars:
    master_host: "{{ groups['master'][0] }}"
  vars_files:
    - variables.yml
  tasks:
    - name: Add master key to authorized keys
      authorized_key:
        user: root
        key: "{{ hostvars[master_host]['ssh_key']['public_key'] }}"
    - name: Enable firewall with ssh
      ufw:
        rule: allow
        port: ssh
        state: enabled
    - name: Restart ssh server with new config
      service:
        name: ssh
        state: restarted
- hosts: master
  become: yes
  vars:
    master_host: "{{ groups['master'][0] }}"
    slave_hosts: "{{ groups['slaves'] }}"
  tasks:
    - name: Open GUI ports to the public
      ufw:
        rule: allow
        port: "{{ item }}"
        proto: tcp
        state: reloaded
      with_items:
        - "8888"  # pyspark jupyter notebook
        - "4040"  # spark ui port
        - "8088"
        - "50070"
        - "50090"
    - name: Open all ports to slaves and master
      ufw:
        rule: allow
        src: "{{ item }}"
        state: reloaded
      with_items: "{{ slave_hosts + [master_host] }}"
    - name: Debug ufw status
      command: "ufw status verbose"
      register: ufw_status
    - debug:
        var: ufw_status
- hosts: slaves
  become: yes
  vars:
    master_host: "{{ groups['master'][0] }}"
    slave_hosts: "{{ groups['slaves'] }}"
  tasks:
    - name: Open GUI ports to the public
      ufw:
        rule: allow
        port: "{{ item }}"
        proto: tcp
        state: reloaded
      with_items:
        - "50075"
    - name: Open all ports to slaves and master
      ufw:
        rule: allow
        src: "{{ item }}"
        state: reloaded
      with_items: "{{ slave_hosts + [master_host] }}"

# 3. start hdfs
- hosts: master
  become: yes
  vars_files:
    - variables.yml
  environment:
    JAVA_HOME: "{{ paths.java }}"
    HADOOP_HOME: "{{ paths.hadoop }}"
    HADOOP_PREFIX: "{{ paths.hadoop }}"
    HADOOP_INSTALL: "{{ paths.hadoop }}"
    HADOOP_MAPRED_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_HOME: "{{ paths.hadoop }}"
    HADOOP_HDFS_HOME: "{{ paths.hadoop }}"
    YARN_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_LIB_NATIVE_DIR: "{{ paths.hadoop }}/lib/native"
    HADOOP_OPTS: "-Djava.library.path={{ paths.hadoop }}/lib/native"
    LD_LIBRARY_PATH: "{{ paths.hadoop }}/lib/native"
    PATH: "{{ ansible_env.PATH }}:{{ paths.hadoop }}/sbin:{{ paths.hadoop }}/bin"
  tasks:
    - name: Debug printenv
      command: printenv
      register: printenv
    - debug:
        var: printenv
    - name: Start HDFS if not running
      block:
        - name: Generate DFS Admin report
          command: "hdfs dfsadmin -report"
          register: dfsadmin_log
        - debug:
            var: dfsadmin_log.stdout_lines
      rescue:
        - name: Clear tmp
          command:
            cmd: "rm -rf /tmp/*"
            removes: "/tmp/*"
        - name: Format HDFS Namenode
          command: "hdfs namenode -format"
        - name: Start DFS
          command: start-dfs.sh
          register: dfs_log
        - debug:
            var: dfs_log.stdout_lines
        - name: Start Yarn
          command: start-yarn.sh
          register: yarn_log
        - debug:
            var: yarn_log.stdout_lines
        - pause:
            seconds: 30
        - name: Generate DFS Admin report
          command: "hdfs dfsadmin -report"
          register: dfsadmin_log
        - debug:
            var: dfsadmin_log.stdout_lines

# 4. install python, spark and pyspark
- hosts: all
  become: yes
  vars_files:
    - variables.yml
  environment:
    JAVA_HOME: "{{ paths.java }}"
    HADOOP_HOME: "{{ paths.hadoop }}"
    HADOOP_PREFIX: "{{ paths.hadoop }}"
    HADOOP_INSTALL: "{{ paths.hadoop }}"
    HADOOP_MAPRED_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_HOME: "{{ paths.hadoop }}"
    HADOOP_HDFS_HOME: "{{ paths.hadoop }}"
    YARN_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_LIB_NATIVE_DIR: "{{ paths.hadoop }}/lib/native"
    HADOOP_OPTS: "-Djava.library.path={{ paths.hadoop }}/lib/native"
    LD_LIBRARY_PATH: "{{ paths.hadoop }}/lib/native"
    PATH: "{{ ansible_env.PATH }}:~/.local/bin:{{ paths.hadoop }}/sbin:{{ paths.hadoop }}/bin"
    YARN_CONF_DIR: "{{ paths.hadoop }}/etc/hadoop"
    SPARK_HOME: "{{ paths.spark }}"
  tasks:
    - name: Download & extract Spark
      unarchive:
        remote_src: yes
        src: "{{ apache_download_proxy }}/spark/{{ spark_version }}/{{ spark_version }}-bin-without-hadoop.tgz"
        dest: "{{ prefix }}"
        creates: "{{ paths.spark }}"
    - name: Install python
      apt:
        name:
          - "python3"
          - "python3-pip"
        state: present
        update_cache: yes
    - name: Install python packages
      pip:
        executable: pip3
        name:
          - "jupyter"
          - "numpy"
          - "pandas"
          - "matplotlib"
          - "py4j"
          - "pyspark"

# 5. setup .bash_profile for easy access via ssh and run spark examples on yarn
- hosts: master
  become: yes
  vars_files:
    - variables.yml
  environment:
    JAVA_HOME: "{{ paths.java }}"
    HADOOP_HOME: "{{ paths.hadoop }}"
    HADOOP_PREFIX: "{{ paths.hadoop }}"
    HADOOP_INSTALL: "{{ paths.hadoop }}"
    HADOOP_MAPRED_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_HOME: "{{ paths.hadoop }}"
    HADOOP_HDFS_HOME: "{{ paths.hadoop }}"
    YARN_HOME: "{{ paths.hadoop }}"
    HADOOP_COMMON_LIB_NATIVE_DIR: "{{ paths.hadoop }}/lib/native"
    HADOOP_OPTS: "-Djava.library.path={{ paths.hadoop }}/lib/native"
    LD_LIBRARY_PATH: "{{ paths.hadoop }}/lib/native"
    PATH: "{{ ansible_env.PATH }}:~/.local/bin:{{ paths.hadoop }}/sbin:{{ paths.hadoop }}/bin"
    YARN_CONF_DIR: "{{ paths.hadoop }}/etc/hadoop"
    SPARK_HOME: "{{ paths.spark }}"
  tasks:
    - name: Create .bash_profile to easy call hdfs, yarn and spark commands via ssh
      template:
        src: master/bash_profile.j2
        dest: "/root/.bash_profile"
    - name: Test on spark example - java
      environment:
        PATH: "{{ paths.spark }}:{{ paths.spark }}/bin:{{ ansible_env.PATH }}:{{ paths.java }}/bin:{{ paths.java }}/jre/bin"
        SPARK_DIST_CLASSPATH: "{{ hdfs_classpath.stdout }}"
      command:
        cmd: >
          spark-submit --master yarn \
            --deploy-mode cluster \
            --driver-memory {{ node_config.spark_submit_driver_memory }} \
            --executor-memory {{ node_config.spark_submit_executor_memory }} \
            --executor-cores 1 \
            --class org.apache.spark.examples.SparkPi \
            $SPARK_HOME/examples/jars/spark-examples*.jar \
            10
      register: example
    - debug:
        var: example.stdout_lines
    - name: Test on spark example - python
      environment:
        PATH: "{{ paths.spark }}:{{ paths.spark }}/bin:{{ ansible_env.PATH }}:{{ paths.java }}/bin:{{ paths.java }}/jre/bin"
        SPARK_DIST_CLASSPATH: "{{ hdfs_classpath.stdout }}"
        PYTHONPATH: "{{ paths.spark }}/python:{{ paths.spark }}/python/build:{{ paths.spark }}/python/lib/pyspark.zip:{{ paths.spark }}/python/lib/py4j-0.10.7-src.zip"  # no pre-existing PYTHONPATH
        PYSPARK_PYTHON: "python3"
      command:
        cmd: >
          spark-submit --master yarn \
            --deploy-mode cluster \
            --driver-memory {{ node_config.spark_submit_driver_memory }} \
            --executor-memory {{ node_config.spark_submit_executor_memory }} \
            --executor-cores 1 \
            {{ paths.spark }}/examples/src/main/python/pi.py \
            10
      register: example
    - debug:
        var: example.stdout_lines
