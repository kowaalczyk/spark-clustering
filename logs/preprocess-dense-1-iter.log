

Container: container_1586780871303_0006_01_000004 on 178.62.200.211_37461
===========================================================================
LogType:stderr
Log Upload Time:Mon Apr 13 14:26:33 +0000 2020
LogLength:96739
Log Contents:
20/04/13 13:12:47 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 20846@178.62.200.211
20/04/13 13:12:47 INFO util.SignalUtils: Registered signal handler for TERM
20/04/13 13:12:47 INFO util.SignalUtils: Registered signal handler for HUP
20/04/13 13:12:47 INFO util.SignalUtils: Registered signal handler for INT
20/04/13 13:12:48 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 13:12:48 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 13:12:48 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 13:12:48 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 13:12:48 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 13:12:49 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:42247 after 64 ms (0 ms spent in bootstraps)
20/04/13 13:12:49 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 13:12:49 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 13:12:49 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 13:12:49 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 13:12:49 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 13:12:49 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:42247 after 6 ms (0 ms spent in bootstraps)
20/04/13 13:12:49 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0006/blockmgr-e4ba5ba9-e5b5-44c9-9b45-ca4ac8749f12
20/04/13 13:12:49 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
20/04/13 13:12:49 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.200.211:42247
20/04/13 13:12:49 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/13 13:12:49 INFO executor.Executor: Starting executor ID 3 on host 178.62.200.211
20/04/13 13:12:49 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40635.
20/04/13 13:12:49 INFO netty.NettyBlockTransferService: Server created on 178.62.200.211:40635
20/04/13 13:12:49 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/13 13:12:49 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(3, 178.62.200.211, 40635, None)
20/04/13 13:12:49 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(3, 178.62.200.211, 40635, None)
20/04/13 13:12:49 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(3, 178.62.200.211, 40635, None)
20/04/13 13:12:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0
20/04/13 13:12:53 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
20/04/13 13:12:54 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/13 13:12:54 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:34497 after 2 ms (0 ms spent in bootstraps)
20/04/13 13:12:54 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1458.6 MB)
20/04/13 13:12:54 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 112 ms
20/04/13 13:12:54 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.9 KB, free 1458.6 MB)
20/04/13 13:12:54 INFO codegen.CodeGenerator: Code generated in 317.500835 ms
20/04/13 13:12:54 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 13:12:54 INFO codegen.CodeGenerator: Code generated in 15.146895 ms
20/04/13 13:12:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
20/04/13 13:12:55 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.6 MB)
20/04/13 13:12:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 11 ms
20/04/13 13:12:55 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 536.2 KB, free 1458.0 MB)
20/04/13 13:12:56 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1375 bytes result sent to driver
20/04/13 13:12:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2
20/04/13 13:12:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5
20/04/13 13:12:56 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 2)
20/04/13 13:12:56 INFO executor.Executor: Running task 4.0 in stage 1.0 (TID 5)
20/04/13 13:12:56 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3
20/04/13 13:12:56 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1458.0 MB)
20/04/13 13:12:56 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 27 ms
20/04/13 13:12:56 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.0 KB, free 1458.0 MB)
20/04/13 13:12:56 INFO codegen.CodeGenerator: Code generated in 20.796648 ms
20/04/13 13:12:59 INFO codegen.CodeGenerator: Code generated in 15.37618 ms
20/04/13 13:12:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 13:12:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 13:12:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
20/04/13 13:12:59 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.5 MB)
20/04/13 13:12:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 32 ms
20/04/13 13:12:59 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 536.2 KB, free 1458.0 MB)
20/04/13 13:12:59 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 1640 bytes result sent to driver
20/04/13 13:12:59 INFO executor.Executor: Finished task 4.0 in stage 1.0 (TID 5). 1597 bytes result sent to driver
20/04/13 13:13:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9
20/04/13 13:13:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12
20/04/13 13:13:01 INFO executor.Executor: Running task 5.0 in stage 2.0 (TID 12)
20/04/13 13:13:01 INFO executor.Executor: Running task 2.0 in stage 2.0 (TID 9)
20/04/13 13:13:01 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
20/04/13 13:13:01 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1458.0 MB)
20/04/13 13:13:01 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 26 ms
20/04/13 13:13:01 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 27.4 KB, free 1458.0 MB)
20/04/13 13:13:02 INFO codegen.CodeGenerator: Code generated in 13.064342 ms
20/04/13 13:13:02 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 13:13:02 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
20/04/13 13:13:02 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 13:13:02 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.0 MB)
20/04/13 13:13:02 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 42 ms
20/04/13 13:13:02 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 536.2 KB, free 1457.4 MB)
20/04/13 13:13:02 INFO codegen.CodeGenerator: Code generated in 94.925474 ms
20/04/13 13:13:02 INFO codegen.CodeGenerator: Code generated in 79.041504 ms
20/04/13 13:13:03 INFO codegen.CodeGenerator: Code generated in 137.269345 ms
20/04/13 13:13:11 INFO python.PythonUDFRunner: Times: total = 1343, boot = 420, init = 314, finish = 609
20/04/13 13:13:12 INFO executor.Executor: Finished task 5.0 in stage 2.0 (TID 12). 2464 bytes result sent to driver
20/04/13 13:13:18 INFO python.PythonUDFRunner: Times: total = 5176, boot = 416, init = 320, finish = 4440
20/04/13 13:13:18 INFO executor.Executor: Finished task 2.0 in stage 2.0 (TID 9). 2421 bytes result sent to driver
20/04/13 13:13:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13
20/04/13 13:13:18 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 13)
20/04/13 13:13:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16
20/04/13 13:13:18 INFO executor.Executor: Running task 3.0 in stage 3.0 (TID 16)
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/13 13:13:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/13 13:13:18 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1976.0 B, free 1457.4 MB)
20/04/13 13:13:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 25 ms
20/04/13 13:13:18 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 1457.4 MB)
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.200.211:42247)
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 13:13:18 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 13:13:18 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 13:13:18 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:37121 after 11 ms (0 ms spent in bootstraps)
20/04/13 13:13:18 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:40647 after 1 ms (0 ms spent in bootstraps)
20/04/13 13:13:18 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 46 ms
20/04/13 13:13:18 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 55 ms
20/04/13 13:13:19 INFO memory.MemoryStore: Block rdd_21_3 stored as values in memory (estimated size 146.2 KB, free 1457.3 MB)
20/04/13 13:13:19 INFO memory.MemoryStore: Block rdd_21_0 stored as values in memory (estimated size 147.7 KB, free 1457.1 MB)
20/04/13 13:13:19 INFO executor.Executor: Finished task 3.0 in stage 3.0 (TID 16). 1176 bytes result sent to driver
20/04/13 13:13:19 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 13). 1176 bytes result sent to driver
20/04/13 13:13:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 21
20/04/13 13:13:19 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 21)
20/04/13 13:13:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 24
20/04/13 13:13:19 INFO executor.Executor: Running task 3.0 in stage 5.0 (TID 24)
20/04/13 13:13:19 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/13 13:13:19 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1457.1 MB)
20/04/13 13:13:19 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 13 ms
20/04/13 13:13:19 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.2 KB, free 1457.1 MB)
20/04/13 13:13:19 INFO storage.BlockManager: Found block rdd_21_0 locally
20/04/13 13:13:19 INFO storage.BlockManager: Found block rdd_21_3 locally
20/04/13 13:13:19 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 21). 40880 bytes result sent to driver
20/04/13 13:13:19 INFO executor.Executor: Finished task 3.0 in stage 5.0 (TID 24). 41008 bytes result sent to driver
20/04/13 13:13:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 26
20/04/13 13:13:20 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 26)
20/04/13 13:13:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 29
20/04/13 13:13:20 INFO executor.Executor: Running task 4.0 in stage 6.0 (TID 29)
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 127.2 KB, free 1457.0 MB)
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 24 ms
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 352.0 KB, free 1456.7 MB)
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 30.608128 ms
20/04/13 13:13:20 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 13:13:20 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 34.200907 ms
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.6 MB)
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 21 ms
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 80.921133 ms
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 536.2 KB, free 1456.1 MB)
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 53.952239 ms
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 42.948183 ms
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/13 13:13:21 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 77.3 KB, free 1328.0 MB)
20/04/13 13:13:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 79 ms
20/04/13 13:13:21 INFO codegen.CodeGenerator: Code generated in 158.04095 ms
20/04/13 13:13:21 INFO codegen.CodeGenerator: Code generated in 100.325466 ms
20/04/13 13:13:21 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 987.3 KB, free 1327.1 MB)
20/04/13 13:13:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:21 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:21 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:21 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:21 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:21 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:21 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:21 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:21 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:55 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 13:13:55 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 13:13:55 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 13:13:55 INFO compress.CodecPool: Got brand-new compressor [.snappy]
20/04/13 13:13:57 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 13:13:57 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 13:13:57 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 13:13:57 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 13:13:57 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 13:13:57 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 13:13:57 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 13:13:57 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 13:13:57 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 13:13:57 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 13:13:57 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 13:13:57 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 13:13:57 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 13:13:57 INFO compress.CodecPool: Got brand-new compressor [.snappy]
20/04/13 13:19:38 INFO storage.BlockManager: Removing RDD 21
20/04/13 13:48:16 INFO python.PythonUDFRunner: Times: total = 37208, boot = -13045, init = 13340, finish = 36913
20/04/13 13:57:39 INFO python.PythonUDFRunner: Times: total = 2658278, boot = 51, init = 2290, finish = 2655937
20/04/13 13:57:39 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6252730
20/04/13 13:57:39 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413131320_0006_m_000001_26' to hdfs://178.62.208.209:9000/data/df_3-shingles_dense-binary-vectors.parquet/_temporary/0/task_20200413131320_0006_m_000001
20/04/13 13:57:39 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413131320_0006_m_000001_26: Committed
20/04/13 13:57:39 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 26). 3331 bytes result sent to driver
20/04/13 14:11:15 INFO python.PythonUDFRunner: Times: total = 151804, boot = -16841, init = 17127, finish = 151518
20/04/13 14:21:55 INFO python.PythonUDFRunner: Times: total = 4114712, boot = 26, init = 2063, finish = 4112623
20/04/13 14:21:55 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7414371
20/04/13 14:21:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413131320_0006_m_000004_29' to hdfs://178.62.208.209:9000/data/df_3-shingles_dense-binary-vectors.parquet/_temporary/0/task_20200413131320_0006_m_000004
20/04/13 14:21:55 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413131320_0006_m_000004_29: Committed
20/04/13 14:21:55 INFO executor.Executor: Finished task 4.0 in stage 6.0 (TID 29). 3288 bytes result sent to driver
20/04/13 14:22:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32
20/04/13 14:22:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 35
20/04/13 14:22:13 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 32)
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14
20/04/13 14:22:13 INFO executor.Executor: Running task 3.0 in stage 8.0 (TID 35)
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1456.5 MB)
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 14 ms
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 15.0 KB, free 1456.5 MB)
20/04/13 14:22:13 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.5 MB)
20/04/13 14:22:13 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 9 ms
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 536.2 KB, free 1456.0 MB)
20/04/13 14:22:14 INFO executor.Executor: Finished task 3.0 in stage 8.0 (TID 35). 1554 bytes result sent to driver
20/04/13 14:22:14 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 32). 1554 bytes result sent to driver
20/04/13 14:22:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 39
20/04/13 14:22:14 INFO executor.Executor: Running task 1.0 in stage 9.0 (TID 39)
20/04/13 14:22:14 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16
20/04/13 14:22:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 42
20/04/13 14:22:14 INFO executor.Executor: Running task 4.0 in stage 9.0 (TID 42)
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1455.9 MB)
20/04/13 14:22:14 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 21 ms
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 27.4 KB, free 1455.9 MB)
20/04/13 14:22:14 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 14:22:14 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1455.9 MB)
20/04/13 14:22:14 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 8 ms
20/04/13 14:22:14 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 536.2 KB, free 1455.4 MB)
20/04/13 14:22:14 INFO codegen.CodeGenerator: Code generated in 86.507034 ms
20/04/13 14:22:28 INFO python.PythonUDFRunner: Times: total = 1102, boot = -18776, init = 18915, finish = 963
20/04/13 14:22:28 INFO executor.Executor: Finished task 4.0 in stage 9.0 (TID 42). 2421 bytes result sent to driver
20/04/13 14:22:28 INFO python.PythonUDFRunner: Times: total = 3687, boot = 22, init = 68, finish = 3597
20/04/13 14:22:28 INFO executor.Executor: Finished task 1.0 in stage 9.0 (TID 39). 2421 bytes result sent to driver
20/04/13 14:22:28 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 44
20/04/13 14:22:28 INFO executor.Executor: Running task 0.0 in stage 10.0 (TID 44)
20/04/13 14:22:28 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 47
20/04/13 14:22:28 INFO executor.Executor: Running task 3.0 in stage 10.0 (TID 47)
20/04/13 14:22:28 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
20/04/13 14:22:28 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17
20/04/13 14:22:28 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 1976.0 B, free 1455.4 MB)
20/04/13 14:22:28 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 8 ms
20/04/13 14:22:28 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 3.3 KB, free 1455.4 MB)
20/04/13 14:22:28 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
20/04/13 14:22:28 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.200.211:42247)
20/04/13 14:22:28 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
20/04/13 14:22:28 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 14:22:28 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 14:22:28 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 14:22:28 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 3 ms
20/04/13 14:22:28 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 4 ms
20/04/13 14:22:29 INFO memory.MemoryStore: Block rdd_54_0 stored as values in memory (estimated size 147.7 KB, free 1455.2 MB)
20/04/13 14:22:29 INFO executor.Executor: Finished task 0.0 in stage 10.0 (TID 44). 1176 bytes result sent to driver
20/04/13 14:22:29 INFO memory.MemoryStore: Block rdd_54_3 stored as values in memory (estimated size 147.4 KB, free 1455.1 MB)
20/04/13 14:22:29 INFO executor.Executor: Finished task 3.0 in stage 10.0 (TID 47). 1176 bytes result sent to driver
20/04/13 14:22:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 51
20/04/13 14:22:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 54
20/04/13 14:22:29 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 51)
20/04/13 14:22:29 INFO executor.Executor: Running task 3.0 in stage 12.0 (TID 54)
20/04/13 14:22:29 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18
20/04/13 14:22:29 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1455.1 MB)
20/04/13 14:22:29 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 14 ms
20/04/13 14:22:29 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 4.2 KB, free 1455.1 MB)
20/04/13 14:22:29 INFO storage.BlockManager: Found block rdd_54_0 locally
20/04/13 14:22:29 INFO storage.BlockManager: Found block rdd_54_3 locally
20/04/13 14:22:29 INFO executor.Executor: Finished task 3.0 in stage 12.0 (TID 54). 41008 bytes result sent to driver
20/04/13 14:22:29 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 51). 40880 bytes result sent to driver
20/04/13 14:22:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 58
20/04/13 14:22:30 INFO executor.Executor: Running task 2.0 in stage 13.0 (TID 58)
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21
20/04/13 14:22:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 61
20/04/13 14:22:30 INFO executor.Executor: Running task 5.0 in stage 13.0 (TID 61)
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 124.8 KB, free 1454.9 MB)
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 17 ms
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 347.5 KB, free 1454.6 MB)
20/04/13 14:22:30 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20
20/04/13 14:22:30 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1454.6 MB)
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 57 ms
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 536.2 KB, free 1454.1 MB)
20/04/13 14:22:30 INFO codegen.CodeGenerator: Code generated in 86.718035 ms
20/04/13 14:22:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:30 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:30 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:30 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:30 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 77.3 KB, free 1390.0 MB)
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 78 ms
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 987.3 KB, free 1389.0 MB)
20/04/13 14:22:30 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:30 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 14:22:30 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 14:22:30 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:30 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 14:22:30 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 14:22:37 INFO python.PythonUDFRunner: Times: total = 547, boot = -14664, init = 14806, finish = 405
20/04/13 14:22:37 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 3218647
20/04/13 14:22:37 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413142229_0013_m_000005_61' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413142229_0013_m_000005
20/04/13 14:22:37 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413142229_0013_m_000005_61: Committed
20/04/13 14:22:37 INFO executor.Executor: Finished task 5.0 in stage 13.0 (TID 61). 3198 bytes result sent to driver
20/04/13 14:22:45 INFO python.PythonUDFRunner: Times: total = 1103, boot = -12117, init = 12231, finish = 989
20/04/13 14:22:45 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8454266
20/04/13 14:22:45 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413142229_0013_m_000002_58' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413142229_0013_m_000002
20/04/13 14:22:45 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413142229_0013_m_000002_58: Committed
20/04/13 14:22:45 INFO executor.Executor: Finished task 2.0 in stage 13.0 (TID 58). 3198 bytes result sent to driver
20/04/13 14:22:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 62
20/04/13 14:22:46 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 62)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 23
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1453.0 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 23 took 4 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 8.9 KB, free 1453.0 MB)
20/04/13 14:22:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1453.0 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 4 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 536.2 KB, free 1452.4 MB)
20/04/13 14:22:46 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 62). 1332 bytes result sent to driver
20/04/13 14:22:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 63
20/04/13 14:22:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 66
20/04/13 14:22:46 INFO executor.Executor: Running task 0.0 in stage 15.0 (TID 63)
20/04/13 14:22:46 INFO executor.Executor: Running task 3.0 in stage 15.0 (TID 66)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1452.4 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 13 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.0 KB, free 1452.4 MB)
20/04/13 14:22:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24
20/04/13 14:22:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1452.4 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 10 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 536.2 KB, free 1451.9 MB)
20/04/13 14:22:46 INFO executor.Executor: Finished task 0.0 in stage 15.0 (TID 63). 1597 bytes result sent to driver
20/04/13 14:22:46 INFO executor.Executor: Finished task 3.0 in stage 15.0 (TID 66). 1554 bytes result sent to driver
20/04/13 14:22:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 71
20/04/13 14:22:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 74
20/04/13 14:22:46 INFO executor.Executor: Running task 2.0 in stage 16.0 (TID 71)
20/04/13 14:22:46 INFO executor.Executor: Running task 5.0 in stage 16.0 (TID 74)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1451.9 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 11 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 27.4 KB, free 1451.8 MB)
20/04/13 14:22:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26
20/04/13 14:22:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1451.8 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 26 took 41 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 536.2 KB, free 1451.3 MB)
20/04/13 14:22:46 INFO codegen.CodeGenerator: Code generated in 111.843128 ms
20/04/13 14:22:52 INFO python.PythonUDFRunner: Times: total = 573, boot = -15338, init = 15471, finish = 440
20/04/13 14:22:52 INFO executor.Executor: Finished task 5.0 in stage 16.0 (TID 74). 2421 bytes result sent to driver
20/04/13 14:22:59 INFO python.PythonUDFRunner: Times: total = 1084, boot = -15833, init = 15997, finish = 920
20/04/13 14:22:59 INFO executor.Executor: Finished task 2.0 in stage 16.0 (TID 71). 2421 bytes result sent to driver
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 76
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 79
20/04/13 14:22:59 INFO executor.Executor: Running task 4.0 in stage 17.0 (TID 79)
20/04/13 14:22:59 INFO executor.Executor: Running task 1.0 in stage 17.0 (TID 76)
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 28
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 1977.0 B, free 1451.3 MB)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 28 took 24 ms
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 3.3 KB, free 1451.3 MB)
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.200.211:42247)
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 14:22:59 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 14:22:59 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 14:22:59 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 14:22:59 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 14:22:59 INFO memory.MemoryStore: Block rdd_84_1 stored as values in memory (estimated size 147.4 KB, free 1451.1 MB)
20/04/13 14:22:59 INFO executor.Executor: Finished task 1.0 in stage 17.0 (TID 76). 1176 bytes result sent to driver
20/04/13 14:22:59 INFO memory.MemoryStore: Block rdd_84_4 stored as values in memory (estimated size 147.4 KB, free 1451.0 MB)
20/04/13 14:22:59 INFO executor.Executor: Finished task 4.0 in stage 17.0 (TID 79). 1176 bytes result sent to driver
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 81
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 84
20/04/13 14:22:59 INFO executor.Executor: Running task 1.0 in stage 19.0 (TID 81)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 29
20/04/13 14:22:59 INFO executor.Executor: Running task 4.0 in stage 19.0 (TID 84)
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1451.0 MB)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 29 took 11 ms
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 4.2 KB, free 1451.0 MB)
20/04/13 14:22:59 INFO storage.BlockManager: Found block rdd_84_1 locally
20/04/13 14:22:59 INFO storage.BlockManager: Found block rdd_84_4 locally
20/04/13 14:22:59 INFO executor.Executor: Finished task 4.0 in stage 19.0 (TID 84). 40923 bytes result sent to driver
20/04/13 14:22:59 INFO executor.Executor: Finished task 1.0 in stage 19.0 (TID 81). 40772 bytes result sent to driver
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 88
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 91
20/04/13 14:22:59 INFO executor.Executor: Running task 1.0 in stage 20.0 (TID 88)
20/04/13 14:22:59 INFO executor.Executor: Running task 4.0 in stage 20.0 (TID 91)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 32
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 127.2 KB, free 1451.9 MB)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 32 took 10 ms
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 352.0 KB, free 1451.5 MB)
20/04/13 14:22:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 31
20/04/13 14:22:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1451.5 MB)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 31 took 43 ms
20/04/13 14:22:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 536.2 KB, free 1451.0 MB)
20/04/13 14:23:00 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:23:00 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:23:00 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:23:00 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:23:00 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:23:00 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:23:00 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 30
20/04/13 14:23:00 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 77.2 KB, free 1322.9 MB)
20/04/13 14:23:00 INFO broadcast.TorrentBroadcast: Reading broadcast variable 30 took 9 ms
20/04/13 14:23:00 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 987.3 KB, free 1322.5 MB)
20/04/13 14:23:01 INFO storage.BlockManager: Removing RDD 84
20/04/13 14:23:01 INFO storage.BlockManager: Removing RDD 54
20/04/13 14:23:32 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:23:32 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 14:23:32 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 14:23:35 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:23:35 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:23:35 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 14:23:35 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 14:23:35 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 14:23:35 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 14:23:35 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 14:23:35 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 14:23:35 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 14:23:35 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 14:23:35 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 14:23:35 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 14:23:35 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 14:26:32 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
20/04/13 14:26:32 ERROR util.Utils: Aborting task
org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:490)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:479)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:71)
	... 23 more
20/04/13 14:26:32 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1076641
20/04/13 14:26:32 ERROR python.PythonUDFRunner: Python worker exited unexpectedly (crashed)
java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:210)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:71)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/13 14:26:32 ERROR python.PythonUDFRunner: This may have been caused by a prior exception:
java.net.SocketException: Connection reset
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:115)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.io.FilterOutputStream.write(FilterOutputStream.java:97)
	at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:212)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)
20/04/13 14:26:32 ERROR util.Utils: Aborting task
java.net.SocketException: Connection reset
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:115)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.io.FilterOutputStream.write(FilterOutputStream.java:97)
	at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:212)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)
	at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:224)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:224)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.writeIteratorToStream(PythonUDFRunner.scala:50)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)
20/04/13 14:26:32 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 603454
20/04/13 14:26:32 WARN hdfs.DataStreamer: DataStreamer Exception
java.io.FileNotFoundException: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000001_88/part-00001-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16622) Holder DFSClient_NONMAPREDUCE_-67892881_32 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1842)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1638)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:704)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000001_88/part-00001-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16622) Holder DFSClient_NONMAPREDUCE_-67892881_32 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy18.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1838)
	... 2 more
20/04/13 14:26:32 WARN hdfs.DataStreamer: DataStreamer Exception
java.io.FileNotFoundException: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000004_91/part-00004-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16618) Holder DFSClient_NONMAPREDUCE_-67892881_32 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1842)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1638)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:704)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000004_91/part-00004-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16618) Holder DFSClient_NONMAPREDUCE_-67892881_32 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy18.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1838)
	... 2 more
20/04/13 14:26:33 INFO memory.MemoryStore: MemoryStore cleared
20/04/13 14:26:33 INFO storage.BlockManager: BlockManager stopped
20/04/13 14:26:33 WARN output.FileOutputCommitter: Could not delete hdfs://178.62.208.209:9000/data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000004_91
20/04/13 14:26:33 WARN output.FileOutputCommitter: Could not delete hdfs://178.62.208.209:9000/data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000001_88
20/04/13 14:26:33 WARN util.Utils: Suppressing exception in catch: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000004_91/part-00004-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16618) Holder DFSClient_NONMAPREDUCE_-67892881_32 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

java.io.FileNotFoundException: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000004_91/part-00004-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16618) Holder DFSClient_NONMAPREDUCE_-67892881_32 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1842)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1638)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:704)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000004_91/part-00004-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16618) Holder DFSClient_NONMAPREDUCE_-67892881_32 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy18.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1838)
	... 2 more
20/04/13 14:26:33 WARN util.Utils: Suppressing exception in catch: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000001_88/part-00001-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16622) Holder DFSClient_NONMAPREDUCE_-67892881_32 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

java.io.FileNotFoundException: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000001_88/part-00001-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16622) Holder DFSClient_NONMAPREDUCE_-67892881_32 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1842)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1638)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:704)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000001_88/part-00001-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16622) Holder DFSClient_NONMAPREDUCE_-67892881_32 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy18.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1838)
	... 2 more
20/04/13 14:26:33 ERROR executor.Executor: Exception in task 1.0 in stage 20.0 (TID 88): Task failed while writing rows.
20/04/13 14:26:33 INFO util.ShutdownHookManager: Shutdown hook called
20/04/13 14:26:33 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0006/spark-54de5437-0994-4105-a6b1-a1cf6d05034c
20/04/13 14:26:33 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/13 14:26:33 ERROR executor.Executor: Exception in task 4.0 in stage 20.0 (TID 91): Task failed while writing rows.
End of LogType:stderr

LogType:stdout
Log Upload Time:Mon Apr 13 14:26:33 +0000 2020
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_1586780871303_0006_01_000002 on 178.62.200.211_37461
===========================================================================
LogType:stderr
Log Upload Time:Mon Apr 13 14:26:33 +0000 2020
LogLength:91181
Log Contents:
20/04/13 13:12:46 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 20815@178.62.200.211
20/04/13 13:12:46 INFO util.SignalUtils: Registered signal handler for TERM
20/04/13 13:12:46 INFO util.SignalUtils: Registered signal handler for HUP
20/04/13 13:12:46 INFO util.SignalUtils: Registered signal handler for INT
20/04/13 13:12:47 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 13:12:47 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 13:12:47 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 13:12:47 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 13:12:47 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 13:12:47 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:42247 after 75 ms (0 ms spent in bootstraps)
20/04/13 13:12:47 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 13:12:47 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 13:12:47 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 13:12:47 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 13:12:47 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 13:12:48 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:42247 after 19 ms (0 ms spent in bootstraps)
20/04/13 13:12:48 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0006/blockmgr-de61588d-7213-4013-ab41-83a253cee42b
20/04/13 13:12:48 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
20/04/13 13:12:48 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.200.211:42247
20/04/13 13:12:48 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/13 13:12:48 INFO executor.Executor: Starting executor ID 1 on host 178.62.200.211
20/04/13 13:12:48 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40647.
20/04/13 13:12:48 INFO netty.NettyBlockTransferService: Server created on 178.62.200.211:40647
20/04/13 13:12:48 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/13 13:12:48 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, 178.62.200.211, 40647, None)
20/04/13 13:12:48 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, 178.62.200.211, 40647, None)
20/04/13 13:12:48 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, 178.62.200.211, 40647, None)
20/04/13 13:12:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1
20/04/13 13:12:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4
20/04/13 13:12:56 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
20/04/13 13:12:56 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 4)
20/04/13 13:12:56 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3
20/04/13 13:12:56 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:34497 after 11 ms (0 ms spent in bootstraps)
20/04/13 13:12:56 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1458.6 MB)
20/04/13 13:12:56 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 128 ms
20/04/13 13:12:57 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.0 KB, free 1458.6 MB)
20/04/13 13:12:58 INFO codegen.CodeGenerator: Code generated in 408.601983 ms
20/04/13 13:12:59 INFO codegen.CodeGenerator: Code generated in 21.614892 ms
20/04/13 13:13:00 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 13:13:00 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 13:13:00 INFO codegen.CodeGenerator: Code generated in 15.364138 ms
20/04/13 13:13:00 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
20/04/13 13:13:00 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:40635 after 2 ms (0 ms spent in bootstraps)
20/04/13 13:13:00 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.5 MB)
20/04/13 13:13:00 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 28 ms
20/04/13 13:13:00 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 536.2 KB, free 1458.0 MB)
20/04/13 13:13:01 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1640 bytes result sent to driver
20/04/13 13:13:01 INFO executor.Executor: Finished task 3.0 in stage 1.0 (TID 4). 1597 bytes result sent to driver
20/04/13 13:13:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8
20/04/13 13:13:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11
20/04/13 13:13:01 INFO executor.Executor: Running task 1.0 in stage 2.0 (TID 8)
20/04/13 13:13:01 INFO executor.Executor: Running task 4.0 in stage 2.0 (TID 11)
20/04/13 13:13:01 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
20/04/13 13:13:02 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1458.0 MB)
20/04/13 13:13:02 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 18 ms
20/04/13 13:13:02 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 27.4 KB, free 1458.0 MB)
20/04/13 13:13:02 INFO codegen.CodeGenerator: Code generated in 21.171204 ms
20/04/13 13:13:02 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 13:13:02 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
20/04/13 13:13:02 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 13:13:02 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.0 MB)
20/04/13 13:13:02 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 67 ms
20/04/13 13:13:02 INFO codegen.CodeGenerator: Code generated in 72.745379 ms
20/04/13 13:13:02 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 536.2 KB, free 1457.4 MB)
20/04/13 13:13:02 INFO codegen.CodeGenerator: Code generated in 63.995307 ms
20/04/13 13:13:02 INFO codegen.CodeGenerator: Code generated in 27.855552 ms
20/04/13 13:13:17 INFO python.PythonUDFRunner: Times: total = 7349, boot = 401, init = 379, finish = 6569
20/04/13 13:13:18 INFO executor.Executor: Finished task 4.0 in stage 2.0 (TID 11). 2464 bytes result sent to driver
20/04/13 13:13:18 INFO python.PythonUDFRunner: Times: total = 5444, boot = 394, init = 397, finish = 4653
20/04/13 13:13:18 INFO executor.Executor: Finished task 1.0 in stage 2.0 (TID 8). 2421 bytes result sent to driver
20/04/13 13:13:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15
20/04/13 13:13:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18
20/04/13 13:13:18 INFO executor.Executor: Running task 5.0 in stage 3.0 (TID 18)
20/04/13 13:13:18 INFO executor.Executor: Running task 2.0 in stage 3.0 (TID 15)
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/13 13:13:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/13 13:13:18 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1976.0 B, free 1457.4 MB)
20/04/13 13:13:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 30 ms
20/04/13 13:13:18 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 1457.4 MB)
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.200.211:42247)
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 13:13:18 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 13:13:18 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 13:13:18 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:37121 after 3 ms (0 ms spent in bootstraps)
20/04/13 13:13:18 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 27 ms
20/04/13 13:13:18 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 25 ms
20/04/13 13:13:19 INFO memory.MemoryStore: Block rdd_21_2 stored as values in memory (estimated size 146.8 KB, free 1457.3 MB)
20/04/13 13:13:19 INFO memory.MemoryStore: Block rdd_21_5 stored as values in memory (estimated size 147.2 KB, free 1457.1 MB)
20/04/13 13:13:19 INFO executor.Executor: Finished task 2.0 in stage 3.0 (TID 15). 1176 bytes result sent to driver
20/04/13 13:13:19 INFO executor.Executor: Finished task 5.0 in stage 3.0 (TID 18). 1176 bytes result sent to driver
20/04/13 13:13:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20
20/04/13 13:13:19 INFO executor.Executor: Running task 2.0 in stage 5.0 (TID 20)
20/04/13 13:13:19 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/13 13:13:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 23
20/04/13 13:13:19 INFO executor.Executor: Running task 5.0 in stage 5.0 (TID 23)
20/04/13 13:13:19 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1457.1 MB)
20/04/13 13:13:19 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 19 ms
20/04/13 13:13:19 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.2 KB, free 1457.1 MB)
20/04/13 13:13:19 INFO storage.BlockManager: Found block rdd_21_5 locally
20/04/13 13:13:19 INFO storage.BlockManager: Found block rdd_21_2 locally
20/04/13 13:13:19 INFO executor.Executor: Finished task 2.0 in stage 5.0 (TID 20). 40807 bytes result sent to driver
20/04/13 13:13:19 INFO executor.Executor: Finished task 5.0 in stage 5.0 (TID 23). 41280 bytes result sent to driver
20/04/13 13:13:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25
20/04/13 13:13:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 28
20/04/13 13:13:20 INFO executor.Executor: Running task 3.0 in stage 6.0 (TID 28)
20/04/13 13:13:20 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 25)
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 127.2 KB, free 1457.0 MB)
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 32 ms
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 352.0 KB, free 1456.7 MB)
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 44.686606 ms
20/04/13 13:13:20 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 13:13:20 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 54.255894 ms
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 74.235157 ms
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.6 MB)
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 48 ms
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 54.560747 ms
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 536.2 KB, free 1456.1 MB)
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 124.106875 ms
20/04/13 13:13:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/13 13:13:21 INFO codegen.CodeGenerator: Code generated in 115.187266 ms
20/04/13 13:13:21 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 77.3 KB, free 1328.0 MB)
20/04/13 13:13:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 122 ms
20/04/13 13:13:21 INFO codegen.CodeGenerator: Code generated in 93.101246 ms
20/04/13 13:13:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:21 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:21 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:21 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:21 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:21 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:21 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:21 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:21 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:21 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 13:13:21 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 13:13:21 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 987.3 KB, free 1327.1 MB)
20/04/13 13:13:21 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 13:13:21 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 13:13:21 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 13:13:21 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 13:13:21 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 13:13:21 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 13:13:21 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 13:13:21 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 13:13:21 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 13:13:21 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 13:13:22 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 13:13:22 INFO compress.CodecPool: Got brand-new compressor [.snappy]
20/04/13 13:13:55 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 13:13:55 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 13:13:55 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 13:13:55 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 13:13:55 INFO compress.CodecPool: Got brand-new compressor [.snappy]
20/04/13 13:19:38 INFO storage.BlockManager: Removing RDD 21
20/04/13 13:48:45 INFO python.PythonUDFRunner: Times: total = 35505, boot = -12829, init = 13079, finish = 35255
20/04/13 13:57:29 INFO python.PythonUDFRunner: Times: total = 2648385, boot = 31, init = 1646, finish = 2646708
20/04/13 13:57:29 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 6273339
20/04/13 13:57:30 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413131320_0006_m_000000_25' to hdfs://178.62.208.209:9000/data/df_3-shingles_dense-binary-vectors.parquet/_temporary/0/task_20200413131320_0006_m_000000
20/04/13 13:57:30 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413131320_0006_m_000000_25: Committed
20/04/13 13:57:30 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 25). 3331 bytes result sent to driver
20/04/13 14:11:19 INFO python.PythonUDFRunner: Times: total = 7655, boot = -10929, init = 11185, finish = 7399
20/04/13 14:22:12 INFO python.PythonUDFRunner: Times: total = 4131665, boot = 93, init = 1528, finish = 4130044
20/04/13 14:22:12 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7540783
20/04/13 14:22:13 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413131320_0006_m_000003_28' to hdfs://178.62.208.209:9000/data/df_3-shingles_dense-binary-vectors.parquet/_temporary/0/task_20200413131320_0006_m_000003
20/04/13 14:22:13 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413131320_0006_m_000003_28: Committed
20/04/13 14:22:13 INFO executor.Executor: Finished task 3.0 in stage 6.0 (TID 28). 3288 bytes result sent to driver
20/04/13 14:22:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 33
20/04/13 14:22:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 36
20/04/13 14:22:13 INFO executor.Executor: Running task 1.0 in stage 8.0 (TID 33)
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14
20/04/13 14:22:13 INFO executor.Executor: Running task 4.0 in stage 8.0 (TID 36)
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1456.5 MB)
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 13 ms
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 15.0 KB, free 1456.5 MB)
20/04/13 14:22:13 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.5 MB)
20/04/13 14:22:13 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 24 ms
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 536.2 KB, free 1456.0 MB)
20/04/13 14:22:14 INFO executor.Executor: Finished task 1.0 in stage 8.0 (TID 33). 1597 bytes result sent to driver
20/04/13 14:22:14 INFO executor.Executor: Finished task 4.0 in stage 8.0 (TID 36). 1597 bytes result sent to driver
20/04/13 14:22:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 38
20/04/13 14:22:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 41
20/04/13 14:22:14 INFO executor.Executor: Running task 3.0 in stage 9.0 (TID 41)
20/04/13 14:22:14 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 38)
20/04/13 14:22:14 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1455.9 MB)
20/04/13 14:22:14 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 20 ms
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 27.4 KB, free 1455.9 MB)
20/04/13 14:22:14 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 14:22:14 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15
20/04/13 14:22:14 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1455.9 MB)
20/04/13 14:22:14 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 56 ms
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 536.2 KB, free 1455.4 MB)
20/04/13 14:22:14 INFO codegen.CodeGenerator: Code generated in 84.052071 ms
20/04/13 14:22:27 INFO python.PythonUDFRunner: Times: total = 1156, boot = -1555, init = 1694, finish = 1017
20/04/13 14:22:27 INFO executor.Executor: Finished task 3.0 in stage 9.0 (TID 41). 2421 bytes result sent to driver
20/04/13 14:22:28 INFO python.PythonUDFRunner: Times: total = 1777, boot = 4, init = 165, finish = 1608
20/04/13 14:22:28 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 38). 2421 bytes result sent to driver
20/04/13 14:22:28 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 46
20/04/13 14:22:28 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 49
20/04/13 14:22:28 INFO executor.Executor: Running task 2.0 in stage 10.0 (TID 46)
20/04/13 14:22:28 INFO executor.Executor: Running task 5.0 in stage 10.0 (TID 49)
20/04/13 14:22:28 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
20/04/13 14:22:28 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17
20/04/13 14:22:28 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 1976.0 B, free 1455.4 MB)
20/04/13 14:22:28 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 10 ms
20/04/13 14:22:28 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 3.3 KB, free 1455.4 MB)
20/04/13 14:22:28 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
20/04/13 14:22:28 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.200.211:42247)
20/04/13 14:22:29 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 14:22:29 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 14:22:29 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 3 ms
20/04/13 14:22:29 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 14:22:29 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 12 ms
20/04/13 14:22:29 INFO memory.MemoryStore: Block rdd_54_2 stored as values in memory (estimated size 146.8 KB, free 1455.2 MB)
20/04/13 14:22:29 INFO memory.MemoryStore: Block rdd_54_5 stored as values in memory (estimated size 147.2 KB, free 1455.1 MB)
20/04/13 14:22:29 INFO executor.Executor: Finished task 2.0 in stage 10.0 (TID 46). 1219 bytes result sent to driver
20/04/13 14:22:29 INFO executor.Executor: Finished task 5.0 in stage 10.0 (TID 49). 1176 bytes result sent to driver
20/04/13 14:22:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 50
20/04/13 14:22:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 53
20/04/13 14:22:29 INFO executor.Executor: Running task 5.0 in stage 12.0 (TID 53)
20/04/13 14:22:29 INFO executor.Executor: Running task 2.0 in stage 12.0 (TID 50)
20/04/13 14:22:29 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18
20/04/13 14:22:29 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1455.1 MB)
20/04/13 14:22:29 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 16 ms
20/04/13 14:22:29 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 4.2 KB, free 1455.1 MB)
20/04/13 14:22:29 INFO storage.BlockManager: Found block rdd_54_2 locally
20/04/13 14:22:29 INFO storage.BlockManager: Found block rdd_54_5 locally
20/04/13 14:22:29 INFO executor.Executor: Finished task 5.0 in stage 12.0 (TID 53). 41280 bytes result sent to driver
20/04/13 14:22:29 INFO executor.Executor: Finished task 2.0 in stage 12.0 (TID 50). 40807 bytes result sent to driver
20/04/13 14:22:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 56
20/04/13 14:22:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 59
20/04/13 14:22:30 INFO executor.Executor: Running task 0.0 in stage 13.0 (TID 56)
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21
20/04/13 14:22:30 INFO executor.Executor: Running task 3.0 in stage 13.0 (TID 59)
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 124.8 KB, free 1454.9 MB)
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 24 ms
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 347.5 KB, free 1454.6 MB)
20/04/13 14:22:30 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20
20/04/13 14:22:30 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1454.6 MB)
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 66 ms
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 536.2 KB, free 1454.1 MB)
20/04/13 14:22:30 INFO codegen.CodeGenerator: Code generated in 120.380714 ms
20/04/13 14:22:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:30 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:30 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:30 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:30 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:30 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:30 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 14:22:30 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 77.3 KB, free 1390.0 MB)
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 45 ms
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 987.3 KB, free 1389.0 MB)
20/04/13 14:22:30 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:30 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 14:22:30 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 14:22:44 INFO python.PythonUDFRunner: Times: total = 3636, boot = -14716, init = 14949, finish = 3403
20/04/13 14:22:44 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7933678
20/04/13 14:22:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413142229_0013_m_000000_56' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413142229_0013_m_000000
20/04/13 14:22:44 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413142229_0013_m_000000_56: Committed
20/04/13 14:22:44 INFO executor.Executor: Finished task 0.0 in stage 13.0 (TID 56). 3198 bytes result sent to driver
20/04/13 14:22:45 INFO python.PythonUDFRunner: Times: total = 1114, boot = -14077, init = 14308, finish = 883
20/04/13 14:22:45 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8690441
20/04/13 14:22:45 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413142229_0013_m_000003_59' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413142229_0013_m_000003
20/04/13 14:22:45 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413142229_0013_m_000003_59: Committed
20/04/13 14:22:45 INFO executor.Executor: Finished task 3.0 in stage 13.0 (TID 59). 3198 bytes result sent to driver
20/04/13 14:22:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 65
20/04/13 14:22:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68
20/04/13 14:22:46 INFO executor.Executor: Running task 2.0 in stage 15.0 (TID 65)
20/04/13 14:22:46 INFO executor.Executor: Running task 5.0 in stage 15.0 (TID 68)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1453.0 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 19 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.0 KB, free 1453.0 MB)
20/04/13 14:22:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 14:22:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1453.0 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 6 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 536.2 KB, free 1452.4 MB)
20/04/13 14:22:46 INFO executor.Executor: Finished task 5.0 in stage 15.0 (TID 68). 1554 bytes result sent to driver
20/04/13 14:22:46 INFO executor.Executor: Finished task 2.0 in stage 15.0 (TID 65). 1554 bytes result sent to driver
20/04/13 14:22:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 70
20/04/13 14:22:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 73
20/04/13 14:22:46 INFO executor.Executor: Running task 4.0 in stage 16.0 (TID 73)
20/04/13 14:22:46 INFO executor.Executor: Running task 1.0 in stage 16.0 (TID 70)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1452.4 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 12 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 27.4 KB, free 1452.4 MB)
20/04/13 14:22:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 14:22:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1452.4 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 26 took 14 ms
20/04/13 14:22:46 INFO codegen.CodeGenerator: Code generated in 61.656319 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 536.2 KB, free 1451.8 MB)
20/04/13 14:22:57 INFO python.PythonUDFRunner: Times: total = 1047, boot = -12766, init = 12869, finish = 944
20/04/13 14:22:57 INFO executor.Executor: Finished task 1.0 in stage 16.0 (TID 70). 2421 bytes result sent to driver
20/04/13 14:22:58 INFO python.PythonUDFRunner: Times: total = 991, boot = -15297, init = 15426, finish = 862
20/04/13 14:22:58 INFO executor.Executor: Finished task 4.0 in stage 16.0 (TID 73). 2421 bytes result sent to driver
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 77
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 80
20/04/13 14:22:59 INFO executor.Executor: Running task 2.0 in stage 17.0 (TID 77)
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 28
20/04/13 14:22:59 INFO executor.Executor: Running task 5.0 in stage 17.0 (TID 80)
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 1977.0 B, free 1451.8 MB)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 28 took 13 ms
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 3.3 KB, free 1451.8 MB)
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.200.211:42247)
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 14:22:59 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 14:22:59 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 14:22:59 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 14:22:59 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 0 ms
20/04/13 14:22:59 INFO memory.MemoryStore: Block rdd_84_2 stored as values in memory (estimated size 146.8 KB, free 1451.7 MB)
20/04/13 14:22:59 INFO executor.Executor: Finished task 2.0 in stage 17.0 (TID 77). 1176 bytes result sent to driver
20/04/13 14:22:59 INFO memory.MemoryStore: Block rdd_84_5 stored as values in memory (estimated size 147.2 KB, free 1451.6 MB)
20/04/13 14:22:59 INFO executor.Executor: Finished task 5.0 in stage 17.0 (TID 80). 1176 bytes result sent to driver
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 82
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 85
20/04/13 14:22:59 INFO executor.Executor: Running task 2.0 in stage 19.0 (TID 82)
20/04/13 14:22:59 INFO executor.Executor: Running task 5.0 in stage 19.0 (TID 85)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 29
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1451.5 MB)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 29 took 17 ms
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 4.2 KB, free 1451.5 MB)
20/04/13 14:22:59 INFO storage.BlockManager: Found block rdd_84_5 locally
20/04/13 14:22:59 INFO storage.BlockManager: Found block rdd_84_2 locally
20/04/13 14:22:59 INFO executor.Executor: Finished task 2.0 in stage 19.0 (TID 82). 40807 bytes result sent to driver
20/04/13 14:22:59 INFO executor.Executor: Finished task 5.0 in stage 19.0 (TID 85). 41280 bytes result sent to driver
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 87
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 90
20/04/13 14:22:59 INFO executor.Executor: Running task 3.0 in stage 20.0 (TID 90)
20/04/13 14:22:59 INFO executor.Executor: Running task 0.0 in stage 20.0 (TID 87)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 32
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 127.2 KB, free 1451.9 MB)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 32 took 8 ms
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 352.0 KB, free 1451.5 MB)
20/04/13 14:22:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 31
20/04/13 14:22:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 14:22:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1451.5 MB)
20/04/13 14:22:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 31 took 32 ms
20/04/13 14:22:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:59 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:59 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:59 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 14:22:59 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 14:22:59 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 14:22:59 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 14:22:59 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 14:22:59 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 14:22:59 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 14:22:59 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 14:22:59 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 14:22:59 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 14:22:59 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 14:23:00 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 536.2 KB, free 1451.0 MB)
20/04/13 14:23:00 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 30
20/04/13 14:23:00 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 77.2 KB, free 1322.9 MB)
20/04/13 14:23:00 INFO broadcast.TorrentBroadcast: Reading broadcast variable 30 took 21 ms
20/04/13 14:23:00 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 987.3 KB, free 1322.5 MB)
20/04/13 14:23:01 INFO storage.BlockManager: Removing RDD 84
20/04/13 14:23:01 INFO storage.BlockManager: Removing RDD 54
20/04/13 14:23:32 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:23:32 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 14:23:32 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 14:23:32 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 14:26:32 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
20/04/13 14:26:32 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/13 14:26:32 ERROR util.Utils: Aborting task
java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:210)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:71)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/13 14:26:32 ERROR util.Utils: Aborting task
java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:210)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:71)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/13 14:26:32 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 586010
20/04/13 14:26:32 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1045197
20/04/13 14:26:33 INFO executor.CoarseGrainedExecutorBackend: Driver from 178.62.200.211:42247 disconnected during shutdown
20/04/13 14:26:33 INFO executor.CoarseGrainedExecutorBackend: Driver from 178.62.200.211:42247 disconnected during shutdown
20/04/13 14:26:33 INFO storage.DiskBlockManager: Shutdown hook called
20/04/13 14:26:33 WARN hdfs.DataStreamer: DataStreamer Exception
java.io.FileNotFoundException: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000003_90/part-00003-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16620) Holder DFSClient_NONMAPREDUCE_-2055450304_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1842)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1638)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:704)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000003_90/part-00003-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16620) Holder DFSClient_NONMAPREDUCE_-2055450304_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy18.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy19.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1838)
	... 2 more
20/04/13 14:26:33 WARN hdfs.DataStreamer: DataStreamer Exception
java.io.FileNotFoundException: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000000_87/part-00000-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16612) Holder DFSClient_NONMAPREDUCE_-2055450304_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1842)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1638)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:704)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000000_87/part-00000-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16612) Holder DFSClient_NONMAPREDUCE_-2055450304_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy18.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy19.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1838)
	... 2 more
20/04/13 14:26:33 INFO util.ShutdownHookManager: Shutdown hook called
20/04/13 14:26:33 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0006/spark-c5d4b555-cdd4-4b49-a4ec-9fc4f892d2df
20/04/13 14:26:33 INFO memory.MemoryStore: MemoryStore cleared
20/04/13 14:26:33 INFO storage.BlockManager: BlockManager stopped
20/04/13 14:26:33 WARN output.FileOutputCommitter: Could not delete hdfs://178.62.208.209:9000/data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000000_87
20/04/13 14:26:33 WARN output.FileOutputCommitter: Could not delete hdfs://178.62.208.209:9000/data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000003_90
20/04/13 14:26:33 WARN util.Utils: Suppressing exception in catch: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000000_87/part-00000-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16612) Holder DFSClient_NONMAPREDUCE_-2055450304_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

java.io.FileNotFoundException: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000000_87/part-00000-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16612) Holder DFSClient_NONMAPREDUCE_-2055450304_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1842)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1638)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:704)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000000_87/part-00000-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16612) Holder DFSClient_NONMAPREDUCE_-2055450304_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy18.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy19.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1838)
	... 2 more
20/04/13 14:26:33 WARN util.Utils: Suppressing exception in catch: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000003_90/part-00003-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16620) Holder DFSClient_NONMAPREDUCE_-2055450304_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

java.io.FileNotFoundException: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000003_90/part-00003-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16620) Holder DFSClient_NONMAPREDUCE_-2055450304_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1842)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1638)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:704)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000003_90/part-00003-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16620) Holder DFSClient_NONMAPREDUCE_-2055450304_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy18.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy19.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1838)
	... 2 more
20/04/13 14:26:33 ERROR executor.Executor: Exception in task 0.0 in stage 20.0 (TID 87): Task failed while writing rows.
20/04/13 14:26:33 ERROR executor.Executor: Exception in task 3.0 in stage 20.0 (TID 90): Task failed while writing rows.
End of LogType:stderr

LogType:stdout
Log Upload Time:Mon Apr 13 14:26:33 +0000 2020
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_1586780871303_0006_01_000001 on 178.62.200.211_37461
===========================================================================
LogType:pyspark.log
Log Upload Time:Mon Apr 13 14:26:33 +0000 2020
LogLength:2874
Log Contents:
2020-04-13 13:12:50,774.774 INFO clustering_experiment - wrapper: preprocess_df keyword arguments:
2020-04-13 13:12:50,777.777 INFO clustering_experiment - wrapper: n_shingles: 3
2020-04-13 13:12:50,777.777 INFO clustering_experiment - wrapper: use_binary_vectors: True
2020-04-13 13:12:50,777.777 INFO clustering_experiment - wrapper: use_dense_vectors: True
2020-04-13 13:12:50,777.777 INFO clustering_experiment - preprocess_df: preprocessing started
2020-04-13 13:12:50,777.777 INFO clustering_experiment - preprocess_df: in_file = '/data/uniprot-proteome_UP000005640.tab'
2020-04-13 13:12:50,777.777 INFO clustering_experiment - preprocess_df: n_shingles = ''
2020-04-13 14:22:13,175.175 INFO clustering_experiment - preprocess_df: out_file=/data/df_3-shingles_dense-binary-vectors.parquet
2020-04-13 14:22:13,175.175 INFO clustering_experiment - wrapper: preprocess_df finished in 4162.40s
2020-04-13 14:22:13,175.175 INFO clustering_experiment - <module>: completed 1 in 4162.40s (4162.40s/it)
2020-04-13 14:22:13,175.175 INFO clustering_experiment - <module>: estimated remaining time: 45786.42s
2020-04-13 14:22:13,176.176 INFO clustering_experiment - wrapper: preprocess_df keyword arguments:
2020-04-13 14:22:13,176.176 INFO clustering_experiment - wrapper: n_shingles: 3
2020-04-13 14:22:13,176.176 INFO clustering_experiment - wrapper: use_binary_vectors: True
2020-04-13 14:22:13,176.176 INFO clustering_experiment - wrapper: use_dense_vectors: False
2020-04-13 14:22:13,176.176 INFO clustering_experiment - preprocess_df: preprocessing started
2020-04-13 14:22:13,176.176 INFO clustering_experiment - preprocess_df: in_file = '/data/uniprot-proteome_UP000005640.tab'
2020-04-13 14:22:13,176.176 INFO clustering_experiment - preprocess_df: n_shingles = ''
2020-04-13 14:22:46,012.012 INFO clustering_experiment - preprocess_df: out_file=/data/df_3-shingles_sparse-binary-vectors.parquet
2020-04-13 14:22:46,013.013 INFO clustering_experiment - wrapper: preprocess_df finished in 32.84s
2020-04-13 14:22:46,013.013 INFO clustering_experiment - <module>: completed 2 in 4195.24s (2097.62s/it)
2020-04-13 14:22:46,013.013 INFO clustering_experiment - <module>: estimated remaining time: 20976.20s
2020-04-13 14:22:46,013.013 INFO clustering_experiment - wrapper: preprocess_df keyword arguments:
2020-04-13 14:22:46,014.014 INFO clustering_experiment - wrapper: n_shingles: 3
2020-04-13 14:22:46,014.014 INFO clustering_experiment - wrapper: use_binary_vectors: False
2020-04-13 14:22:46,014.014 INFO clustering_experiment - wrapper: use_dense_vectors: True
2020-04-13 14:22:46,014.014 INFO clustering_experiment - preprocess_df: preprocessing started
2020-04-13 14:22:46,014.014 INFO clustering_experiment - preprocess_df: in_file = '/data/uniprot-proteome_UP000005640.tab'
2020-04-13 14:22:46,014.014 INFO clustering_experiment - preprocess_df: n_shingles = ''
End of LogType:pyspark.log

LogType:stderr
Log Upload Time:Mon Apr 13 14:26:33 +0000 2020
LogLength:163890
Log Contents:
20/04/13 13:12:41 INFO util.SignalUtils: Registered signal handler for TERM
20/04/13 13:12:41 INFO util.SignalUtils: Registered signal handler for HUP
20/04/13 13:12:41 INFO util.SignalUtils: Registered signal handler for INT
20/04/13 13:12:41 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 13:12:41 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 13:12:41 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 13:12:41 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 13:12:41 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 13:12:42 INFO yarn.ApplicationMaster: Preparing Local resources
20/04/13 13:12:43 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1586780871303_0006_000001
20/04/13 13:12:43 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread
20/04/13 13:12:43 INFO yarn.ApplicationMaster: Waiting for spark context initialization...
20/04/13 13:12:43 INFO spark.SparkContext: Running Spark version 2.4.5
20/04/13 13:12:43 INFO spark.SparkContext: Submitted application: ClusteringExperiment
20/04/13 13:12:43 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 13:12:43 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 13:12:43 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 13:12:43 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 13:12:43 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 13:12:43 INFO util.Utils: Successfully started service 'sparkDriver' on port 42247.
20/04/13 13:12:43 INFO spark.SparkEnv: Registering MapOutputTracker
20/04/13 13:12:43 INFO spark.SparkEnv: Registering BlockManagerMaster
20/04/13 13:12:43 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/04/13 13:12:43 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/04/13 13:12:43 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0006/blockmgr-88e5e728-0650-470a-8903-5684f023b6b8
20/04/13 13:12:43 INFO memory.MemoryStore: MemoryStore started with capacity 912.3 MB
20/04/13 13:12:44 INFO spark.SparkEnv: Registering OutputCommitCoordinator
20/04/13 13:12:44 INFO util.log: Logging initialized @3653ms
20/04/13 13:12:44 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
20/04/13 13:12:44 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
20/04/13 13:12:44 INFO server.Server: Started @3766ms
20/04/13 13:12:44 INFO server.AbstractConnector: Started ServerConnector@259ce822{HTTP/1.1,[http/1.1]}{0.0.0.0:39083}
20/04/13 13:12:44 INFO util.Utils: Successfully started service 'SparkUI' on port 39083.
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4135ebd9{/jobs,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@445c876f{/jobs/json,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@37ba8bb2{/jobs/job,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@8804a22{/jobs/job/json,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2847c3a2{/stages,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4bc33a03{/stages/json,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48cf2ba1{/stages/stage,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6991359{/stages/stage/json,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@373ab208{/stages/pool,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5087d786{/stages/pool/json,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@458df255{/storage,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@bc03266{/storage/json,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69087be4{/storage/rdd,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ddbaa4{/storage/rdd/json,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d68ff50{/environment,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27b1bb99{/environment/json,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@dcc33d0{/executors,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@672eb057{/executors/json,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d56519d{/executors/threadDump,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27915edd{/executors/threadDump/json,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b2fbb4c{/static,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ce8deca{/,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74805af4{/api,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e818c25{/jobs/job/kill,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@525af1fa{/stages/stage/kill,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://178.62.200.211:39083
20/04/13 13:12:44 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler
20/04/13 13:12:44 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1586780871303_0006 and attemptId Some(appattempt_1586780871303_0006_000001)
20/04/13 13:12:44 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34497.
20/04/13 13:12:44 INFO netty.NettyBlockTransferService: Server created on 178.62.200.211:34497
20/04/13 13:12:44 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/13 13:12:44 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 178.62.200.211, 34497, None)
20/04/13 13:12:44 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.200.211:34497 with 912.3 MB RAM, BlockManagerId(driver, 178.62.200.211, 34497, None)
20/04/13 13:12:44 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 178.62.200.211, 34497, None)
20/04/13 13:12:44 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 178.62.200.211, 34497, None)
20/04/13 13:12:44 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
20/04/13 13:12:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63a27df1{/metrics/json,null,AVAILABLE,@Spark}
20/04/13 13:12:44 INFO client.RMProxy: Connecting to ResourceManager at /178.62.208.209:8030
20/04/13 13:12:44 INFO yarn.YarnRMClient: Registering the ApplicationMaster
20/04/13 13:12:45 INFO yarn.ApplicationMaster: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/src/hadoop-2.8.5/etc/hadoop:/usr/src/hadoop-2.8.5/share/hadoop/common/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/common/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/*:/usr/src/hadoop-2.8.5/contrib/capacity-scheduler/*.jar<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__
    SPARK_DIST_CLASSPATH -> /usr/src/hadoop-2.8.5/etc/hadoop:/usr/src/hadoop-2.8.5/share/hadoop/common/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/common/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/*:/usr/src/hadoop-2.8.5/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> hdfs://178.62.208.209:9000/user/root/.sparkStaging/application_1586780871303_0006
    SPARK_USER -> root
    PYTHONPATH -> /usr/src/spark-2.4.5-bin-without-hadoop/python:/usr/src/spark-2.4.5-bin-without-hadoop/python/build:/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/pyspark.zip:/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip

  command:
    {{JAVA_HOME}}/bin/java \ 
      -server \ 
      -Xmx3072m \ 
      -Djava.io.tmpdir={{PWD}}/tmp \ 
      '-Dspark.ui.port=0' \ 
      '-Dspark.driver.port=42247' \ 
      -Dspark.yarn.app.container.log.dir=<LOG_DIR> \ 
      -XX:OnOutOfMemoryError='kill %p' \ 
      org.apache.spark.executor.CoarseGrainedExecutorBackend \ 
      --driver-url \ 
      spark://CoarseGrainedScheduler@178.62.200.211:42247 \ 
      --executor-id \ 
      <executorId> \ 
      --hostname \ 
      <hostname> \ 
      --cores \ 
      2 \ 
      --app-id \ 
      application_1586780871303_0006 \ 
      --user-class-path \ 
      file:$PWD/__app__.jar \ 
      1><LOG_DIR>/stdout \ 
      2><LOG_DIR>/stderr

  resources:
    pyspark.zip -> resource { scheme: "hdfs" host: "178.62.208.209" port: 9000 file: "/user/root/.sparkStaging/application_1586780871303_0006/pyspark.zip" } size: 591945 timestamp: 1586783557722 type: FILE visibility: PRIVATE
    py4j-0.10.7-src.zip -> resource { scheme: "hdfs" host: "178.62.208.209" port: 9000 file: "/user/root/.sparkStaging/application_1586780871303_0006/py4j-0.10.7-src.zip" } size: 42437 timestamp: 1586783557752 type: FILE visibility: PRIVATE
    __spark_libs__ -> resource { scheme: "hdfs" host: "178.62.208.209" port: 9000 file: "/user/root/.sparkStaging/application_1586780871303_0006/__spark_libs__7063352374738231526.zip" } size: 168822862 timestamp: 1586783557495 type: ARCHIVE visibility: PRIVATE
    __spark_conf__ -> resource { scheme: "hdfs" host: "178.62.208.209" port: 9000 file: "/user/root/.sparkStaging/application_1586780871303_0006/__spark_conf__.zip" } size: 233322 timestamp: 1586783557957 type: ARCHIVE visibility: PRIVATE

===============================================================================
20/04/13 13:12:45 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@178.62.200.211:42247)
20/04/13 13:12:45 INFO yarn.YarnAllocator: Will request 3 executor container(s), each with 2 core(s) and 3456 MB memory (including 384 MB of overhead)
20/04/13 13:12:45 INFO yarn.YarnAllocator: Submitted 3 unlocalized container requests.
20/04/13 13:12:45 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals
20/04/13 13:12:45 INFO impl.AMRMClientImpl: Received new token for : 178.62.200.211:37461
20/04/13 13:12:45 INFO yarn.YarnAllocator: Launching container container_1586780871303_0006_01_000002 on host 178.62.200.211 for executor with ID 1
20/04/13 13:12:45 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/13 13:12:45 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/13 13:12:45 INFO impl.AMRMClientImpl: Received new token for : 178.62.210.13:35211
20/04/13 13:12:45 INFO yarn.YarnAllocator: Launching container container_1586780871303_0006_01_000003 on host 178.62.210.13 for executor with ID 2
20/04/13 13:12:45 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/13 13:12:45 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/13 13:12:46 INFO yarn.YarnAllocator: Launching container container_1586780871303_0006_01_000004 on host 178.62.200.211 for executor with ID 3
20/04/13 13:12:46 INFO yarn.YarnAllocator: Received 2 containers from YARN, launching executors on 1 of them.
20/04/13 13:12:46 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/13 13:12:48 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.200.211:33498) with ID 1
20/04/13 13:12:48 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.200.211:40647 with 1458.6 MB RAM, BlockManagerId(1, 178.62.200.211, 40647, None)
20/04/13 13:12:49 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.210.13:36772) with ID 2
20/04/13 13:12:49 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.210.13:37121 with 1458.6 MB RAM, BlockManagerId(2, 178.62.210.13, 37121, None)
20/04/13 13:12:49 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 0 of them.
20/04/13 13:12:49 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.200.211:33502) with ID 3
20/04/13 13:12:49 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/04/13 13:12:49 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
20/04/13 13:12:49 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.200.211:40635 with 1458.6 MB RAM, BlockManagerId(3, 178.62.200.211, 40635, None)
20/04/13 13:12:50 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0006/container_1586780871303_0006_01_000001/spark-warehouse').
20/04/13 13:12:50 INFO internal.SharedState: Warehouse path is 'file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0006/container_1586780871303_0006_01_000001/spark-warehouse'.
20/04/13 13:12:50 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.
20/04/13 13:12:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14a64ef3{/SQL,null,AVAILABLE,@Spark}
20/04/13 13:12:50 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.
20/04/13 13:12:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74b6869f{/SQL/json,null,AVAILABLE,@Spark}
20/04/13 13:12:50 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.
20/04/13 13:12:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@13ae4d01{/SQL/execution,null,AVAILABLE,@Spark}
20/04/13 13:12:50 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.
20/04/13 13:12:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45150c8a{/SQL/execution/json,null,AVAILABLE,@Spark}
20/04/13 13:12:50 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.
20/04/13 13:12:50 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fd3bac6{/static/sql,null,AVAILABLE,@Spark}
20/04/13 13:12:50 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/04/13 13:12:50 INFO datasources.InMemoryFileIndex: It took 92 ms to list leaf files for 1 paths.
20/04/13 13:12:50 INFO datasources.InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
20/04/13 13:12:52 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 13:12:52 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
20/04/13 13:12:52 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 13:12:52 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 13:12:53 INFO codegen.CodeGenerator: Code generated in 286.944812 ms
20/04/13 13:12:53 INFO codegen.CodeGenerator: Code generated in 22.973313 ms
20/04/13 13:12:53 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 337.3 KB, free 912.0 MB)
20/04/13 13:12:53 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 31.5 KB, free 911.9 MB)
20/04/13 13:12:53 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 178.62.200.211:34497 (size: 31.5 KB, free: 912.3 MB)
20/04/13 13:12:53 INFO spark.SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
20/04/13 13:12:53 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 13:12:53 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 13:12:53 INFO scheduler.DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/04/13 13:12:53 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 13:12:53 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 13:12:53 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 13:12:53 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 13:12:53 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.9 KB, free 911.9 MB)
20/04/13 13:12:53 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 911.9 MB)
20/04/13 13:12:53 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.200.211:34497 (size: 4.6 KB, free: 912.3 MB)
20/04/13 13:12:53 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163
20/04/13 13:12:53 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/04/13 13:12:53 INFO cluster.YarnClusterScheduler: Adding task set 0.0 with 1 tasks
20/04/13 13:12:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 13:12:54 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.200.211:40635 (size: 4.6 KB, free: 1458.6 MB)
20/04/13 13:12:55 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 178.62.200.211:40635 (size: 31.5 KB, free: 1458.6 MB)
20/04/13 13:12:56 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2230 ms on 178.62.200.211 (executor 3) (1/1)
20/04/13 13:12:56 INFO cluster.YarnClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/04/13 13:12:56 INFO scheduler.DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 2.321 s
20/04/13 13:12:56 INFO scheduler.DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 2.375715 s
20/04/13 13:12:56 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 13:12:56 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 13:12:56 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 13:12:56 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 13:12:56 INFO codegen.CodeGenerator: Code generated in 10.701517 ms
20/04/13 13:12:56 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 337.3 KB, free 911.6 MB)
20/04/13 13:12:56 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 31.5 KB, free 911.6 MB)
20/04/13 13:12:56 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.200.211:34497 (size: 31.5 KB, free: 912.2 MB)
20/04/13 13:12:56 INFO spark.SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
20/04/13 13:12:56 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 13:12:56 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 13:12:56 INFO scheduler.DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 13:12:56 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 13:12:56 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 13:12:56 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 22
20/04/13 13:12:56 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 1
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 5
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 12
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 25
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 19
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 13
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 26
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 28
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 11
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 9
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 2
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 16
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 6
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 8
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 15
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 17
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 31
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 14
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 10
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 20
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 24
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 30
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 21
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 29
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 27
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 18
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 23
20/04/13 13:12:56 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.0 KB, free 911.6 MB)
20/04/13 13:12:56 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KB, free 911.5 MB)
20/04/13 13:12:56 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 178.62.200.211:34497 (size: 8.0 KB, free: 912.2 MB)
20/04/13 13:12:56 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163
20/04/13 13:12:56 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 13:12:56 INFO cluster.YarnClusterScheduler: Adding task set 1.0 with 6 tasks
20/04/13 13:12:56 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 13:12:56 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, 178.62.200.211, executor 3, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 13:12:56 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 13:12:56 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 13:12:56 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, 178.62.200.211, executor 3, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 13:12:56 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 13:12:56 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 178.62.200.211:34497 in memory (size: 4.6 KB, free: 912.2 MB)
20/04/13 13:12:56 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 178.62.200.211:40635 in memory (size: 4.6 KB, free: 1458.6 MB)
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 4
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 7
20/04/13 13:12:56 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 178.62.200.211:40635 (size: 8.0 KB, free: 1458.6 MB)
20/04/13 13:12:56 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 178.62.200.211:34497 in memory (size: 31.5 KB, free: 912.3 MB)
20/04/13 13:12:56 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 178.62.200.211:40635 in memory (size: 31.5 KB, free: 1458.6 MB)
20/04/13 13:12:56 INFO spark.ContextCleaner: Cleaned accumulator 3
20/04/13 13:12:56 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 178.62.210.13:37121 (size: 8.0 KB, free: 1458.6 MB)
20/04/13 13:12:56 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 178.62.200.211:40647 (size: 8.0 KB, free: 1458.6 MB)
20/04/13 13:12:58 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.210.13:37121 (size: 31.5 KB, free: 1458.6 MB)
20/04/13 13:12:59 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.200.211:40635 (size: 31.5 KB, free: 1458.6 MB)
20/04/13 13:12:59 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 3194 ms on 178.62.200.211 (executor 3) (1/6)
20/04/13 13:12:59 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 3216 ms on 178.62.200.211 (executor 3) (2/6)
20/04/13 13:13:00 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.200.211:40647 (size: 31.5 KB, free: 1458.6 MB)
20/04/13 13:13:00 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 3603 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 13:13:00 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 3677 ms on 178.62.210.13 (executor 2) (4/6)
20/04/13 13:13:01 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 4848 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 13:13:01 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 4847 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 13:13:01 INFO cluster.YarnClusterScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/04/13 13:13:01 INFO scheduler.DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 4.888 s
20/04/13 13:13:01 INFO scheduler.DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 4.897966 s
20/04/13 13:13:01 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 13:13:01 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 13:13:01 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Sequence: string>
20/04/13 13:13:01 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 13:13:01 INFO codegen.CodeGenerator: Code generated in 22.481897 ms
20/04/13 13:13:01 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 337.3 KB, free 911.6 MB)
20/04/13 13:13:01 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 31.5 KB, free 911.6 MB)
20/04/13 13:13:01 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.200.211:34497 (size: 31.5 KB, free: 912.2 MB)
20/04/13 13:13:01 INFO spark.SparkContext: Created broadcast 4 from rdd at CountVectorizer.scala:187
20/04/13 13:13:01 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 13:13:01 INFO spark.SparkContext: Starting job: count at CountVectorizer.scala:230
20/04/13 13:13:01 INFO scheduler.DAGScheduler: Registering RDD 19 (flatMap at CountVectorizer.scala:205) as input to shuffle 0
20/04/13 13:13:01 INFO scheduler.DAGScheduler: Got job 2 (count at CountVectorizer.scala:230) with 6 output partitions
20/04/13 13:13:01 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (count at CountVectorizer.scala:230)
20/04/13 13:13:01 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/04/13 13:13:01 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/04/13 13:13:01 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[19] at flatMap at CountVectorizer.scala:205), which has no missing parents
20/04/13 13:13:01 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 27.4 KB, free 911.5 MB)
20/04/13 13:13:01 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.4 KB, free 911.5 MB)
20/04/13 13:13:01 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.200.211:34497 (size: 13.4 KB, free: 912.2 MB)
20/04/13 13:13:01 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1163
20/04/13 13:13:01 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[19] at flatMap at CountVectorizer.scala:205) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 13:13:01 INFO cluster.YarnClusterScheduler: Adding task set 2.0 with 6 tasks
20/04/13 13:13:01 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 7, 178.62.210.13, executor 2, partition 0, NODE_LOCAL, 8258 bytes)
20/04/13 13:13:01 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 8, 178.62.200.211, executor 1, partition 1, NODE_LOCAL, 8258 bytes)
20/04/13 13:13:01 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 9, 178.62.200.211, executor 3, partition 2, NODE_LOCAL, 8258 bytes)
20/04/13 13:13:01 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 2.0 (TID 10, 178.62.210.13, executor 2, partition 3, NODE_LOCAL, 8258 bytes)
20/04/13 13:13:01 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 2.0 (TID 11, 178.62.200.211, executor 1, partition 4, NODE_LOCAL, 8258 bytes)
20/04/13 13:13:01 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 2.0 (TID 12, 178.62.200.211, executor 3, partition 5, NODE_LOCAL, 8258 bytes)
20/04/13 13:13:01 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.200.211:40635 (size: 13.4 KB, free: 1458.5 MB)
20/04/13 13:13:01 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.210.13:37121 (size: 13.4 KB, free: 1458.5 MB)
20/04/13 13:13:02 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.200.211:40647 (size: 13.4 KB, free: 1458.5 MB)
20/04/13 13:13:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.210.13:37121 (size: 31.5 KB, free: 1458.5 MB)
20/04/13 13:13:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.200.211:40635 (size: 31.5 KB, free: 1458.5 MB)
20/04/13 13:13:02 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.200.211:40647 (size: 31.5 KB, free: 1458.5 MB)
20/04/13 13:13:12 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 2.0 (TID 12) in 10187 ms on 178.62.200.211 (executor 3) (1/6)
20/04/13 13:13:12 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 54579
20/04/13 13:13:14 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 2.0 (TID 10) in 12637 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 13:13:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 7) in 13215 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 13:13:18 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 2.0 (TID 11) in 16158 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 13:13:18 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 8) in 16458 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 13:13:18 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 9) in 16638 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 13:13:18 INFO cluster.YarnClusterScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/04/13 13:13:18 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (flatMap at CountVectorizer.scala:205) finished in 16.685 s
20/04/13 13:13:18 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/04/13 13:13:18 INFO scheduler.DAGScheduler: running: Set()
20/04/13 13:13:18 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 3)
20/04/13 13:13:18 INFO scheduler.DAGScheduler: failed: Set()
20/04/13 13:13:18 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[21] at map at CountVectorizer.scala:223), which has no missing parents
20/04/13 13:13:18 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 911.5 MB)
20/04/13 13:13:18 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1976.0 B, free 911.5 MB)
20/04/13 13:13:18 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.200.211:34497 (size: 1976.0 B, free: 912.2 MB)
20/04/13 13:13:18 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1163
20/04/13 13:13:18 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 3 (MapPartitionsRDD[21] at map at CountVectorizer.scala:223) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 13:13:18 INFO cluster.YarnClusterScheduler: Adding task set 3.0 with 6 tasks
20/04/13 13:13:18 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 13, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 7651 bytes)
20/04/13 13:13:18 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 14, 178.62.210.13, executor 2, partition 1, NODE_LOCAL, 7651 bytes)
20/04/13 13:13:18 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 15, 178.62.200.211, executor 1, partition 2, NODE_LOCAL, 7651 bytes)
20/04/13 13:13:18 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 3.0 (TID 16, 178.62.200.211, executor 3, partition 3, NODE_LOCAL, 7651 bytes)
20/04/13 13:13:18 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 3.0 (TID 17, 178.62.210.13, executor 2, partition 4, NODE_LOCAL, 7651 bytes)
20/04/13 13:13:18 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 3.0 (TID 18, 178.62.200.211, executor 1, partition 5, NODE_LOCAL, 7651 bytes)
20/04/13 13:13:18 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.210.13:37121 (size: 1976.0 B, free: 1458.5 MB)
20/04/13 13:13:18 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.200.211:40635 (size: 1976.0 B, free: 1458.5 MB)
20/04/13 13:13:18 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.200.211:40647 (size: 1976.0 B, free: 1458.5 MB)
20/04/13 13:13:18 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.210.13:36772
20/04/13 13:13:18 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.200.211:33502
20/04/13 13:13:18 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.200.211:33498
20/04/13 13:13:18 INFO storage.BlockManagerInfo: Added rdd_21_4 in memory on 178.62.210.13:37121 (size: 147.7 KB, free: 1458.4 MB)
20/04/13 13:13:18 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 3.0 (TID 17) in 317 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 13:13:18 INFO storage.BlockManagerInfo: Added rdd_21_1 in memory on 178.62.210.13:37121 (size: 147.4 KB, free: 1458.2 MB)
20/04/13 13:13:18 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 14) in 326 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 13:13:19 INFO storage.BlockManagerInfo: Added rdd_21_3 in memory on 178.62.200.211:40635 (size: 146.2 KB, free: 1458.4 MB)
20/04/13 13:13:19 INFO storage.BlockManagerInfo: Added rdd_21_0 in memory on 178.62.200.211:40635 (size: 147.7 KB, free: 1458.2 MB)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 13) in 480 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 3.0 (TID 16) in 479 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 13:13:19 INFO storage.BlockManagerInfo: Added rdd_21_5 in memory on 178.62.200.211:40647 (size: 147.2 KB, free: 1458.4 MB)
20/04/13 13:13:19 INFO storage.BlockManagerInfo: Added rdd_21_2 in memory on 178.62.200.211:40647 (size: 146.8 KB, free: 1458.2 MB)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 15) in 529 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 3.0 (TID 18) in 535 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 13:13:19 INFO cluster.YarnClusterScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/04/13 13:13:19 INFO scheduler.DAGScheduler: ResultStage 3 (count at CountVectorizer.scala:230) finished in 0.545 s
20/04/13 13:13:19 INFO scheduler.DAGScheduler: Job 2 finished: count at CountVectorizer.scala:230, took 17.257328 s
20/04/13 13:13:19 INFO spark.SparkContext: Starting job: top at CountVectorizer.scala:233
20/04/13 13:13:19 INFO scheduler.DAGScheduler: Got job 3 (top at CountVectorizer.scala:233) with 6 output partitions
20/04/13 13:13:19 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (top at CountVectorizer.scala:233)
20/04/13 13:13:19 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
20/04/13 13:13:19 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 13:13:19 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at top at CountVectorizer.scala:233), which has no missing parents
20/04/13 13:13:19 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.2 KB, free 911.5 MB)
20/04/13 13:13:19 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 911.5 MB)
20/04/13 13:13:19 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.200.211:34497 (size: 2.4 KB, free: 912.2 MB)
20/04/13 13:13:19 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1163
20/04/13 13:13:19 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at top at CountVectorizer.scala:233) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 13:13:19 INFO cluster.YarnClusterScheduler: Adding task set 5.0 with 6 tasks
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 19, 178.62.210.13, executor 2, partition 1, PROCESS_LOCAL, 7651 bytes)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 20, 178.62.200.211, executor 1, partition 2, PROCESS_LOCAL, 7651 bytes)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 21, 178.62.200.211, executor 3, partition 0, PROCESS_LOCAL, 7651 bytes)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 5.0 (TID 22, 178.62.210.13, executor 2, partition 4, PROCESS_LOCAL, 7651 bytes)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 5.0 (TID 23, 178.62.200.211, executor 1, partition 5, PROCESS_LOCAL, 7651 bytes)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 5.0 (TID 24, 178.62.200.211, executor 3, partition 3, PROCESS_LOCAL, 7651 bytes)
20/04/13 13:13:19 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.210.13:37121 (size: 2.4 KB, free: 1458.2 MB)
20/04/13 13:13:19 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.200.211:40647 (size: 2.4 KB, free: 1458.2 MB)
20/04/13 13:13:19 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.200.211:40635 (size: 2.4 KB, free: 1458.2 MB)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 19) in 90 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 5.0 (TID 22) in 94 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 21) in 113 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 20) in 143 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 5.0 (TID 23) in 155 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 13:13:19 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 5.0 (TID 24) in 154 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 13:13:19 INFO cluster.YarnClusterScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/04/13 13:13:19 INFO scheduler.DAGScheduler: ResultStage 5 (top at CountVectorizer.scala:233) finished in 0.174 s
20/04/13 13:13:19 INFO scheduler.DAGScheduler: Job 3 finished: top at CountVectorizer.scala:233, took 0.181596 s
20/04/13 13:13:19 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 1186.5 KB, free 910.3 MB)
20/04/13 13:13:19 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 77.3 KB, free 910.3 MB)
20/04/13 13:13:19 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.200.211:34497 (size: 77.3 KB, free: 912.1 MB)
20/04/13 13:13:19 INFO spark.SparkContext: Created broadcast 8 from broadcast at CountVectorizer.scala:298
20/04/13 13:13:19 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 13:13:19 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 13:13:19 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Entry: string, Entry name: string, Sequence: string ... 1 more fields>
20/04/13 13:13:19 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 13:13:20 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:20 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:20 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:20 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:20 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:20 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:20 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 28.649945 ms
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 19.243748 ms
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 337.3 KB, free 909.9 MB)
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 31.5 KB, free 909.9 MB)
20/04/13 13:13:20 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.200.211:34497 (size: 31.5 KB, free: 912.1 MB)
20/04/13 13:13:20 INFO spark.SparkContext: Created broadcast 9 from parquet at NativeMethodAccessorImpl.java:0
20/04/13 13:13:20 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 13:13:20 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
20/04/13 13:13:20 INFO scheduler.DAGScheduler: Got job 4 (parquet at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 13:13:20 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0)
20/04/13 13:13:20 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 13:13:20 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 13:13:20 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[30] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 352.0 KB, free 909.6 MB)
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 127.2 KB, free 909.4 MB)
20/04/13 13:13:20 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.200.211:34497 (size: 127.2 KB, free: 912.0 MB)
20/04/13 13:13:20 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1163
20/04/13 13:13:20 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 6 (MapPartitionsRDD[30] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 13:13:20 INFO cluster.YarnClusterScheduler: Adding task set 6.0 with 6 tasks
20/04/13 13:13:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 25, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 13:13:20 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 26, 178.62.200.211, executor 3, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 13:13:20 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 27, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 13:13:20 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 28, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 13:13:20 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 29, 178.62.200.211, executor 3, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 13:13:20 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 30, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 13:13:20 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.200.211:40635 (size: 127.2 KB, free: 1458.1 MB)
20/04/13 13:13:20 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.210.13:37121 (size: 127.2 KB, free: 1458.1 MB)
20/04/13 13:13:20 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.200.211:40647 (size: 127.2 KB, free: 1458.1 MB)
20/04/13 13:13:20 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.210.13:37121 (size: 31.5 KB, free: 1458.1 MB)
20/04/13 13:13:20 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.200.211:40635 (size: 31.5 KB, free: 1458.1 MB)
20/04/13 13:13:20 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.200.211:40647 (size: 31.5 KB, free: 1458.1 MB)
20/04/13 13:13:20 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.210.13:37121 (size: 77.3 KB, free: 1458.0 MB)
20/04/13 13:13:21 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.200.211:40635 (size: 77.3 KB, free: 1458.0 MB)
20/04/13 13:13:21 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.200.211:40647 (size: 77.3 KB, free: 1458.0 MB)
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 92
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 106
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 50
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 39
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 44
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 128
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 60
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 65
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 109
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 77
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 82
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 49
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 53
20/04/13 13:19:38 INFO storage.BlockManager: Removing RDD 21
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned RDD 21
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 120
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 85
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 66
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 136
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 100
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 88
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 38
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 129
20/04/13 13:19:38 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 178.62.210.13:37121 in memory (size: 2.4 KB, free: 1458.3 MB)
20/04/13 13:19:38 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 178.62.200.211:34497 in memory (size: 2.4 KB, free: 912.0 MB)
20/04/13 13:19:38 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 178.62.200.211:40647 in memory (size: 2.4 KB, free: 1458.3 MB)
20/04/13 13:19:38 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 178.62.200.211:40635 in memory (size: 2.4 KB, free: 1458.3 MB)
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 138
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 40
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 93
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 104
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 111
20/04/13 13:19:38 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.210.13:37121 in memory (size: 31.5 KB, free: 1458.3 MB)
20/04/13 13:19:38 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.200.211:40647 in memory (size: 31.5 KB, free: 1458.3 MB)
20/04/13 13:19:38 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.200.211:40635 in memory (size: 31.5 KB, free: 1458.3 MB)
20/04/13 13:19:38 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.200.211:34497 in memory (size: 31.5 KB, free: 912.0 MB)
20/04/13 13:19:38 INFO spark.ContextCleaner: Cleaned accumulator 127
20/04/13 13:19:38 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 178.62.210.13:37121 in memory (size: 8.0 KB, free: 1458.3 MB)
20/04/13 13:19:38 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 178.62.200.211:34497 in memory (size: 8.0 KB, free: 912.0 MB)
20/04/13 13:19:38 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 178.62.200.211:40635 in memory (size: 8.0 KB, free: 1458.3 MB)
20/04/13 13:19:38 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 178.62.200.211:40647 in memory (size: 8.0 KB, free: 1458.3 MB)
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 55
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 64
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 110
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 114
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 115
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 124
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 96
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 86
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 89
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 68
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 91
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 131
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 142
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 47
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 122
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 102
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 84
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 71
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 123
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 46
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 63
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 72
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 107
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 57
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 56
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 83
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 130
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 79
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 108
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 43
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 141
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 54
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 97
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 112
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 42
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 48
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 61
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 51
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 137
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 62
20/04/13 13:19:39 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.210.13:37121 in memory (size: 13.4 KB, free: 1458.3 MB)
20/04/13 13:19:39 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.200.211:40647 in memory (size: 13.4 KB, free: 1458.3 MB)
20/04/13 13:19:39 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.200.211:34497 in memory (size: 13.4 KB, free: 912.0 MB)
20/04/13 13:19:39 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.200.211:40635 in memory (size: 13.4 KB, free: 1458.3 MB)
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 140
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 37
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 75
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 90
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 116
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 121
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 78
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 67
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 58
20/04/13 13:19:39 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 178.62.210.13:37121 in memory (size: 1976.0 B, free: 1458.3 MB)
20/04/13 13:19:39 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 178.62.200.211:40635 in memory (size: 1976.0 B, free: 1458.3 MB)
20/04/13 13:19:39 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 178.62.200.211:40647 in memory (size: 1976.0 B, free: 1458.3 MB)
20/04/13 13:19:39 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 178.62.200.211:34497 in memory (size: 1976.0 B, free: 912.0 MB)
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 73
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 119
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 101
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 41
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 134
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 125
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 76
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 52
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 45
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 113
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned shuffle 0
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 99
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 139
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 117
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 118
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 69
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 59
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 74
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 80
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 98
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 103
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 95
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 126
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 132
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 135
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 133
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 81
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 87
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 70
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 105
20/04/13 13:19:39 INFO spark.ContextCleaner: Cleaned accumulator 94
20/04/13 13:33:47 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 6.0 (TID 30) in 1227617 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 13:42:45 INFO spark.ContextCleaner: Cleaned accumulator 32
20/04/13 13:42:45 INFO spark.ContextCleaner: Cleaned accumulator 34
20/04/13 13:42:45 INFO spark.ContextCleaner: Cleaned accumulator 36
20/04/13 13:42:45 INFO spark.ContextCleaner: Cleaned accumulator 33
20/04/13 13:42:45 INFO spark.ContextCleaner: Cleaned accumulator 35
20/04/13 13:42:45 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.210.13:37121 in memory (size: 31.5 KB, free: 1458.4 MB)
20/04/13 13:42:45 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.200.211:40647 in memory (size: 31.5 KB, free: 1458.4 MB)
20/04/13 13:42:45 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.200.211:34497 in memory (size: 31.5 KB, free: 912.1 MB)
20/04/13 13:42:45 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.200.211:40635 in memory (size: 31.5 KB, free: 1458.4 MB)
20/04/13 13:57:30 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 25) in 2649824 ms on 178.62.200.211 (executor 1) (2/6)
20/04/13 13:57:39 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 26) in 2659066 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 14:00:52 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 27) in 2852486 ms on 178.62.210.13 (executor 2) (4/6)
20/04/13 14:21:55 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 6.0 (TID 29) in 4115403 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 14:22:13 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 6.0 (TID 28) in 4132698 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 14:22:13 INFO cluster.YarnClusterScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool 
20/04/13 14:22:13 INFO scheduler.DAGScheduler: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0) finished in 4132.761 s
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Job 4 finished: parquet at NativeMethodAccessorImpl.java:0, took 4132.765413 s
20/04/13 14:22:13 INFO datasources.FileFormatWriter: Write Job eb8e8609-0b69-450b-92e0-43d9421a45bc committed.
20/04/13 14:22:13 INFO datasources.FileFormatWriter: Finished processing stats for write job eb8e8609-0b69-450b-92e0-43d9421a45bc.
20/04/13 14:22:13 INFO datasources.InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.
20/04/13 14:22:13 INFO datasources.InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/04/13 14:22:13 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 14:22:13 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#62, None)) > 0)
20/04/13 14:22:13 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 14:22:13 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 337.3 KB, free 909.9 MB)
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 31.5 KB, free 909.9 MB)
20/04/13 14:22:13 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 178.62.200.211:34497 (size: 31.5 KB, free: 912.0 MB)
20/04/13 14:22:13 INFO spark.SparkContext: Created broadcast 11 from csv at NativeMethodAccessorImpl.java:0
20/04/13 14:22:13 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 14:22:13 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Got job 5 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[36] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 8.9 KB, free 909.9 MB)
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.6 KB, free 909.9 MB)
20/04/13 14:22:13 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 178.62.200.211:34497 (size: 4.6 KB, free: 912.0 MB)
20/04/13 14:22:13 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1163
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[36] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/04/13 14:22:13 INFO cluster.YarnClusterScheduler: Adding task set 7.0 with 1 tasks
20/04/13 14:22:13 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 31, 178.62.210.13, executor 2, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:13 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 178.62.210.13:37121 (size: 4.6 KB, free: 1458.4 MB)
20/04/13 14:22:13 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 178.62.210.13:37121 (size: 31.5 KB, free: 1458.3 MB)
20/04/13 14:22:13 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 31) in 104 ms on 178.62.210.13 (executor 2) (1/1)
20/04/13 14:22:13 INFO cluster.YarnClusterScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool 
20/04/13 14:22:13 INFO scheduler.DAGScheduler: ResultStage 7 (csv at NativeMethodAccessorImpl.java:0) finished in 0.148 s
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Job 5 finished: csv at NativeMethodAccessorImpl.java:0, took 0.150144 s
20/04/13 14:22:13 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 14:22:13 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 14:22:13 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 14:22:13 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 337.3 KB, free 909.5 MB)
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 31.5 KB, free 909.5 MB)
20/04/13 14:22:13 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 178.62.200.211:34497 (size: 31.5 KB, free: 912.0 MB)
20/04/13 14:22:13 INFO spark.SparkContext: Created broadcast 13 from csv at NativeMethodAccessorImpl.java:0
20/04/13 14:22:13 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 14:22:13 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Got job 6 (csv at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[42] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 15.0 KB, free 909.5 MB)
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.0 KB, free 909.5 MB)
20/04/13 14:22:13 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 178.62.200.211:34497 (size: 8.0 KB, free: 912.0 MB)
20/04/13 14:22:13 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1163
20/04/13 14:22:13 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 8 (MapPartitionsRDD[42] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 14:22:13 INFO cluster.YarnClusterScheduler: Adding task set 8.0 with 6 tasks
20/04/13 14:22:13 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 32, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:13 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 33, 178.62.200.211, executor 1, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:13 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 8.0 (TID 34, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:13 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 8.0 (TID 35, 178.62.200.211, executor 3, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:13 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 8.0 (TID 36, 178.62.200.211, executor 1, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:13 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 8.0 (TID 37, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:13 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 178.62.200.211:40635 (size: 8.0 KB, free: 1458.4 MB)
20/04/13 14:22:13 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 178.62.200.211:40647 (size: 8.0 KB, free: 1458.4 MB)
20/04/13 14:22:13 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 178.62.210.13:37121 (size: 8.0 KB, free: 1458.3 MB)
20/04/13 14:22:13 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 178.62.210.13:37121 (size: 31.5 KB, free: 1458.3 MB)
20/04/13 14:22:13 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 178.62.200.211:40635 (size: 31.5 KB, free: 1458.3 MB)
20/04/13 14:22:13 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 178.62.200.211:40647 (size: 31.5 KB, free: 1458.3 MB)
20/04/13 14:22:13 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 8.0 (TID 37) in 151 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 14:22:13 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 8.0 (TID 34) in 204 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 14:22:14 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 8.0 (TID 35) in 396 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 14:22:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 32) in 437 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 14:22:14 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 33) in 440 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 14:22:14 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 8.0 (TID 36) in 449 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 14:22:14 INFO cluster.YarnClusterScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool 
20/04/13 14:22:14 INFO scheduler.DAGScheduler: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0) finished in 0.456 s
20/04/13 14:22:14 INFO scheduler.DAGScheduler: Job 6 finished: csv at NativeMethodAccessorImpl.java:0, took 0.459040 s
20/04/13 14:22:14 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 14:22:14 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 14:22:14 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Sequence: string>
20/04/13 14:22:14 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 337.3 KB, free 909.2 MB)
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 31.5 KB, free 909.1 MB)
20/04/13 14:22:14 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 178.62.200.211:34497 (size: 31.5 KB, free: 912.0 MB)
20/04/13 14:22:14 INFO spark.SparkContext: Created broadcast 15 from rdd at CountVectorizer.scala:187
20/04/13 14:22:14 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 14:22:14 INFO spark.SparkContext: Starting job: count at CountVectorizer.scala:230
20/04/13 14:22:14 INFO scheduler.DAGScheduler: Registering RDD 52 (flatMap at CountVectorizer.scala:205) as input to shuffle 1
20/04/13 14:22:14 INFO scheduler.DAGScheduler: Got job 7 (count at CountVectorizer.scala:230) with 6 output partitions
20/04/13 14:22:14 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (count at CountVectorizer.scala:230)
20/04/13 14:22:14 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
20/04/13 14:22:14 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)
20/04/13 14:22:14 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[52] at flatMap at CountVectorizer.scala:205), which has no missing parents
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 27.4 KB, free 909.1 MB)
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.4 KB, free 909.1 MB)
20/04/13 14:22:14 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 178.62.200.211:34497 (size: 13.4 KB, free: 912.0 MB)
20/04/13 14:22:14 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1163
20/04/13 14:22:14 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[52] at flatMap at CountVectorizer.scala:205) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 14:22:14 INFO cluster.YarnClusterScheduler: Adding task set 9.0 with 6 tasks
20/04/13 14:22:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 38, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8258 bytes)
20/04/13 14:22:14 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 39, 178.62.200.211, executor 3, partition 1, NODE_LOCAL, 8258 bytes)
20/04/13 14:22:14 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 9.0 (TID 40, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 8258 bytes)
20/04/13 14:22:14 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 9.0 (TID 41, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 8258 bytes)
20/04/13 14:22:14 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 9.0 (TID 42, 178.62.200.211, executor 3, partition 4, NODE_LOCAL, 8258 bytes)
20/04/13 14:22:14 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 9.0 (TID 43, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 8258 bytes)
20/04/13 14:22:14 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 178.62.210.13:37121 (size: 13.4 KB, free: 1458.3 MB)
20/04/13 14:22:14 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 178.62.200.211:40647 (size: 13.4 KB, free: 1458.3 MB)
20/04/13 14:22:14 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 178.62.200.211:40635 (size: 13.4 KB, free: 1458.3 MB)
20/04/13 14:22:14 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 178.62.210.13:37121 (size: 31.5 KB, free: 1458.3 MB)
20/04/13 14:22:14 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 178.62.200.211:40635 (size: 31.5 KB, free: 1458.3 MB)
20/04/13 14:22:14 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 178.62.200.211:40647 (size: 31.5 KB, free: 1458.3 MB)
20/04/13 14:22:18 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 9.0 (TID 43) in 4212 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 14:22:23 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 9.0 (TID 40) in 9269 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 14:22:27 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 9.0 (TID 41) in 13625 ms on 178.62.200.211 (executor 1) (3/6)
20/04/13 14:22:28 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 9.0 (TID 42) in 13852 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 14:22:28 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 38) in 14500 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 14:22:28 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 39) in 14681 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 14:22:28 INFO cluster.YarnClusterScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool 
20/04/13 14:22:28 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (flatMap at CountVectorizer.scala:205) finished in 14.710 s
20/04/13 14:22:28 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/04/13 14:22:28 INFO scheduler.DAGScheduler: running: Set()
20/04/13 14:22:28 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)
20/04/13 14:22:28 INFO scheduler.DAGScheduler: failed: Set()
20/04/13 14:22:28 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[54] at map at CountVectorizer.scala:223), which has no missing parents
20/04/13 14:22:28 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 3.3 KB, free 909.1 MB)
20/04/13 14:22:28 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 1976.0 B, free 909.1 MB)
20/04/13 14:22:28 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 178.62.200.211:34497 (size: 1976.0 B, free: 911.9 MB)
20/04/13 14:22:28 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1163
20/04/13 14:22:28 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 10 (MapPartitionsRDD[54] at map at CountVectorizer.scala:223) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 14:22:28 INFO cluster.YarnClusterScheduler: Adding task set 10.0 with 6 tasks
20/04/13 14:22:28 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 44, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 7651 bytes)
20/04/13 14:22:28 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 45, 178.62.210.13, executor 2, partition 1, NODE_LOCAL, 7651 bytes)
20/04/13 14:22:28 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 10.0 (TID 46, 178.62.200.211, executor 1, partition 2, NODE_LOCAL, 7651 bytes)
20/04/13 14:22:28 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 10.0 (TID 47, 178.62.200.211, executor 3, partition 3, NODE_LOCAL, 7651 bytes)
20/04/13 14:22:28 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 10.0 (TID 48, 178.62.210.13, executor 2, partition 4, NODE_LOCAL, 7651 bytes)
20/04/13 14:22:28 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 10.0 (TID 49, 178.62.200.211, executor 1, partition 5, NODE_LOCAL, 7651 bytes)
20/04/13 14:22:28 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 178.62.210.13:37121 (size: 1976.0 B, free: 1458.2 MB)
20/04/13 14:22:28 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 178.62.200.211:40635 (size: 1976.0 B, free: 1458.3 MB)
20/04/13 14:22:28 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 178.62.200.211:40647 (size: 1976.0 B, free: 1458.3 MB)
20/04/13 14:22:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 178.62.200.211:33502
20/04/13 14:22:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 178.62.210.13:36772
20/04/13 14:22:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 178.62.200.211:33498
20/04/13 14:22:29 INFO storage.BlockManagerInfo: Added rdd_54_4 in memory on 178.62.210.13:37121 (size: 147.4 KB, free: 1458.1 MB)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 10.0 (TID 48) in 96 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 14:22:29 INFO storage.BlockManagerInfo: Added rdd_54_1 in memory on 178.62.210.13:37121 (size: 147.4 KB, free: 1458.0 MB)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 45) in 109 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 14:22:29 INFO storage.BlockManagerInfo: Added rdd_54_0 in memory on 178.62.200.211:40635 (size: 147.7 KB, free: 1458.1 MB)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 44) in 260 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 14:22:29 INFO storage.BlockManagerInfo: Added rdd_54_3 in memory on 178.62.200.211:40635 (size: 147.4 KB, free: 1458.0 MB)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 10.0 (TID 47) in 291 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 14:22:29 INFO storage.BlockManagerInfo: Added rdd_54_2 in memory on 178.62.200.211:40647 (size: 146.8 KB, free: 1458.1 MB)
20/04/13 14:22:29 INFO storage.BlockManagerInfo: Added rdd_54_5 in memory on 178.62.200.211:40647 (size: 147.2 KB, free: 1458.0 MB)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 10.0 (TID 46) in 308 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 10.0 (TID 49) in 327 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 14:22:29 INFO cluster.YarnClusterScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool 
20/04/13 14:22:29 INFO scheduler.DAGScheduler: ResultStage 10 (count at CountVectorizer.scala:230) finished in 0.334 s
20/04/13 14:22:29 INFO scheduler.DAGScheduler: Job 7 finished: count at CountVectorizer.scala:230, took 15.053054 s
20/04/13 14:22:29 INFO spark.SparkContext: Starting job: top at CountVectorizer.scala:233
20/04/13 14:22:29 INFO scheduler.DAGScheduler: Got job 8 (top at CountVectorizer.scala:233) with 6 output partitions
20/04/13 14:22:29 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (top at CountVectorizer.scala:233)
20/04/13 14:22:29 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
20/04/13 14:22:29 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 14:22:29 INFO scheduler.DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[55] at top at CountVectorizer.scala:233), which has no missing parents
20/04/13 14:22:29 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 4.2 KB, free 909.1 MB)
20/04/13 14:22:29 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.4 KB, free 909.1 MB)
20/04/13 14:22:29 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 178.62.200.211:34497 (size: 2.4 KB, free: 911.9 MB)
20/04/13 14:22:29 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1163
20/04/13 14:22:29 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 12 (MapPartitionsRDD[55] at top at CountVectorizer.scala:233) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 14:22:29 INFO cluster.YarnClusterScheduler: Adding task set 12.0 with 6 tasks
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 12.0 (TID 50, 178.62.200.211, executor 1, partition 2, PROCESS_LOCAL, 7651 bytes)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 51, 178.62.200.211, executor 3, partition 0, PROCESS_LOCAL, 7651 bytes)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 12.0 (TID 52, 178.62.210.13, executor 2, partition 1, PROCESS_LOCAL, 7651 bytes)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 12.0 (TID 53, 178.62.200.211, executor 1, partition 5, PROCESS_LOCAL, 7651 bytes)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 12.0 (TID 54, 178.62.200.211, executor 3, partition 3, PROCESS_LOCAL, 7651 bytes)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 12.0 (TID 55, 178.62.210.13, executor 2, partition 4, PROCESS_LOCAL, 7651 bytes)
20/04/13 14:22:29 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 178.62.200.211:40635 (size: 2.4 KB, free: 1458.0 MB)
20/04/13 14:22:29 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 178.62.210.13:37121 (size: 2.4 KB, free: 1458.0 MB)
20/04/13 14:22:29 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 178.62.200.211:40647 (size: 2.4 KB, free: 1458.0 MB)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 12.0 (TID 52) in 39 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 12.0 (TID 55) in 42 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 12.0 (TID 54) in 55 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 51) in 61 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 12.0 (TID 53) in 73 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 14:22:29 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 12.0 (TID 50) in 77 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 14:22:29 INFO cluster.YarnClusterScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool 
20/04/13 14:22:29 INFO scheduler.DAGScheduler: ResultStage 12 (top at CountVectorizer.scala:233) finished in 0.098 s
20/04/13 14:22:29 INFO scheduler.DAGScheduler: Job 8 finished: top at CountVectorizer.scala:233, took 0.104230 s
20/04/13 14:22:29 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 1186.5 KB, free 907.9 MB)
20/04/13 14:22:29 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 77.3 KB, free 907.8 MB)
20/04/13 14:22:29 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 178.62.200.211:34497 (size: 77.3 KB, free: 911.9 MB)
20/04/13 14:22:29 INFO spark.SparkContext: Created broadcast 19 from broadcast at CountVectorizer.scala:298
20/04/13 14:22:29 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 14:22:29 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 14:22:29 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Entry: string, Entry name: string, Sequence: string ... 1 more fields>
20/04/13 14:22:29 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 14:22:29 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:29 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:29 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:29 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:29 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:29 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:29 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:29 INFO codegen.CodeGenerator: Code generated in 69.798449 ms
20/04/13 14:22:29 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 337.3 KB, free 907.5 MB)
20/04/13 14:22:29 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 31.5 KB, free 907.5 MB)
20/04/13 14:22:29 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 178.62.200.211:34497 (size: 31.5 KB, free: 911.8 MB)
20/04/13 14:22:29 INFO spark.SparkContext: Created broadcast 20 from parquet at NativeMethodAccessorImpl.java:0
20/04/13 14:22:29 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 14:22:30 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
20/04/13 14:22:30 INFO scheduler.DAGScheduler: Got job 9 (parquet at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 14:22:30 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (parquet at NativeMethodAccessorImpl.java:0)
20/04/13 14:22:30 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 14:22:30 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 14:22:30 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[60] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 347.5 KB, free 907.1 MB)
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 124.8 KB, free 907.0 MB)
20/04/13 14:22:30 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 178.62.200.211:34497 (size: 124.8 KB, free: 911.7 MB)
20/04/13 14:22:30 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1163
20/04/13 14:22:30 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 13 (MapPartitionsRDD[60] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 14:22:30 INFO cluster.YarnClusterScheduler: Adding task set 13.0 with 6 tasks
20/04/13 14:22:30 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 56, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:30 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 57, 178.62.210.13, executor 2, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:30 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 13.0 (TID 58, 178.62.200.211, executor 3, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:30 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 13.0 (TID 59, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:30 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 13.0 (TID 60, 178.62.210.13, executor 2, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:30 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 13.0 (TID 61, 178.62.200.211, executor 3, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:30 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 178.62.210.13:37121 (size: 124.8 KB, free: 1457.8 MB)
20/04/13 14:22:30 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 178.62.200.211:40635 (size: 124.8 KB, free: 1457.9 MB)
20/04/13 14:22:30 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 178.62.200.211:40647 (size: 124.8 KB, free: 1457.9 MB)
20/04/13 14:22:30 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 178.62.210.13:37121 (size: 31.5 KB, free: 1457.8 MB)
20/04/13 14:22:30 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 178.62.200.211:40635 (size: 31.5 KB, free: 1457.8 MB)
20/04/13 14:22:30 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 178.62.210.13:37121 (size: 77.3 KB, free: 1457.7 MB)
20/04/13 14:22:30 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 178.62.200.211:40647 (size: 31.5 KB, free: 1457.8 MB)
20/04/13 14:22:30 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 178.62.200.211:40635 (size: 77.3 KB, free: 1457.8 MB)
20/04/13 14:22:30 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 178.62.200.211:40647 (size: 77.3 KB, free: 1457.8 MB)
20/04/13 14:22:37 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 13.0 (TID 61) in 7436 ms on 178.62.200.211 (executor 3) (1/6)
20/04/13 14:22:42 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 13.0 (TID 60) in 12735 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 14:22:43 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 57) in 13299 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 14:22:44 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 56) in 14519 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 14:22:45 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 13.0 (TID 59) in 15453 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 14:22:45 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 13.0 (TID 58) in 15885 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 14:22:45 INFO cluster.YarnClusterScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool 
20/04/13 14:22:45 INFO scheduler.DAGScheduler: ResultStage 13 (parquet at NativeMethodAccessorImpl.java:0) finished in 15.932 s
20/04/13 14:22:45 INFO scheduler.DAGScheduler: Job 9 finished: parquet at NativeMethodAccessorImpl.java:0, took 15.936814 s
20/04/13 14:22:46 INFO datasources.FileFormatWriter: Write Job ba807676-ba98-44a5-bd13-bbf9f397e6e2 committed.
20/04/13 14:22:46 INFO datasources.FileFormatWriter: Finished processing stats for write job ba807676-ba98-44a5-bd13-bbf9f397e6e2.
20/04/13 14:22:46 INFO datasources.InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/04/13 14:22:46 INFO datasources.InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/04/13 14:22:46 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 14:22:46 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#122, None)) > 0)
20/04/13 14:22:46 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 14:22:46 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 337.3 KB, free 906.7 MB)
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 31.5 KB, free 906.7 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 178.62.200.211:34497 (size: 31.5 KB, free: 911.7 MB)
20/04/13 14:22:46 INFO spark.SparkContext: Created broadcast 22 from csv at NativeMethodAccessorImpl.java:0
20/04/13 14:22:46 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 14:22:46 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Got job 10 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[66] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 8.9 KB, free 906.6 MB)
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.6 KB, free 906.6 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 178.62.200.211:34497 (size: 4.6 KB, free: 911.7 MB)
20/04/13 14:22:46 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1163
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[66] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/04/13 14:22:46 INFO cluster.YarnClusterScheduler: Adding task set 14.0 with 1 tasks
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 62, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 178.62.200.211:40635 (size: 4.6 KB, free: 1457.8 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 178.62.200.211:40635 (size: 31.5 KB, free: 1457.7 MB)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 62) in 37 ms on 178.62.200.211 (executor 3) (1/1)
20/04/13 14:22:46 INFO cluster.YarnClusterScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool 
20/04/13 14:22:46 INFO scheduler.DAGScheduler: ResultStage 14 (csv at NativeMethodAccessorImpl.java:0) finished in 0.045 s
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Job 10 finished: csv at NativeMethodAccessorImpl.java:0, took 0.048214 s
20/04/13 14:22:46 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 14:22:46 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 14:22:46 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 14:22:46 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 337.3 KB, free 906.3 MB)
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 31.5 KB, free 906.3 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 178.62.200.211:34497 (size: 31.5 KB, free: 911.7 MB)
20/04/13 14:22:46 INFO spark.SparkContext: Created broadcast 24 from csv at NativeMethodAccessorImpl.java:0
20/04/13 14:22:46 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 14:22:46 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Got job 11 (csv at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 15 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[72] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.0 KB, free 906.3 MB)
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.0 KB, free 906.3 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 178.62.200.211:34497 (size: 8.0 KB, free: 911.6 MB)
20/04/13 14:22:46 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1163
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 15 (MapPartitionsRDD[72] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 14:22:46 INFO cluster.YarnClusterScheduler: Adding task set 15.0 with 6 tasks
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 63, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 64, 178.62.210.13, executor 2, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 15.0 (TID 65, 178.62.200.211, executor 1, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 15.0 (TID 66, 178.62.200.211, executor 3, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 15.0 (TID 67, 178.62.210.13, executor 2, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 15.0 (TID 68, 178.62.200.211, executor 1, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 178.62.210.13:37121 (size: 8.0 KB, free: 1457.7 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 178.62.200.211:40635 (size: 8.0 KB, free: 1457.7 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 178.62.200.211:40647 (size: 8.0 KB, free: 1457.8 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 178.62.210.13:37121 (size: 31.5 KB, free: 1457.7 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 178.62.200.211:40635 (size: 31.5 KB, free: 1457.7 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 178.62.200.211:40647 (size: 31.5 KB, free: 1457.7 MB)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 64) in 139 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 15.0 (TID 67) in 148 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 15.0 (TID 68) in 162 ms on 178.62.200.211 (executor 1) (3/6)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 63) in 177 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 15.0 (TID 66) in 208 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 15.0 (TID 65) in 214 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 14:22:46 INFO cluster.YarnClusterScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool 
20/04/13 14:22:46 INFO scheduler.DAGScheduler: ResultStage 15 (csv at NativeMethodAccessorImpl.java:0) finished in 0.219 s
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Job 11 finished: csv at NativeMethodAccessorImpl.java:0, took 0.222321 s
20/04/13 14:22:46 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 14:22:46 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 14:22:46 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Sequence: string>
20/04/13 14:22:46 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 337.3 KB, free 905.9 MB)
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 31.5 KB, free 905.9 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 178.62.200.211:34497 (size: 31.5 KB, free: 911.6 MB)
20/04/13 14:22:46 INFO spark.SparkContext: Created broadcast 26 from rdd at CountVectorizer.scala:187
20/04/13 14:22:46 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 14:22:46 INFO spark.SparkContext: Starting job: count at CountVectorizer.scala:230
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Registering RDD 82 (flatMap at CountVectorizer.scala:205) as input to shuffle 2
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Got job 12 (count at CountVectorizer.scala:230) with 6 output partitions
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (count at CountVectorizer.scala:230)
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 16)
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[82] at flatMap at CountVectorizer.scala:205), which has no missing parents
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 27.4 KB, free 905.9 MB)
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 13.4 KB, free 905.9 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 178.62.200.211:34497 (size: 13.4 KB, free: 911.6 MB)
20/04/13 14:22:46 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1163
20/04/13 14:22:46 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[82] at flatMap at CountVectorizer.scala:205) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 14:22:46 INFO cluster.YarnClusterScheduler: Adding task set 16.0 with 6 tasks
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 69, 178.62.210.13, executor 2, partition 0, NODE_LOCAL, 8258 bytes)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 16.0 (TID 70, 178.62.200.211, executor 1, partition 1, NODE_LOCAL, 8258 bytes)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 16.0 (TID 71, 178.62.200.211, executor 3, partition 2, NODE_LOCAL, 8258 bytes)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 16.0 (TID 72, 178.62.210.13, executor 2, partition 3, NODE_LOCAL, 8258 bytes)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 16.0 (TID 73, 178.62.200.211, executor 1, partition 4, NODE_LOCAL, 8258 bytes)
20/04/13 14:22:46 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 16.0 (TID 74, 178.62.200.211, executor 3, partition 5, NODE_LOCAL, 8258 bytes)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 178.62.210.13:37121 (size: 13.4 KB, free: 1457.7 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 178.62.200.211:40635 (size: 13.4 KB, free: 1457.7 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 178.62.200.211:40647 (size: 13.4 KB, free: 1457.7 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 178.62.210.13:37121 (size: 31.5 KB, free: 1457.6 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 178.62.200.211:40647 (size: 31.5 KB, free: 1457.7 MB)
20/04/13 14:22:46 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 178.62.200.211:40635 (size: 31.5 KB, free: 1457.6 MB)
20/04/13 14:22:52 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 16.0 (TID 74) in 5691 ms on 178.62.200.211 (executor 3) (1/6)
20/04/13 14:22:57 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 16.0 (TID 72) in 10597 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 14:22:57 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 16.0 (TID 70) in 11105 ms on 178.62.200.211 (executor 1) (3/6)
20/04/13 14:22:57 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 69) in 11119 ms on 178.62.210.13 (executor 2) (4/6)
20/04/13 14:22:58 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 16.0 (TID 73) in 11997 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 16.0 (TID 71) in 12585 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 14:22:59 INFO cluster.YarnClusterScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool 
20/04/13 14:22:59 INFO scheduler.DAGScheduler: ShuffleMapStage 16 (flatMap at CountVectorizer.scala:205) finished in 12.597 s
20/04/13 14:22:59 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/04/13 14:22:59 INFO scheduler.DAGScheduler: running: Set()
20/04/13 14:22:59 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 17)
20/04/13 14:22:59 INFO scheduler.DAGScheduler: failed: Set()
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[84] at map at CountVectorizer.scala:223), which has no missing parents
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 3.3 KB, free 905.9 MB)
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 1977.0 B, free 905.9 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 178.62.200.211:34497 (size: 1977.0 B, free: 911.6 MB)
20/04/13 14:22:59 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1163
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 17 (MapPartitionsRDD[84] at map at CountVectorizer.scala:223) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 14:22:59 INFO cluster.YarnClusterScheduler: Adding task set 17.0 with 6 tasks
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 75, 178.62.210.13, executor 2, partition 0, NODE_LOCAL, 7651 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 76, 178.62.200.211, executor 3, partition 1, NODE_LOCAL, 7651 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 17.0 (TID 77, 178.62.200.211, executor 1, partition 2, NODE_LOCAL, 7651 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 17.0 (TID 78, 178.62.210.13, executor 2, partition 3, NODE_LOCAL, 7651 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 17.0 (TID 79, 178.62.200.211, executor 3, partition 4, NODE_LOCAL, 7651 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 17.0 (TID 80, 178.62.200.211, executor 1, partition 5, NODE_LOCAL, 7651 bytes)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 178.62.210.13:37121 (size: 1977.0 B, free: 1457.6 MB)
20/04/13 14:22:59 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 178.62.210.13:36772
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 178.62.200.211:40647 (size: 1977.0 B, free: 1457.7 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 178.62.200.211:40635 (size: 1977.0 B, free: 1457.6 MB)
20/04/13 14:22:59 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 178.62.200.211:33498
20/04/13 14:22:59 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 178.62.200.211:33502
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added rdd_84_0 in memory on 178.62.210.13:37121 (size: 147.7 KB, free: 1457.5 MB)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 75) in 84 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added rdd_84_3 in memory on 178.62.210.13:37121 (size: 146.5 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 17.0 (TID 78) in 92 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added rdd_84_2 in memory on 178.62.200.211:40647 (size: 146.8 KB, free: 1457.5 MB)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 17.0 (TID 77) in 161 ms on 178.62.200.211 (executor 1) (3/6)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added rdd_84_1 in memory on 178.62.200.211:40635 (size: 147.4 KB, free: 1457.5 MB)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 76) in 177 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added rdd_84_5 in memory on 178.62.200.211:40647 (size: 147.2 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 17.0 (TID 80) in 183 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added rdd_84_4 in memory on 178.62.200.211:40635 (size: 147.4 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 17.0 (TID 79) in 196 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 14:22:59 INFO cluster.YarnClusterScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool 
20/04/13 14:22:59 INFO scheduler.DAGScheduler: ResultStage 17 (count at CountVectorizer.scala:230) finished in 0.202 s
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Job 12 finished: count at CountVectorizer.scala:230, took 12.811587 s
20/04/13 14:22:59 INFO spark.SparkContext: Starting job: top at CountVectorizer.scala:233
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Got job 13 (top at CountVectorizer.scala:233) with 6 output partitions
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (top at CountVectorizer.scala:233)
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[85] at top at CountVectorizer.scala:233), which has no missing parents
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 4.2 KB, free 905.8 MB)
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.4 KB, free 905.8 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 178.62.200.211:34497 (size: 2.4 KB, free: 911.6 MB)
20/04/13 14:22:59 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1163
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 19 (MapPartitionsRDD[85] at top at CountVectorizer.scala:233) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 14:22:59 INFO cluster.YarnClusterScheduler: Adding task set 19.0 with 6 tasks
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 19.0 (TID 81, 178.62.200.211, executor 3, partition 1, PROCESS_LOCAL, 7651 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 19.0 (TID 82, 178.62.200.211, executor 1, partition 2, PROCESS_LOCAL, 7651 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 83, 178.62.210.13, executor 2, partition 0, PROCESS_LOCAL, 7651 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 19.0 (TID 84, 178.62.200.211, executor 3, partition 4, PROCESS_LOCAL, 7651 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 19.0 (TID 85, 178.62.200.211, executor 1, partition 5, PROCESS_LOCAL, 7651 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 19.0 (TID 86, 178.62.210.13, executor 2, partition 3, PROCESS_LOCAL, 7651 bytes)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 178.62.210.13:37121 (size: 2.4 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 178.62.200.211:40635 (size: 2.4 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 178.62.200.211:40647 (size: 2.4 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 19.0 (TID 86) in 26 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 83) in 35 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 19.0 (TID 84) in 38 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 19.0 (TID 82) in 50 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 19.0 (TID 85) in 51 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 19.0 (TID 81) in 61 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 14:22:59 INFO cluster.YarnClusterScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool 
20/04/13 14:22:59 INFO scheduler.DAGScheduler: ResultStage 19 (top at CountVectorizer.scala:233) finished in 0.066 s
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Job 13 finished: top at CountVectorizer.scala:233, took 0.071881 s
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 1186.5 KB, free 904.7 MB)
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 77.2 KB, free 904.6 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 178.62.200.211:34497 (size: 77.2 KB, free: 911.5 MB)
20/04/13 14:22:59 INFO spark.SparkContext: Created broadcast 30 from broadcast at CountVectorizer.scala:298
20/04/13 14:22:59 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 14:22:59 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 14:22:59 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Entry: string, Entry name: string, Sequence: string ... 1 more fields>
20/04/13 14:22:59 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 14:22:59 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 337.3 KB, free 904.3 MB)
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 31.5 KB, free 904.3 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 178.62.200.211:34497 (size: 31.5 KB, free: 911.5 MB)
20/04/13 14:22:59 INFO spark.SparkContext: Created broadcast 31 from parquet at NativeMethodAccessorImpl.java:0
20/04/13 14:22:59 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 14:22:59 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Got job 14 (parquet at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0)
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[93] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 178.62.210.13:37121 in memory (size: 124.8 KB, free: 1457.5 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 178.62.200.211:40635 in memory (size: 124.8 KB, free: 1457.5 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 178.62.200.211:40647 in memory (size: 124.8 KB, free: 1457.5 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 178.62.200.211:34497 in memory (size: 124.8 KB, free: 911.6 MB)
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 434
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 472
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 178.62.210.13:37121 in memory (size: 1977.0 B, free: 1457.5 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 178.62.200.211:34497 in memory (size: 1977.0 B, free: 911.6 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 178.62.200.211:40647 in memory (size: 1977.0 B, free: 1457.5 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 178.62.200.211:40635 in memory (size: 1977.0 B, free: 1457.5 MB)
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 352.0 KB, free 904.4 MB)
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 127.2 KB, free 904.3 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 178.62.200.211:34497 (size: 127.2 KB, free: 911.5 MB)
20/04/13 14:22:59 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1163
20/04/13 14:22:59 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 20 (MapPartitionsRDD[93] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 14:22:59 INFO cluster.YarnClusterScheduler: Adding task set 20.0 with 6 tasks
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 87, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 20.0 (TID 88, 178.62.200.211, executor 3, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 20.0 (TID 89, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 20.0 (TID 90, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 20.0 (TID 91, 178.62.200.211, executor 3, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:59 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 20.0 (TID 92, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 178.62.200.211:40635 in memory (size: 31.5 KB, free: 1457.5 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 178.62.200.211:34497 in memory (size: 31.5 KB, free: 911.5 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 178.62.210.13:37121 (size: 127.2 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 178.62.200.211:40647 (size: 127.2 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 178.62.200.211:40635 (size: 127.2 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 187
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 359
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 276
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 399
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 189
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 356
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 496
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 279
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 436
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 449
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 386
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 397
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 426
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 193
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 277
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 308
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned shuffle 1
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 349
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 442
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 403
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 265
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 246
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 465
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 180
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 178.62.210.13:37121 in memory (size: 31.5 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 178.62.200.211:34497 in memory (size: 31.5 KB, free: 911.6 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 178.62.210.13:37121 (size: 31.5 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 258
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 312
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 338
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 444
20/04/13 14:22:59 INFO spark.ContextCleaner: Cleaned accumulator 487
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 178.62.210.13:37121 in memory (size: 4.6 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 178.62.200.211:40647 (size: 31.5 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 178.62.200.211:40635 (size: 31.5 KB, free: 1457.4 MB)
20/04/13 14:22:59 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 178.62.200.211:34497 in memory (size: 4.6 KB, free: 911.6 MB)
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 178.62.210.13:37121 (size: 77.2 KB, free: 1457.3 MB)
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 178.62.200.211:40635 (size: 77.2 KB, free: 1457.3 MB)
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 178.62.200.211:40647 (size: 77.2 KB, free: 1457.3 MB)
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 192
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 366
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 236
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 418
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 441
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 190
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 321
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 475
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 287
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 370
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 407
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 241
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 194
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 278
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 230
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 340
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 231
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 271
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 402
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 234
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 330
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 489
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 360
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 406
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 495
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 203
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 350
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 295
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 178.62.210.13:37121 in memory (size: 31.5 KB, free: 1457.3 MB)
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 178.62.200.211:40647 in memory (size: 31.5 KB, free: 1457.3 MB)
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 178.62.200.211:40635 in memory (size: 31.5 KB, free: 1457.3 MB)
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 178.62.200.211:34497 in memory (size: 31.5 KB, free: 911.6 MB)
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 306
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 348
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 469
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 311
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 198
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 275
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 264
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 266
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 477
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 462
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 191
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 205
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 188
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 383
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 316
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 335
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 296
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 178.62.200.211:40647 in memory (size: 31.5 KB, free: 1457.3 MB)
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 178.62.200.211:34497 in memory (size: 31.5 KB, free: 911.6 MB)
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 178.62.210.13:37121 in memory (size: 31.5 KB, free: 1457.3 MB)
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 178.62.200.211:40635 in memory (size: 31.5 KB, free: 1457.3 MB)
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 318
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 394
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 249
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 358
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 364
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 400
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 352
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 384
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 280
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 228
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 363
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 461
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 255
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 209
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 291
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 482
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 479
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 457
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 342
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 309
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 447
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 298
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 334
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 460
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 327
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 438
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 343
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 375
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 435
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 232
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 486
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 243
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 320
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 446
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned shuffle 2
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 226
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 317
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 467
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 420
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 491
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 310
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 357
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 216
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 372
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 411
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 301
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 217
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 245
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 247
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 405
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 206
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 244
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 450
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 354
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 430
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 408
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 239
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 388
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 197
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 284
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 345
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 490
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 183
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 416
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 260
20/04/13 14:23:00 INFO spark.ContextCleaner: Cleaned accumulator 195
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 178.62.200.211:40647 in memory (size: 31.5 KB, free: 1457.4 MB)
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 178.62.200.211:34497 in memory (size: 31.5 KB, free: 911.6 MB)
20/04/13 14:23:00 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 178.62.200.211:40635 in memory (size: 31.5 KB, free: 1457.4 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 178.62.210.13:37121 in memory (size: 31.5 KB, free: 1457.4 MB)
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 307
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 419
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 351
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 483
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 451
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 208
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 204
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 396
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 178.62.200.211:34497 in memory (size: 31.5 KB, free: 911.7 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 178.62.210.13:37121 in memory (size: 31.5 KB, free: 1457.4 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 178.62.200.211:40647 in memory (size: 31.5 KB, free: 1457.4 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 178.62.200.211:40635 in memory (size: 31.5 KB, free: 1457.4 MB)
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 404
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 494
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 476
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 293
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 389
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 439
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 478
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 178.62.200.211:34497 in memory (size: 8.0 KB, free: 911.7 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 178.62.210.13:37121 in memory (size: 8.0 KB, free: 1457.4 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 178.62.200.211:40635 in memory (size: 8.0 KB, free: 1457.4 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 178.62.200.211:40647 in memory (size: 8.0 KB, free: 1457.4 MB)
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 315
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 178.62.200.211:34497 in memory (size: 31.5 KB, free: 911.7 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 178.62.210.13:37121 in memory (size: 31.5 KB, free: 1457.4 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 178.62.200.211:40635 in memory (size: 31.5 KB, free: 1457.4 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 178.62.200.211:40647 in memory (size: 31.5 KB, free: 1457.4 MB)
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 424
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 431
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 425
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 443
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 305
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 367
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 225
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 347
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 432
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 353
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 458
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 282
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 178.62.210.13:37121 in memory (size: 13.4 KB, free: 1457.5 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 178.62.200.211:34497 in memory (size: 13.4 KB, free: 911.7 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 178.62.200.211:40635 in memory (size: 13.4 KB, free: 1457.5 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 178.62.200.211:40647 in memory (size: 13.4 KB, free: 1457.5 MB)
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 401
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 440
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 480
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 235
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 184
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 422
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 186
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 214
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 470
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 466
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 250
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 463
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 473
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 211
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 324
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 256
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 464
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 428
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 292
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 326
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 378
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 221
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 337
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 415
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 488
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 417
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 272
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 382
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 427
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 492
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 455
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 303
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 294
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 374
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 493
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 339
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 368
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 273
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 268
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 474
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 365
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 215
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 328
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 224
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 380
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 253
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 288
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 269
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 254
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 323
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 199
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 238
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 369
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 196
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 257
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 200
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 178.62.200.211:34497 in memory (size: 2.4 KB, free: 911.7 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 178.62.200.211:40647 in memory (size: 2.4 KB, free: 1457.5 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 178.62.210.13:37121 in memory (size: 2.4 KB, free: 1457.5 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 178.62.200.211:40635 in memory (size: 2.4 KB, free: 1457.5 MB)
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 179
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 331
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 376
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 202
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 437
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 314
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 229
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 319
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 286
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 240
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 371
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 201
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 267
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 448
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 456
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 336
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 412
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 263
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 423
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 223
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 481
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 322
20/04/13 14:23:01 INFO storage.BlockManager: Removing RDD 84
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned RDD 84
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 452
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 182
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 242
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 178.62.200.211:34497 in memory (size: 8.0 KB, free: 911.7 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 178.62.210.13:37121 in memory (size: 8.0 KB, free: 1457.8 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 178.62.200.211:40635 in memory (size: 8.0 KB, free: 1457.8 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 178.62.200.211:40647 in memory (size: 8.0 KB, free: 1457.8 MB)
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 261
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 429
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 361
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 210
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 398
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 290
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 300
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 281
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 299
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 222
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 251
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 391
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 185
20/04/13 14:23:01 INFO storage.BlockManager: Removing RDD 54
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned RDD 54
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 219
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 218
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 270
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 313
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 485
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 325
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 237
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 395
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 459
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 333
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 393
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 262
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 259
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 297
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 283
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 385
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 387
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 390
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 220
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 302
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 373
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 252
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 181
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 355
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 178.62.200.211:34497 in memory (size: 4.6 KB, free: 911.7 MB)
20/04/13 14:23:01 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 178.62.200.211:40635 in memory (size: 4.6 KB, free: 1458.0 MB)
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 413
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 454
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 274
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 332
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 471
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 329
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 410
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 207
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 414
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 341
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 344
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 213
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 433
20/04/13 14:23:01 INFO spark.ContextCleaner: Cleaned accumulator 468
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 178.62.210.13:37121 in memory (size: 77.3 KB, free: 1458.1 MB)
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 178.62.200.211:40635 in memory (size: 77.3 KB, free: 1458.1 MB)
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 178.62.200.211:34497 in memory (size: 77.3 KB, free: 911.8 MB)
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 178.62.200.211:40647 in memory (size: 77.3 KB, free: 1458.1 MB)
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 212
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 484
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 445
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 285
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 346
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 453
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 379
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 377
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 178.62.200.211:40635 in memory (size: 1976.0 B, free: 1458.1 MB)
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 178.62.210.13:37121 in memory (size: 1976.0 B, free: 1458.1 MB)
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 178.62.200.211:34497 in memory (size: 1976.0 B, free: 911.8 MB)
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 178.62.200.211:40647 in memory (size: 1976.0 B, free: 1458.1 MB)
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 421
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 227
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 392
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 381
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 409
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 497
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 233
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 248
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 178.62.200.211:34497 in memory (size: 2.4 KB, free: 911.8 MB)
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 178.62.200.211:40635 in memory (size: 2.4 KB, free: 1458.1 MB)
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 178.62.200.211:40647 in memory (size: 2.4 KB, free: 1458.1 MB)
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 178.62.210.13:37121 in memory (size: 2.4 KB, free: 1458.1 MB)
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 289
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 178.62.200.211:34497 in memory (size: 13.4 KB, free: 911.8 MB)
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 178.62.200.211:40635 in memory (size: 13.4 KB, free: 1458.1 MB)
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 178.62.210.13:37121 in memory (size: 13.4 KB, free: 1458.1 MB)
20/04/13 14:23:02 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 178.62.200.211:40647 in memory (size: 13.4 KB, free: 1458.1 MB)
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 304
20/04/13 14:23:02 INFO spark.ContextCleaner: Cleaned accumulator 362
20/04/13 14:26:32 ERROR yarn.ApplicationMaster: User application exited with status 143
20/04/13 14:26:32 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 143, (reason: User application exited with status 143)
20/04/13 14:26:32 ERROR yarn.ApplicationMaster: RECEIVED SIGNAL TERM
20/04/13 14:26:32 INFO spark.SparkContext: Invoking stop() from shutdown hook
20/04/13 14:26:32 INFO server.AbstractConnector: Stopped Spark@259ce822{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
20/04/13 14:26:32 INFO ui.SparkUI: Stopped Spark web UI at http://178.62.200.211:39083
20/04/13 14:26:32 INFO scheduler.DAGScheduler: Job 14 failed: parquet at NativeMethodAccessorImpl.java:0, took 213.135224 s
20/04/13 14:26:32 INFO scheduler.DAGScheduler: ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0) failed in 213.133 s due to Stage cancelled because SparkContext was shut down
20/04/13 14:26:32 ERROR datasources.FileFormatWriter: Aborting job 143343f0-e572-42a7-a559-8339be44b8b0.
org.apache.spark.SparkException: Job 14 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:933)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:931)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:931)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2130)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2043)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
20/04/13 14:26:32 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s).
20/04/13 14:26:32 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors
20/04/13 14:26:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/04/13 14:26:32 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/04/13 14:26:32 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/04/13 14:26:33 INFO memory.MemoryStore: MemoryStore cleared
20/04/13 14:26:33 INFO storage.BlockManager: BlockManager stopped
20/04/13 14:26:33 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
20/04/13 14:26:33 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/04/13 14:26:33 INFO spark.SparkContext: Successfully stopped SparkContext
20/04/13 14:26:33 INFO util.ShutdownHookManager: Shutdown hook called
20/04/13 14:26:33 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0006/spark-e520d379-4a32-49d2-a760-941550b0d72f
20/04/13 14:26:33 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0006/spark-e520d379-4a32-49d2-a760-941550b0d72f/pyspark-dc1234c3-1176-41a5-86ac-db7f8ea6fe21
End of LogType:stderr

LogType:stdout
Log Upload Time:Mon Apr 13 14:26:33 +0000 2020
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_1586780871303_0006_01_000003 on 178.62.210.13_35211
==========================================================================
LogType:stderr
Log Upload Time:Mon Apr 13 14:26:33 +0000 2020
LogLength:77652
Log Contents:
20/04/13 13:12:47 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 19533@178.62.210.13
20/04/13 13:12:47 INFO util.SignalUtils: Registered signal handler for TERM
20/04/13 13:12:47 INFO util.SignalUtils: Registered signal handler for HUP
20/04/13 13:12:47 INFO util.SignalUtils: Registered signal handler for INT
20/04/13 13:12:48 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 13:12:48 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 13:12:48 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 13:12:48 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 13:12:48 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 13:12:48 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:42247 after 67 ms (0 ms spent in bootstraps)
20/04/13 13:12:48 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 13:12:48 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 13:12:48 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 13:12:48 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 13:12:48 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 13:12:48 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:42247 after 4 ms (0 ms spent in bootstraps)
20/04/13 13:12:49 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0006/blockmgr-d1f86cd6-2855-41c3-b53c-b81c592ae29e
20/04/13 13:12:49 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
20/04/13 13:12:49 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.200.211:42247
20/04/13 13:12:49 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/13 13:12:49 INFO executor.Executor: Starting executor ID 2 on host 178.62.210.13
20/04/13 13:12:49 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37121.
20/04/13 13:12:49 INFO netty.NettyBlockTransferService: Server created on 178.62.210.13:37121
20/04/13 13:12:49 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/13 13:12:49 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(2, 178.62.210.13, 37121, None)
20/04/13 13:12:49 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(2, 178.62.210.13, 37121, None)
20/04/13 13:12:49 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(2, 178.62.210.13, 37121, None)
20/04/13 13:12:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3
20/04/13 13:12:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6
20/04/13 13:12:56 INFO executor.Executor: Running task 5.0 in stage 1.0 (TID 6)
20/04/13 13:12:56 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 3)
20/04/13 13:12:56 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3
20/04/13 13:12:56 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:40635 after 3 ms (0 ms spent in bootstraps)
20/04/13 13:12:56 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1458.6 MB)
20/04/13 13:12:56 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 164 ms
20/04/13 13:12:57 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.0 KB, free 1458.6 MB)
20/04/13 13:12:57 INFO codegen.CodeGenerator: Code generated in 200.062892 ms
20/04/13 13:12:58 INFO codegen.CodeGenerator: Code generated in 18.907969 ms
20/04/13 13:12:58 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 13:12:58 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 13:12:58 INFO codegen.CodeGenerator: Code generated in 13.829335 ms
20/04/13 13:12:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
20/04/13 13:12:58 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:34497 after 2 ms (0 ms spent in bootstraps)
20/04/13 13:12:58 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.5 MB)
20/04/13 13:12:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 37 ms
20/04/13 13:12:59 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 536.2 KB, free 1458.0 MB)
20/04/13 13:13:00 INFO executor.Executor: Finished task 5.0 in stage 1.0 (TID 6). 1640 bytes result sent to driver
20/04/13 13:13:00 INFO executor.Executor: Finished task 2.0 in stage 1.0 (TID 3). 1597 bytes result sent to driver
20/04/13 13:13:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7
20/04/13 13:13:01 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 7)
20/04/13 13:13:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10
20/04/13 13:13:01 INFO executor.Executor: Running task 3.0 in stage 2.0 (TID 10)
20/04/13 13:13:01 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
20/04/13 13:13:01 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1458.0 MB)
20/04/13 13:13:01 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 13 ms
20/04/13 13:13:01 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 27.4 KB, free 1458.0 MB)
20/04/13 13:13:02 INFO codegen.CodeGenerator: Code generated in 13.578353 ms
20/04/13 13:13:02 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 13:13:02 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 13:13:02 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
20/04/13 13:13:02 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.0 MB)
20/04/13 13:13:02 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 16 ms
20/04/13 13:13:02 INFO codegen.CodeGenerator: Code generated in 26.822553 ms
20/04/13 13:13:02 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 536.2 KB, free 1457.4 MB)
20/04/13 13:13:02 INFO codegen.CodeGenerator: Code generated in 46.049083 ms
20/04/13 13:13:02 INFO codegen.CodeGenerator: Code generated in 60.256882 ms
20/04/13 13:13:14 INFO python.PythonUDFRunner: Times: total = 4077, boot = 295, init = 216, finish = 3566
20/04/13 13:13:14 INFO executor.Executor: Finished task 3.0 in stage 2.0 (TID 10). 2463 bytes result sent to driver
20/04/13 13:13:15 INFO python.PythonUDFRunner: Times: total = 4171, boot = 291, init = 183, finish = 3697
20/04/13 13:13:15 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 7). 2420 bytes result sent to driver
20/04/13 13:13:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14
20/04/13 13:13:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17
20/04/13 13:13:18 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 14)
20/04/13 13:13:18 INFO executor.Executor: Running task 4.0 in stage 3.0 (TID 17)
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/13 13:13:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/13 13:13:18 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1976.0 B, free 1457.4 MB)
20/04/13 13:13:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 19 ms
20/04/13 13:13:18 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 1457.4 MB)
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.200.211:42247)
20/04/13 13:13:18 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 13:13:18 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 13:13:18 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 13:13:18 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:40647 after 2 ms (0 ms spent in bootstraps)
20/04/13 13:13:18 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 15 ms
20/04/13 13:13:18 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 15 ms
20/04/13 13:13:18 INFO memory.MemoryStore: Block rdd_21_4 stored as values in memory (estimated size 147.7 KB, free 1457.3 MB)
20/04/13 13:13:18 INFO memory.MemoryStore: Block rdd_21_1 stored as values in memory (estimated size 147.4 KB, free 1457.1 MB)
20/04/13 13:13:18 INFO executor.Executor: Finished task 4.0 in stage 3.0 (TID 17). 1176 bytes result sent to driver
20/04/13 13:13:18 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 14). 1176 bytes result sent to driver
20/04/13 13:13:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19
20/04/13 13:13:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22
20/04/13 13:13:19 INFO executor.Executor: Running task 4.0 in stage 5.0 (TID 22)
20/04/13 13:13:19 INFO executor.Executor: Running task 1.0 in stage 5.0 (TID 19)
20/04/13 13:13:19 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/13 13:13:19 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1457.1 MB)
20/04/13 13:13:19 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 15 ms
20/04/13 13:13:19 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.2 KB, free 1457.1 MB)
20/04/13 13:13:19 INFO storage.BlockManager: Found block rdd_21_4 locally
20/04/13 13:13:19 INFO storage.BlockManager: Found block rdd_21_1 locally
20/04/13 13:13:19 INFO executor.Executor: Finished task 1.0 in stage 5.0 (TID 19). 40772 bytes result sent to driver
20/04/13 13:13:19 INFO executor.Executor: Finished task 4.0 in stage 5.0 (TID 22). 40923 bytes result sent to driver
20/04/13 13:13:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 27
20/04/13 13:13:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 30
20/04/13 13:13:20 INFO executor.Executor: Running task 2.0 in stage 6.0 (TID 27)
20/04/13 13:13:20 INFO executor.Executor: Running task 5.0 in stage 6.0 (TID 30)
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 127.2 KB, free 1457.0 MB)
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 25 ms
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 352.0 KB, free 1456.7 MB)
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 11.041517 ms
20/04/13 13:13:20 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 13:13:20 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 23.819371 ms
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.6 MB)
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 14 ms
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 42.684733 ms
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 536.2 KB, free 1456.1 MB)
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 23.884636 ms
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 18.777656 ms
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 104.552636 ms
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 77.3 KB, free 1328.0 MB)
20/04/13 13:13:20 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 41 ms
20/04/13 13:13:20 INFO codegen.CodeGenerator: Code generated in 68.273449 ms
20/04/13 13:13:20 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:20 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:20 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:20 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:20 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:20 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:20 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:20 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 13:13:20 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:20 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 13:13:20 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:20 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 13:13:20 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 987.3 KB, free 1327.1 MB)
20/04/13 13:13:39 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 13:13:39 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 13:13:39 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 13:13:39 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 13:13:39 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 13:13:39 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 13:13:39 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 13:13:39 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 13:13:39 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 13:13:39 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 13:13:39 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 13:13:39 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 13:13:39 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 13:13:40 INFO compress.CodecPool: Got brand-new compressor [.snappy]
20/04/13 13:13:41 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 13:13:41 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 13:13:41 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 13:13:41 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 13:13:41 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 13:13:41 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 13:13:41 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 13:13:41 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 13:13:41 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 13:13:41 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 13:13:41 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 13:13:41 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 13:13:41 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 13:13:41 INFO compress.CodecPool: Got brand-new compressor [.snappy]
20/04/13 13:19:38 INFO storage.BlockManager: Removing RDD 21
20/04/13 13:21:19 INFO python.PythonUDFRunner: Times: total = 356, boot = -14225, init = 14335, finish = 246
20/04/13 13:33:47 INFO python.PythonUDFRunner: Times: total = 1226748, boot = 15, init = 813, finish = 1225920
20/04/13 13:33:47 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 2607624
20/04/13 13:33:47 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413131320_0006_m_000005_30' to hdfs://178.62.208.209:9000/data/df_3-shingles_dense-binary-vectors.parquet/_temporary/0/task_20200413131320_0006_m_000005
20/04/13 13:33:47 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413131320_0006_m_000005_30: Committed
20/04/13 13:33:47 INFO executor.Executor: Finished task 5.0 in stage 6.0 (TID 30). 3331 bytes result sent to driver
20/04/13 13:50:40 INFO python.PythonUDFRunner: Times: total = 21832, boot = -14145, init = 14258, finish = 21719
20/04/13 14:00:52 INFO python.PythonUDFRunner: Times: total = 2852035, boot = 36, init = 896, finish = 2851103
20/04/13 14:00:52 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7319853
20/04/13 14:00:52 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413131320_0006_m_000002_27' to hdfs://178.62.208.209:9000/data/df_3-shingles_dense-binary-vectors.parquet/_temporary/0/task_20200413131320_0006_m_000002
20/04/13 14:00:52 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413131320_0006_m_000002_27: Committed
20/04/13 14:00:52 INFO executor.Executor: Finished task 2.0 in stage 6.0 (TID 27). 3288 bytes result sent to driver
20/04/13 14:22:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 31
20/04/13 14:22:13 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 31)
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1456.5 MB)
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 11 ms
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 8.9 KB, free 1456.5 MB)
20/04/13 14:22:13 INFO codegen.CodeGenerator: Code generated in 29.974604 ms
20/04/13 14:22:13 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.5 MB)
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 7 ms
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 536.2 KB, free 1456.0 MB)
20/04/13 14:22:13 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 31). 1375 bytes result sent to driver
20/04/13 14:22:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 34
20/04/13 14:22:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 37
20/04/13 14:22:13 INFO executor.Executor: Running task 2.0 in stage 8.0 (TID 34)
20/04/13 14:22:13 INFO executor.Executor: Running task 5.0 in stage 8.0 (TID 37)
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1456.0 MB)
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 10 ms
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 15.0 KB, free 1455.9 MB)
20/04/13 14:22:13 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13
20/04/13 14:22:13 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1455.9 MB)
20/04/13 14:22:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 13 ms
20/04/13 14:22:13 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 536.2 KB, free 1455.4 MB)
20/04/13 14:22:13 INFO executor.Executor: Finished task 5.0 in stage 8.0 (TID 37). 1597 bytes result sent to driver
20/04/13 14:22:13 INFO executor.Executor: Finished task 2.0 in stage 8.0 (TID 34). 1597 bytes result sent to driver
20/04/13 14:22:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 40
20/04/13 14:22:14 INFO executor.Executor: Running task 2.0 in stage 9.0 (TID 40)
20/04/13 14:22:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 43
20/04/13 14:22:14 INFO executor.Executor: Running task 5.0 in stage 9.0 (TID 43)
20/04/13 14:22:14 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1455.4 MB)
20/04/13 14:22:14 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 13 ms
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 27.4 KB, free 1455.4 MB)
20/04/13 14:22:14 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 14:22:14 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15
20/04/13 14:22:14 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1455.3 MB)
20/04/13 14:22:14 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 13 ms
20/04/13 14:22:14 INFO codegen.CodeGenerator: Code generated in 26.804529 ms
20/04/13 14:22:14 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 536.2 KB, free 1454.8 MB)
20/04/13 14:22:18 INFO python.PythonUDFRunner: Times: total = 278, boot = 3, init = 70, finish = 205
20/04/13 14:22:18 INFO executor.Executor: Finished task 5.0 in stage 9.0 (TID 43). 2463 bytes result sent to driver
20/04/13 14:22:23 INFO python.PythonUDFRunner: Times: total = 1936, boot = 5, init = 51, finish = 1880
20/04/13 14:22:23 INFO executor.Executor: Finished task 2.0 in stage 9.0 (TID 40). 2420 bytes result sent to driver
20/04/13 14:22:28 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 45
20/04/13 14:22:28 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 48
20/04/13 14:22:28 INFO executor.Executor: Running task 4.0 in stage 10.0 (TID 48)
20/04/13 14:22:28 INFO executor.Executor: Running task 1.0 in stage 10.0 (TID 45)
20/04/13 14:22:28 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
20/04/13 14:22:28 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17
20/04/13 14:22:28 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 1976.0 B, free 1454.8 MB)
20/04/13 14:22:28 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 6 ms
20/04/13 14:22:28 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 3.3 KB, free 1454.8 MB)
20/04/13 14:22:28 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
20/04/13 14:22:28 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
20/04/13 14:22:28 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.200.211:42247)
20/04/13 14:22:28 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 14:22:28 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 14:22:28 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 14:22:28 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 14:22:28 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 3 ms
20/04/13 14:22:29 INFO memory.MemoryStore: Block rdd_54_4 stored as values in memory (estimated size 147.4 KB, free 1454.7 MB)
20/04/13 14:22:29 INFO executor.Executor: Finished task 4.0 in stage 10.0 (TID 48). 1176 bytes result sent to driver
20/04/13 14:22:29 INFO memory.MemoryStore: Block rdd_54_1 stored as values in memory (estimated size 147.4 KB, free 1454.5 MB)
20/04/13 14:22:29 INFO executor.Executor: Finished task 1.0 in stage 10.0 (TID 45). 1176 bytes result sent to driver
20/04/13 14:22:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 52
20/04/13 14:22:29 INFO executor.Executor: Running task 1.0 in stage 12.0 (TID 52)
20/04/13 14:22:29 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 55
20/04/13 14:22:29 INFO executor.Executor: Running task 4.0 in stage 12.0 (TID 55)
20/04/13 14:22:29 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18
20/04/13 14:22:29 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1454.5 MB)
20/04/13 14:22:29 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 17 ms
20/04/13 14:22:29 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 4.2 KB, free 1454.5 MB)
20/04/13 14:22:29 INFO storage.BlockManager: Found block rdd_54_4 locally
20/04/13 14:22:29 INFO storage.BlockManager: Found block rdd_54_1 locally
20/04/13 14:22:29 INFO executor.Executor: Finished task 1.0 in stage 12.0 (TID 52). 40772 bytes result sent to driver
20/04/13 14:22:29 INFO executor.Executor: Finished task 4.0 in stage 12.0 (TID 55). 40923 bytes result sent to driver
20/04/13 14:22:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 57
20/04/13 14:22:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 60
20/04/13 14:22:30 INFO executor.Executor: Running task 1.0 in stage 13.0 (TID 57)
20/04/13 14:22:30 INFO executor.Executor: Running task 4.0 in stage 13.0 (TID 60)
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 124.8 KB, free 1454.4 MB)
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 11 ms
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 347.5 KB, free 1454.0 MB)
20/04/13 14:22:30 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1454.0 MB)
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 14 ms
20/04/13 14:22:30 INFO codegen.CodeGenerator: Code generated in 33.67557 ms
20/04/13 14:22:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:30 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:30 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:30 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:30 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:30 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:30 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 536.2 KB, free 1453.5 MB)
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 77.3 KB, free 1389.4 MB)
20/04/13 14:22:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 13 ms
20/04/13 14:22:30 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 987.3 KB, free 1388.4 MB)
20/04/13 14:22:30 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:30 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:30 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 14:22:30 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 14:22:30 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 14:22:30 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 14:22:30 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 14:22:42 INFO python.PythonUDFRunner: Times: total = 421, boot = -13903, init = 13961, finish = 363
20/04/13 14:22:42 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8811881
20/04/13 14:22:42 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413142229_0013_m_000004_60' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413142229_0013_m_000004
20/04/13 14:22:42 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413142229_0013_m_000004_60: Committed
20/04/13 14:22:42 INFO executor.Executor: Finished task 4.0 in stage 13.0 (TID 60). 3198 bytes result sent to driver
20/04/13 14:22:43 INFO python.PythonUDFRunner: Times: total = 4356, boot = -15536, init = 15622, finish = 4270
20/04/13 14:22:43 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7707840
20/04/13 14:22:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413142229_0013_m_000001_57' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413142229_0013_m_000001
20/04/13 14:22:43 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413142229_0013_m_000001_57: Committed
20/04/13 14:22:43 INFO executor.Executor: Finished task 1.0 in stage 13.0 (TID 57). 3198 bytes result sent to driver
20/04/13 14:22:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 64
20/04/13 14:22:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 67
20/04/13 14:22:46 INFO executor.Executor: Running task 1.0 in stage 15.0 (TID 64)
20/04/13 14:22:46 INFO executor.Executor: Running task 4.0 in stage 15.0 (TID 67)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1452.4 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 7 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.0 KB, free 1452.4 MB)
20/04/13 14:22:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 14:22:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1452.4 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 8 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 536.2 KB, free 1451.9 MB)
20/04/13 14:22:46 INFO executor.Executor: Finished task 1.0 in stage 15.0 (TID 64). 1597 bytes result sent to driver
20/04/13 14:22:46 INFO executor.Executor: Finished task 4.0 in stage 15.0 (TID 67). 1554 bytes result sent to driver
20/04/13 14:22:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 69
20/04/13 14:22:46 INFO executor.Executor: Running task 0.0 in stage 16.0 (TID 69)
20/04/13 14:22:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 72
20/04/13 14:22:46 INFO executor.Executor: Running task 3.0 in stage 16.0 (TID 72)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1451.9 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 12 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 27.4 KB, free 1451.8 MB)
20/04/13 14:22:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26
20/04/13 14:22:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1451.8 MB)
20/04/13 14:22:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 26 took 14 ms
20/04/13 14:22:46 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 536.2 KB, free 1451.3 MB)
20/04/13 14:22:46 INFO codegen.CodeGenerator: Code generated in 24.989722 ms
20/04/13 14:22:57 INFO python.PythonUDFRunner: Times: total = 2782, boot = -12126, init = 12174, finish = 2734
20/04/13 14:22:57 INFO executor.Executor: Finished task 3.0 in stage 16.0 (TID 72). 2420 bytes result sent to driver
20/04/13 14:22:57 INFO python.PythonUDFRunner: Times: total = 2325, boot = -16034, init = 16080, finish = 2279
20/04/13 14:22:57 INFO executor.Executor: Finished task 0.0 in stage 16.0 (TID 69). 2420 bytes result sent to driver
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 75
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 78
20/04/13 14:22:59 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 75)
20/04/13 14:22:59 INFO executor.Executor: Running task 3.0 in stage 17.0 (TID 78)
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 28
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 1977.0 B, free 1451.3 MB)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 28 took 8 ms
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 3.3 KB, free 1451.3 MB)
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.200.211:42247)
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
20/04/13 14:22:59 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 14:22:59 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 14:22:59 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 14:22:59 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 14:22:59 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 2 ms
20/04/13 14:22:59 INFO memory.MemoryStore: Block rdd_84_0 stored as values in memory (estimated size 147.7 KB, free 1451.1 MB)
20/04/13 14:22:59 INFO executor.Executor: Finished task 0.0 in stage 17.0 (TID 75). 1176 bytes result sent to driver
20/04/13 14:22:59 INFO memory.MemoryStore: Block rdd_84_3 stored as values in memory (estimated size 146.5 KB, free 1451.0 MB)
20/04/13 14:22:59 INFO executor.Executor: Finished task 3.0 in stage 17.0 (TID 78). 1176 bytes result sent to driver
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 83
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 86
20/04/13 14:22:59 INFO executor.Executor: Running task 0.0 in stage 19.0 (TID 83)
20/04/13 14:22:59 INFO executor.Executor: Running task 3.0 in stage 19.0 (TID 86)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 29
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1451.0 MB)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 29 took 9 ms
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 4.2 KB, free 1451.0 MB)
20/04/13 14:22:59 INFO storage.BlockManager: Found block rdd_84_3 locally
20/04/13 14:22:59 INFO storage.BlockManager: Found block rdd_84_0 locally
20/04/13 14:22:59 INFO executor.Executor: Finished task 3.0 in stage 19.0 (TID 86). 41008 bytes result sent to driver
20/04/13 14:22:59 INFO executor.Executor: Finished task 0.0 in stage 19.0 (TID 83). 40880 bytes result sent to driver
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 89
20/04/13 14:22:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 92
20/04/13 14:22:59 INFO executor.Executor: Running task 2.0 in stage 20.0 (TID 89)
20/04/13 14:22:59 INFO executor.Executor: Running task 5.0 in stage 20.0 (TID 92)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 32
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 127.2 KB, free 1451.3 MB)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 32 took 11 ms
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 352.0 KB, free 1451.0 MB)
20/04/13 14:22:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 31
20/04/13 14:22:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 14:22:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 14:22:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 14:22:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1451.5 MB)
20/04/13 14:22:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 31 took 40 ms
20/04/13 14:22:59 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 536.2 KB, free 1451.0 MB)
20/04/13 14:23:00 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 30
20/04/13 14:23:00 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 77.2 KB, free 1322.9 MB)
20/04/13 14:23:00 INFO broadcast.TorrentBroadcast: Reading broadcast variable 30 took 26 ms
20/04/13 14:23:00 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 987.3 KB, free 1321.9 MB)
20/04/13 14:23:01 INFO storage.BlockManager: Removing RDD 84
20/04/13 14:23:01 INFO storage.BlockManager: Removing RDD 54
20/04/13 14:23:18 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:23:18 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:23:18 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 14:23:18 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 14:23:18 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 14:23:18 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 14:23:18 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 14:23:18 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 14:23:18 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 14:23:18 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 14:23:18 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 14:23:18 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 14:23:18 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 14:23:19 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:23:19 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 14:23:19 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 14:23:19 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 14:23:19 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 14:23:19 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 14:23:19 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 14:23:19 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 14:23:19 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 14:23:19 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 14:23:19 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 14:23:19 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 14:23:19 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 14:26:32 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
20/04/13 14:26:32 ERROR util.Utils: Aborting task
org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:490)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$3.applyOrElse(PythonRunner.scala:479)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:71)
	... 23 more
20/04/13 14:26:32 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1041313
20/04/13 14:26:32 ERROR util.Utils: Aborting task
java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:210)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:71)
	at org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:244)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/04/13 14:26:32 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1439295
20/04/13 14:26:32 INFO memory.MemoryStore: MemoryStore cleared
20/04/13 14:26:32 INFO storage.BlockManager: BlockManager stopped
20/04/13 14:26:32 WARN hdfs.DataStreamer: DataStreamer Exception
java.io.FileNotFoundException: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000005_92/part-00005-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16614) Holder DFSClient_NONMAPREDUCE_-1590679523_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1842)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1638)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:704)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000005_92/part-00005-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16614) Holder DFSClient_NONMAPREDUCE_-1590679523_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy18.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy19.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1838)
	... 2 more
20/04/13 14:26:32 WARN hdfs.DataStreamer: DataStreamer Exception
java.io.FileNotFoundException: File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000002_89/part-00002-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16616) Holder DFSClient_NONMAPREDUCE_-1590679523_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1842)
	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1638)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:704)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000002_89/part-00002-a059968a-cbc6-4473-a9a5-2f95bf0658c5-c000.snappy.parquet (inode 16616) Holder DFSClient_NONMAPREDUCE_-1590679523_36 does not have any open files.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2674)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2555)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:829)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy18.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:444)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy19.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1838)
	... 2 more
20/04/13 14:26:32 INFO util.ShutdownHookManager: Shutdown hook called
20/04/13 14:26:32 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0006/spark-7e6f4ff3-9924-49e4-af0c-f5a071af14a8
20/04/13 14:26:32 WARN output.FileOutputCommitter: Could not delete hdfs://178.62.208.209:9000/data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000002_89
20/04/13 14:26:32 WARN output.FileOutputCommitter: Could not delete hdfs://178.62.208.209:9000/data/df_3-shingles_dense-count-vectors.parquet/_temporary/0/_temporary/attempt_20200413142259_0020_m_000005_92
End of LogType:stderr

LogType:stdout
Log Upload Time:Mon Apr 13 14:26:33 +0000 2020
LogLength:0
Log Contents:
End of LogType:stdout

