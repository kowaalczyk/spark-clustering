

Container: container_1586849858644_0006_02_000009 on 178.62.198.251_42461
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:13032
Log Contents:
20/04/14 08:00:15 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21863@178.62.198.251
20/04/14 08:00:15 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 08:00:15 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 08:00:15 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 08:00:15 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 08:00:15 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 08:00:15 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 08:00:15 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 08:00:15 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 08:00:15 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 46 ms (0 ms spent in bootstraps)
20/04/14 08:00:15 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 08:00:15 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 08:00:15 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 08:00:15 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 08:00:15 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 08:00:16 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 2 ms (0 ms spent in bootstraps)
20/04/14 08:00:16 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-b5eeb72b-0471-4fc3-bc3f-67e18986b4e5
20/04/14 08:00:16 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 08:00:16 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.199.118:44353
20/04/14 08:00:16 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 08:00:16 INFO executor.Executor: Starting executor ID 7 on host 178.62.198.251
20/04/14 08:00:16 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41743.
20/04/14 08:00:16 INFO netty.NettyBlockTransferService: Server created on 178.62.198.251:41743
20/04/14 08:00:16 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 08:00:16 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(7, 178.62.198.251, 41743, None)
20/04/14 08:00:16 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(7, 178.62.198.251, 41743, None)
20/04/14 08:00:16 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(7, 178.62.198.251, 41743, None)
20/04/14 08:00:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 80
20/04/14 08:00:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 81
20/04/14 08:00:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 82
20/04/14 08:00:16 INFO executor.Executor: Running task 7.1 in stage 8.1 (TID 80)
20/04/14 08:00:16 INFO executor.Executor: Running task 8.1 in stage 8.1 (TID 81)
20/04/14 08:00:16 INFO executor.Executor: Running task 6.1 in stage 8.1 (TID 82)
20/04/14 08:00:16 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache
20/04/14 08:00:16 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
20/04/14 08:00:16 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:45281 after 2 ms (0 ms spent in bootstraps)
20/04/14 08:00:16 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.2 KB, free 3.0 GB)
20/04/14 08:00:16 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 115 ms
20/04/14 08:00:16 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.7 KB, free 3.0 GB)
20/04/14 08:00:16 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 08:00:16 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 08:00:16 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 08:00:16 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.199.118:44353)
20/04/14 08:00:16 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 08:00:16 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 08:00:16 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 08:00:16 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 08:00:16 WARN storage.BlockManager: Putting block rdd_7_6 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 08:00:16 WARN storage.BlockManager: Putting block rdd_7_8 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 08:00:16 WARN storage.BlockManager: Putting block rdd_7_7 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 08:00:16 WARN storage.BlockManager: Block rdd_7_8 could not be removed as it was not found on disk or in memory
20/04/14 08:00:16 WARN storage.BlockManager: Block rdd_7_7 could not be removed as it was not found on disk or in memory
20/04/14 08:00:16 WARN storage.BlockManager: Block rdd_7_6 could not be removed as it was not found on disk or in memory
20/04/14 08:00:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 83
20/04/14 08:00:16 INFO executor.Executor: Running task 4.1 in stage 8.1 (TID 83)
20/04/14 08:00:16 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 08:00:16 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.199.118:44353)
20/04/14 08:00:16 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 08:00:16 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 08:00:16 WARN storage.BlockManager: Putting block rdd_7_4 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 08:00:16 WARN storage.BlockManager: Block rdd_7_4 could not be removed as it was not found on disk or in memory
20/04/14 08:00:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 84
20/04/14 08:00:17 INFO executor.Executor: Running task 0.0 in stage 7.1 (TID 84)
20/04/14 08:00:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 85
20/04/14 08:00:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 86
20/04/14 08:00:17 INFO executor.Executor: Running task 1.0 in stage 7.1 (TID 85)
20/04/14 08:00:17 INFO executor.Executor: Running task 2.0 in stage 7.1 (TID 86)
20/04/14 08:00:17 INFO spark.MapOutputTrackerWorker: Updating epoch to 8 and clearing cache
20/04/14 08:00:17 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11
20/04/14 08:00:17 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.9 KB, free 3.0 GB)
20/04/14 08:00:17 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 6 ms
20/04/14 08:00:17 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 08:00:17 INFO codegen.CodeGenerator: Code generated in 300.59316 ms
20/04/14 08:00:17 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00007-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4944913, partition values: [empty row]
20/04/14 08:00:17 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00005-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4950456, partition values: [empty row]
20/04/14 08:00:17 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/14 08:00:17 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00006-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4956444, partition values: [empty row]
20/04/14 08:00:17 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 08:00:17 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 9 ms
20/04/14 08:00:17 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 540.2 KB, free 3.0 GB)
20/04/14 08:00:18 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 08:00:18 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 08:00:18 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 08:00:19 INFO codegen.CodeGenerator: Code generated in 43.998592 ms
20/04/14 08:00:19 INFO codegen.CodeGenerator: Code generated in 43.235441 ms
20/04/14 08:00:19 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11674 records.
20/04/14 08:00:19 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11734 records.
20/04/14 08:00:19 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11838 records.
20/04/14 08:00:19 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 08:00:19 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 08:00:19 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 08:00:19 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 08:00:19 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 08:00:19 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 08:00:19 INFO hadoop.InternalParquetRecordReader: block read in memory in 41 ms. row count = 11838
20/04/14 08:00:19 INFO hadoop.InternalParquetRecordReader: block read in memory in 39 ms. row count = 11734
20/04/14 08:00:19 INFO hadoop.InternalParquetRecordReader: block read in memory in 46 ms. row count = 11674
20/04/14 08:00:19 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
20/04/14 08:00:19 INFO memory.MemoryStore: MemoryStore cleared
20/04/14 08:00:19 INFO storage.BlockManager: BlockManager stopped
20/04/14 08:00:19 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_1586849858644_0006_01_000010 on 178.62.198.251_42461
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:4174
Log Contents:
20/04/14 07:59:26 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21388@178.62.198.251
20/04/14 07:59:26 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:59:26 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:59:26 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:59:26 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:26 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:26 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:26 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:26 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
Exception in thread "main" java.lang.reflect.UndeclaredThrowableException
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1862)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:64)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:188)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.main(CoarseGrainedExecutorBackend.scala:285)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend.main(CoarseGrainedExecutorBackend.scala)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1.apply$mcV$sp(CoarseGrainedExecutorBackend.scala:201)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$2.run(SparkHadoopUtil.scala:65)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$2.run(SparkHadoopUtil.scala:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
	... 4 more
Caused by: java.io.IOException: Failed to connect to /178.62.198.251:39507
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:198)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: /178.62.198.251:39507
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:327)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:688)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:635)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:552)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:514)
	at io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1044)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_1586849858644_0006_01_000009 on 178.62.198.251_42461
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:7518
Log Contents:
20/04/14 07:59:13 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21309@178.62.198.251
20/04/14 07:59:13 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:59:13 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:59:13 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:59:13 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:13 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:13 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:13 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:13 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:14 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:39507 after 45 ms (0 ms spent in bootstraps)
20/04/14 07:59:14 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:14 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:14 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:14 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:14 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:14 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:39507 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:59:14 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-464df544-2cfe-4076-99a0-37740651460f
20/04/14 07:59:14 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 07:59:14 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.198.251:39507
20/04/14 07:59:14 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 07:59:14 INFO executor.Executor: Starting executor ID 6 on host 178.62.198.251
20/04/14 07:59:14 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33143.
20/04/14 07:59:14 INFO netty.NettyBlockTransferService: Server created on 178.62.198.251:33143
20/04/14 07:59:14 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 07:59:14 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(6, 178.62.198.251, 33143, None)
20/04/14 07:59:14 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(6, 178.62.198.251, 33143, None)
20/04/14 07:59:14 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(6, 178.62.198.251, 33143, None)
20/04/14 07:59:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 77
20/04/14 07:59:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 78
20/04/14 07:59:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 79
20/04/14 07:59:17 INFO executor.Executor: Running task 8.0 in stage 8.1 (TID 79)
20/04/14 07:59:17 INFO executor.Executor: Running task 7.0 in stage 8.1 (TID 78)
20/04/14 07:59:17 INFO executor.Executor: Running task 6.0 in stage 8.1 (TID 77)
20/04/14 07:59:17 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache
20/04/14 07:59:17 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
20/04/14 07:59:17 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:41011 after 1 ms (0 ms spent in bootstraps)
20/04/14 07:59:17 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.2 KB, free 3.0 GB)
20/04/14 07:59:17 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 68 ms
20/04/14 07:59:17 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.7 KB, free 3.0 GB)
20/04/14 07:59:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:17 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.198.251:39507)
20/04/14 07:59:17 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:59:17 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 0 local blocks and 9 remote blocks
20/04/14 07:59:17 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 0 local blocks and 9 remote blocks
20/04/14 07:59:17 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 0 local blocks and 9 remote blocks
20/04/14 07:59:17 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:35143 after 8 ms (0 ms spent in bootstraps)
20/04/14 07:59:17 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44505 after 8 ms (0 ms spent in bootstraps)
20/04/14 07:59:17 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 20 ms
20/04/14 07:59:17 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 20 ms
20/04/14 07:59:17 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 20 ms
20/04/14 07:59:18 INFO memory.MemoryStore: Block rdd_7_7 stored as values in memory (estimated size 14.6 MB, free 3.0 GB)
20/04/14 07:59:18 INFO memory.MemoryStore: Block rdd_7_8 stored as values in memory (estimated size 14.7 MB, free 3.0 GB)
20/04/14 07:59:18 INFO memory.MemoryStore: Block rdd_7_6 stored as values in memory (estimated size 15.1 MB, free 3.0 GB)
20/04/14 07:59:18 INFO codegen.CodeGenerator: Code generated in 145.824134 ms
20/04/14 07:59:18 INFO codegen.CodeGenerator: Code generated in 30.729674 ms
20/04/14 07:59:19 INFO codegen.CodeGenerator: Code generated in 12.007814 ms
20/04/14 07:59:19 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/14 07:59:19 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 3.0 GB)
20/04/14 07:59:19 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 8 ms
20/04/14 07:59:19 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 3.0 GB)
20/04/14 07:59:19 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/14 07:59:19 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 3.0 GB)
20/04/14 07:59:19 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 13 ms
20/04/14 07:59:20 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.4 GB)
20/04/14 07:59:25 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/14 07:59:25 INFO storage.DiskBlockManager: Shutdown hook called
20/04/14 07:59:25 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:124
Log Contents:
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill %p"
#   Executing /bin/sh -c "kill 21309"...
End of LogType:stdout



Container: container_1586849858644_0006_02_000010 on 178.62.198.251_42461
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:5883
Log Contents:
20/04/14 08:00:16 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21918@178.62.198.251
20/04/14 08:00:16 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 08:00:16 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 08:00:16 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 08:00:17 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 08:00:17 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 08:00:17 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 08:00:17 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 08:00:17 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 08:00:17 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 65 ms (0 ms spent in bootstraps)
20/04/14 08:00:17 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 08:00:17 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 08:00:17 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 08:00:17 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 08:00:17 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 08:00:17 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 2 ms (0 ms spent in bootstraps)
20/04/14 08:00:17 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-6d3004c5-707e-4011-8d96-e4342d95c90d
20/04/14 08:00:17 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 08:00:18 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.199.118:44353
20/04/14 08:00:18 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 08:00:18 INFO executor.Executor: Starting executor ID 8 on host 178.62.198.251
20/04/14 08:00:18 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44973.
20/04/14 08:00:18 INFO netty.NettyBlockTransferService: Server created on 178.62.198.251:44973
20/04/14 08:00:18 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 08:00:18 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(8, 178.62.198.251, 44973, None)
20/04/14 08:00:18 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(8, 178.62.198.251, 44973, None)
20/04/14 08:00:18 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(8, 178.62.198.251, 44973, None)
20/04/14 08:00:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 87
20/04/14 08:00:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 88
20/04/14 08:00:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 89
20/04/14 08:00:18 INFO executor.Executor: Running task 3.0 in stage 7.1 (TID 87)
20/04/14 08:00:18 INFO executor.Executor: Running task 5.0 in stage 7.1 (TID 89)
20/04/14 08:00:18 INFO executor.Executor: Running task 4.0 in stage 7.1 (TID 88)
20/04/14 08:00:18 INFO spark.MapOutputTrackerWorker: Updating epoch to 8 and clearing cache
20/04/14 08:00:18 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11
20/04/14 08:00:18 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:41743 after 1 ms (0 ms spent in bootstraps)
20/04/14 08:00:18 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.9 KB, free 3.0 GB)
20/04/14 08:00:18 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 114 ms
20/04/14 08:00:18 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 08:00:18 INFO codegen.CodeGenerator: Code generated in 185.005697 ms
20/04/14 08:00:19 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00004-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4935174, partition values: [empty row]
20/04/14 08:00:19 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00000-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4791031, partition values: [empty row]
20/04/14 08:00:19 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/14 08:00:19 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00003-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4855590, partition values: [empty row]
20/04/14 08:00:19 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 08:00:19 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 10 ms
20/04/14 08:00:19 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 540.2 KB, free 3.0 GB)
20/04/14 08:00:19 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
20/04/14 08:00:19 INFO memory.MemoryStore: MemoryStore cleared
20/04/14 08:00:19 INFO storage.BlockManager: BlockManager stopped
20/04/14 08:00:19 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_1586849858644_0006_02_000005 on 178.62.198.251_42461
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:28366
Log Contents:
20/04/14 07:59:55 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21677@178.62.198.251
20/04/14 07:59:55 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:59:55 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:59:55 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:59:56 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:56 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:56 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:56 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:56 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:56 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 48 ms (0 ms spent in bootstraps)
20/04/14 07:59:56 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:56 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:56 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:56 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:56 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:56 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:59:56 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-db6ee7ea-ba8c-4eaa-ae7a-73698070e896
20/04/14 07:59:56 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 07:59:56 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.199.118:44353
20/04/14 07:59:56 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 07:59:56 INFO executor.Executor: Starting executor ID 4 on host 178.62.198.251
20/04/14 07:59:57 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35331.
20/04/14 07:59:57 INFO netty.NettyBlockTransferService: Server created on 178.62.198.251:35331
20/04/14 07:59:57 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 07:59:57 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(4, 178.62.198.251, 35331, None)
20/04/14 07:59:57 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(4, 178.62.198.251, 35331, None)
20/04/14 07:59:57 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(4, 178.62.198.251, 35331, None)
20/04/14 07:59:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 57
20/04/14 07:59:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 58
20/04/14 07:59:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 59
20/04/14 07:59:57 INFO executor.Executor: Running task 0.1 in stage 8.0 (TID 58)
20/04/14 07:59:57 INFO executor.Executor: Running task 9.1 in stage 8.0 (TID 59)
20/04/14 07:59:57 INFO executor.Executor: Running task 1.1 in stage 8.0 (TID 57)
20/04/14 07:59:57 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/14 07:59:57 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/14 07:59:57 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:45281 after 3 ms (0 ms spent in bootstraps)
20/04/14 07:59:57 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 3.0 GB)
20/04/14 07:59:57 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 77 ms
20/04/14 07:59:57 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 19.7 KB, free 3.0 GB)
20/04/14 07:59:57 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:57 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:57 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:57 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.199.118:44353)
20/04/14 07:59:57 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:59:57 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 07:59:57 WARN storage.BlockManager: Putting block rdd_7_1 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 07:59:57 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 07:59:57 WARN storage.BlockManager: Putting block rdd_7_0 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 07:59:57 WARN storage.BlockManager: Block rdd_7_1 could not be removed as it was not found on disk or in memory
20/04/14 07:59:57 WARN storage.BlockManager: Block rdd_7_0 could not be removed as it was not found on disk or in memory
20/04/14 07:59:57 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 07:59:57 WARN storage.BlockManager: Putting block rdd_7_9 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 07:59:57 WARN storage.BlockManager: Block rdd_7_9 could not be removed as it was not found on disk or in memory
20/04/14 07:59:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 60
20/04/14 07:59:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 61
20/04/14 07:59:57 INFO executor.Executor: Running task 7.1 in stage 8.0 (TID 61)
20/04/14 07:59:57 INFO executor.Executor: Running task 2.1 in stage 8.0 (TID 60)
20/04/14 07:59:57 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:57 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:57 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.199.118:44353)
20/04/14 07:59:57 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:59:57 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 07:59:57 WARN storage.BlockManager: Putting block rdd_7_2 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 07:59:57 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.199.118:44353)
20/04/14 07:59:57 WARN storage.BlockManager: Block rdd_7_2 could not be removed as it was not found on disk or in memory
20/04/14 07:59:57 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:59:57 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 07:59:57 WARN storage.BlockManager: Putting block rdd_7_7 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 07:59:57 WARN storage.BlockManager: Block rdd_7_7 could not be removed as it was not found on disk or in memory
20/04/14 07:59:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 62
20/04/14 07:59:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 63
20/04/14 07:59:57 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 62)
20/04/14 07:59:57 INFO executor.Executor: Running task 1.0 in stage 7.0 (TID 63)
20/04/14 07:59:57 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 64
20/04/14 07:59:57 INFO executor.Executor: Running task 2.0 in stage 7.0 (TID 64)
20/04/14 07:59:57 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache
20/04/14 07:59:57 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9
20/04/14 07:59:57 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.9 KB, free 3.0 GB)
20/04/14 07:59:57 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 7 ms
20/04/14 07:59:57 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 07:59:58 INFO codegen.CodeGenerator: Code generated in 281.941444 ms
20/04/14 07:59:58 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00005-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4950456, partition values: [empty row]
20/04/14 07:59:58 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00006-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4956444, partition values: [empty row]
20/04/14 07:59:58 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00007-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4944913, partition values: [empty row]
20/04/14 07:59:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/14 07:59:58 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 07:59:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 8 ms
20/04/14 07:59:58 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 540.2 KB, free 3.0 GB)
20/04/14 07:59:59 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:59 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:59 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:59 INFO codegen.CodeGenerator: Code generated in 35.030276 ms
20/04/14 07:59:59 INFO codegen.CodeGenerator: Code generated in 21.997224 ms
20/04/14 07:59:59 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11838 records.
20/04/14 07:59:59 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11734 records.
20/04/14 07:59:59 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11674 records.
20/04/14 07:59:59 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:59 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:59 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:59 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:59 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:59 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:59 INFO hadoop.InternalParquetRecordReader: block read in memory in 57 ms. row count = 11674
20/04/14 07:59:59 INFO hadoop.InternalParquetRecordReader: block read in memory in 58 ms. row count = 11838
20/04/14 07:59:59 INFO hadoop.InternalParquetRecordReader: block read in memory in 75 ms. row count = 11734
20/04/14 08:00:01 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 62). 1487 bytes result sent to driver
20/04/14 08:00:01 INFO executor.Executor: Finished task 2.0 in stage 7.0 (TID 64). 1487 bytes result sent to driver
20/04/14 08:00:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68
20/04/14 08:00:01 INFO executor.Executor: Running task 6.0 in stage 7.0 (TID 68)
20/04/14 08:00:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 69
20/04/14 08:00:01 INFO executor.Executor: Running task 7.0 in stage 7.0 (TID 69)
20/04/14 08:00:01 INFO executor.Executor: Finished task 1.0 in stage 7.0 (TID 63). 1487 bytes result sent to driver
20/04/14 08:00:01 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 70
20/04/14 08:00:01 INFO executor.Executor: Running task 8.0 in stage 7.0 (TID 70)
20/04/14 08:00:01 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00008-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-213844, partition values: [empty row]
20/04/14 08:00:01 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00001-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4785170, partition values: [empty row]
20/04/14 08:00:01 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00002-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4735846, partition values: [empty row]
20/04/14 08:00:01 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 08:00:01 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 08:00:01 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 472 records.
20/04/14 08:00:01 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 08:00:01 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 08:00:01 INFO hadoop.InternalParquetRecordReader: block read in memory in 2 ms. row count = 472
20/04/14 08:00:01 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 6080 records.
20/04/14 08:00:01 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 08:00:01 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 5926 records.
20/04/14 08:00:01 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 08:00:01 INFO hadoop.InternalParquetRecordReader: block read in memory in 11 ms. row count = 6080
20/04/14 08:00:01 INFO hadoop.InternalParquetRecordReader: block read in memory in 10 ms. row count = 5926
20/04/14 08:00:01 INFO executor.Executor: Finished task 8.0 in stage 7.0 (TID 70). 1444 bytes result sent to driver
20/04/14 08:00:01 INFO executor.Executor: Finished task 6.0 in stage 7.0 (TID 68). 1487 bytes result sent to driver
20/04/14 08:00:01 INFO executor.Executor: Finished task 7.0 in stage 7.0 (TID 69). 1487 bytes result sent to driver
20/04/14 08:00:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 72
20/04/14 08:00:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 74
20/04/14 08:00:03 INFO executor.Executor: Running task 1.0 in stage 8.1 (TID 72)
20/04/14 08:00:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 76
20/04/14 08:00:03 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache
20/04/14 08:00:03 INFO executor.Executor: Running task 5.0 in stage 8.1 (TID 76)
20/04/14 08:00:03 INFO executor.Executor: Running task 3.0 in stage 8.1 (TID 74)
20/04/14 08:00:03 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
20/04/14 08:00:03 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.2 KB, free 3.0 GB)
20/04/14 08:00:03 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 11 ms
20/04/14 08:00:03 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.7 KB, free 3.0 GB)
20/04/14 08:00:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 08:00:03 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.199.118:44353)
20/04/14 08:00:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 08:00:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 08:00:03 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 08:00:03 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 6 local blocks and 3 remote blocks
20/04/14 08:00:03 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 6 local blocks and 3 remote blocks
20/04/14 08:00:03 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 6 local blocks and 3 remote blocks
20/04/14 08:00:03 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:45589 after 1 ms (0 ms spent in bootstraps)
20/04/14 08:00:03 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 11 ms
20/04/14 08:00:03 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 11 ms
20/04/14 08:00:03 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 11 ms
20/04/14 08:00:03 INFO memory.MemoryStore: Block rdd_7_1 stored as values in memory (estimated size 14.9 MB, free 3.0 GB)
20/04/14 08:00:03 INFO memory.MemoryStore: Block rdd_7_5 stored as values in memory (estimated size 14.7 MB, free 3.0 GB)
20/04/14 08:00:03 INFO codegen.CodeGenerator: Code generated in 7.829752 ms
20/04/14 08:00:03 INFO memory.MemoryStore: Block rdd_7_3 stored as values in memory (estimated size 14.8 MB, free 3.0 GB)
20/04/14 08:00:03 INFO codegen.CodeGenerator: Code generated in 30.293575 ms
20/04/14 08:00:04 INFO codegen.CodeGenerator: Code generated in 13.272323 ms
20/04/14 08:00:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/14 08:00:04 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 3.0 GB)
20/04/14 08:00:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 9 ms
20/04/14 08:00:04 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 3.0 GB)
20/04/14 08:00:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/14 08:00:04 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 3.0 GB)
20/04/14 08:00:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 25 ms
20/04/14 08:00:05 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.4 GB)
20/04/14 08:00:11 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/14 08:00:12 INFO storage.DiskBlockManager: Shutdown hook called
20/04/14 08:00:12 ERROR executor.Executor: Exception in task 1.0 in stage 8.1 (TID 72)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 08:00:12 INFO executor.Executor: Not reporting error to driver during JVM shutdown.
20/04/14 08:00:12 ERROR util.SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 72,5,main]
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 08:00:12 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:124
Log Contents:
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill %p"
#   Executing /bin/sh -c "kill 21677"...
End of LogType:stdout



Container: container_1586849858644_0006_02_000004 on 178.62.198.251_42461
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:22134
Log Contents:
20/04/14 07:59:32 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21492@178.62.198.251
20/04/14 07:59:32 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:59:32 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:59:32 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:59:33 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:33 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:33 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:33 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:33 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:33 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 47 ms (0 ms spent in bootstraps)
20/04/14 07:59:33 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:33 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:33 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:33 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:33 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:33 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:59:33 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-03a81f40-8a3d-4b0a-9296-4bfe1ef20e89
20/04/14 07:59:33 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 07:59:33 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.199.118:44353
20/04/14 07:59:33 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 07:59:33 INFO executor.Executor: Starting executor ID 3 on host 178.62.198.251
20/04/14 07:59:33 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41839.
20/04/14 07:59:33 INFO netty.NettyBlockTransferService: Server created on 178.62.198.251:41839
20/04/14 07:59:33 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 07:59:33 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(3, 178.62.198.251, 41839, None)
20/04/14 07:59:33 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(3, 178.62.198.251, 41839, None)
20/04/14 07:59:33 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(3, 178.62.198.251, 41839, None)
20/04/14 07:59:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3
20/04/14 07:59:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6
20/04/14 07:59:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9
20/04/14 07:59:37 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 3)
20/04/14 07:59:37 INFO executor.Executor: Running task 5.0 in stage 1.0 (TID 6)
20/04/14 07:59:37 INFO executor.Executor: Running task 8.0 in stage 1.0 (TID 9)
20/04/14 07:59:37 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
20/04/14 07:59:38 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:38045 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:59:38 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.5 KB, free 3.0 GB)
20/04/14 07:59:38 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 138 ms
20/04/14 07:59:38 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.1 KB, free 3.0 GB)
20/04/14 07:59:38 INFO codegen.CodeGenerator: Code generated in 237.614841 ms
20/04/14 07:59:38 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00000-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4791031, partition values: [empty row]
20/04/14 07:59:38 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00007-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4944913, partition values: [empty row]
20/04/14 07:59:38 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00008-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-213844, partition values: [empty row]
20/04/14 07:59:38 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/14 07:59:38 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:37949 after 1 ms (0 ms spent in bootstraps)
20/04/14 07:59:38 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 07:59:38 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 95 ms
20/04/14 07:59:39 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 540.2 KB, free 3.0 GB)
20/04/14 07:59:40 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:40 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:40 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:40 INFO codegen.CodeGenerator: Code generated in 26.444082 ms
20/04/14 07:59:40 INFO codegen.CodeGenerator: Code generated in 20.61939 ms
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 472 records.
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11674 records.
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 5980 records.
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:40 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:40 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:40 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: block read in memory in 62 ms. row count = 472
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: block read in memory in 59 ms. row count = 5980
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: block read in memory in 58 ms. row count = 11674
20/04/14 07:59:41 INFO executor.Executor: Finished task 8.0 in stage 1.0 (TID 9). 1530 bytes result sent to driver
20/04/14 07:59:41 INFO executor.Executor: Finished task 5.0 in stage 1.0 (TID 6). 1487 bytes result sent to driver
20/04/14 07:59:41 INFO executor.Executor: Finished task 2.0 in stage 1.0 (TID 3). 1487 bytes result sent to driver
20/04/14 07:59:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13
20/04/14 07:59:42 INFO executor.Executor: Running task 2.0 in stage 4.0 (TID 13)
20/04/14 07:59:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16
20/04/14 07:59:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18
20/04/14 07:59:42 INFO executor.Executor: Running task 5.0 in stage 4.0 (TID 16)
20/04/14 07:59:42 INFO executor.Executor: Running task 7.0 in stage 4.0 (TID 18)
20/04/14 07:59:42 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/14 07:59:42 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
20/04/14 07:59:43 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.6 KB, free 3.0 GB)
20/04/14 07:59:43 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 25 ms
20/04/14 07:59:43 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 07:59:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:43 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.199.118:44353)
20/04/14 07:59:43 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 17 ms
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 17 ms
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 17 ms
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_7 stored as values in memory (estimated size 14.6 MB, free 3.0 GB)
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_5 stored as values in memory (estimated size 14.7 MB, free 3.0 GB)
20/04/14 07:59:43 INFO codegen.CodeGenerator: Code generated in 8.346944 ms
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_2 stored as values in memory (estimated size 14.5 MB, free 3.0 GB)
20/04/14 07:59:43 INFO codegen.CodeGenerator: Code generated in 37.034387 ms
20/04/14 07:59:44 INFO codegen.CodeGenerator: Code generated in 16.077932 ms
20/04/14 07:59:44 INFO executor.Executor: Finished task 7.0 in stage 4.0 (TID 18). 1583 bytes result sent to driver
20/04/14 07:59:44 INFO executor.Executor: Finished task 2.0 in stage 4.0 (TID 13). 1583 bytes result sent to driver
20/04/14 07:59:44 INFO executor.Executor: Finished task 5.0 in stage 4.0 (TID 16). 1583 bytes result sent to driver
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 30
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 33
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 36
20/04/14 07:59:44 INFO executor.Executor: Running task 5.0 in stage 6.0 (TID 33)
20/04/14 07:59:44 INFO executor.Executor: Running task 2.0 in stage 6.0 (TID 30)
20/04/14 07:59:44 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
20/04/14 07:59:44 INFO executor.Executor: Running task 7.0 in stage 6.0 (TID 36)
20/04/14 07:59:44 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:45281 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:59:44 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.7 KB, free 3.0 GB)
20/04/14 07:59:44 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 21 ms
20/04/14 07:59:44 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.1 KB, free 3.0 GB)
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_5 locally
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_2 locally
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_7 locally
20/04/14 07:59:44 INFO executor.Executor: Finished task 5.0 in stage 6.0 (TID 33). 1347 bytes result sent to driver
20/04/14 07:59:44 INFO executor.Executor: Finished task 7.0 in stage 6.0 (TID 36). 1304 bytes result sent to driver
20/04/14 07:59:44 INFO executor.Executor: Finished task 2.0 in stage 6.0 (TID 30). 8152 bytes result sent to driver
20/04/14 07:59:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 47
20/04/14 07:59:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 50
20/04/14 07:59:47 INFO executor.Executor: Running task 2.0 in stage 8.0 (TID 47)
20/04/14 07:59:47 INFO executor.Executor: Running task 5.0 in stage 8.0 (TID 50)
20/04/14 07:59:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 53
20/04/14 07:59:47 INFO executor.Executor: Running task 7.0 in stage 8.0 (TID 53)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 3.0 GB)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 12 ms
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 19.7 KB, free 3.0 GB)
20/04/14 07:59:47 INFO storage.BlockManager: Found block rdd_7_7 locally
20/04/14 07:59:47 INFO storage.BlockManager: Found block rdd_7_2 locally
20/04/14 07:59:47 INFO storage.BlockManager: Found block rdd_7_5 locally
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 3.0 GB)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 9 ms
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 3.0 GB)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 3.0 GB)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 14 ms
20/04/14 07:59:48 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.4 GB)
20/04/14 07:59:56 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/14 07:59:56 INFO storage.DiskBlockManager: Shutdown hook called
20/04/14 07:59:56 ERROR executor.Executor: Exception in task 2.0 in stage 8.0 (TID 47)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:56 INFO executor.Executor: Not reporting error to driver during JVM shutdown.
20/04/14 07:59:56 ERROR util.SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 47,5,main]
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:56 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:124
Log Contents:
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill %p"
#   Executing /bin/sh -c "kill 21492"...
End of LogType:stdout



Container: container_1586849858644_0006_02_000006 on 178.62.198.251_42461
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:18953
Log Contents:
20/04/14 07:59:57 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21749@178.62.198.251
20/04/14 07:59:57 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:59:57 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:59:57 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:59:58 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:58 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:58 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:58 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:58 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:58 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 60 ms (0 ms spent in bootstraps)
20/04/14 07:59:58 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:58 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:58 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:58 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:58 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:59 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:59:59 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-06a99274-21a9-431a-a387-18e77f12b46d
20/04/14 07:59:59 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 07:59:59 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.199.118:44353
20/04/14 07:59:59 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 07:59:59 INFO executor.Executor: Starting executor ID 5 on host 178.62.198.251
20/04/14 07:59:59 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45589.
20/04/14 07:59:59 INFO netty.NettyBlockTransferService: Server created on 178.62.198.251:45589
20/04/14 07:59:59 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 07:59:59 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(5, 178.62.198.251, 45589, None)
20/04/14 07:59:59 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(5, 178.62.198.251, 45589, None)
20/04/14 07:59:59 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(5, 178.62.198.251, 45589, None)
20/04/14 07:59:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 65
20/04/14 07:59:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 66
20/04/14 07:59:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 67
20/04/14 07:59:59 INFO executor.Executor: Running task 3.0 in stage 7.0 (TID 65)
20/04/14 07:59:59 INFO executor.Executor: Running task 4.0 in stage 7.0 (TID 66)
20/04/14 07:59:59 INFO executor.Executor: Running task 5.0 in stage 7.0 (TID 67)
20/04/14 07:59:59 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache
20/04/14 07:59:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9
20/04/14 07:59:59 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:35331 after 7 ms (0 ms spent in bootstraps)
20/04/14 07:59:59 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.9 KB, free 3.0 GB)
20/04/14 07:59:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 144 ms
20/04/14 07:59:59 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 08:00:00 INFO codegen.CodeGenerator: Code generated in 321.954158 ms
20/04/14 08:00:00 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00000-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4791031, partition values: [empty row]
20/04/14 08:00:00 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00003-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4855590, partition values: [empty row]
20/04/14 08:00:00 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/14 08:00:00 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00004-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4935174, partition values: [empty row]
20/04/14 08:00:00 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 08:00:00 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 124 ms
20/04/14 08:00:00 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 540.2 KB, free 3.0 GB)
20/04/14 08:00:01 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 08:00:01 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 08:00:01 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 08:00:02 INFO codegen.CodeGenerator: Code generated in 17.74397 ms
20/04/14 08:00:02 INFO codegen.CodeGenerator: Code generated in 12.54131 ms
20/04/14 08:00:02 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 9550 records.
20/04/14 08:00:02 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 5980 records.
20/04/14 08:00:02 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11557 records.
20/04/14 08:00:02 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 08:00:02 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 08:00:02 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 08:00:02 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 08:00:02 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 08:00:02 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 08:00:02 INFO hadoop.InternalParquetRecordReader: block read in memory in 38 ms. row count = 5980
20/04/14 08:00:02 INFO hadoop.InternalParquetRecordReader: block read in memory in 40 ms. row count = 9550
20/04/14 08:00:02 INFO hadoop.InternalParquetRecordReader: block read in memory in 38 ms. row count = 11557
20/04/14 08:00:03 INFO executor.Executor: Finished task 4.0 in stage 7.0 (TID 66). 1530 bytes result sent to driver
20/04/14 08:00:03 INFO executor.Executor: Finished task 3.0 in stage 7.0 (TID 65). 1530 bytes result sent to driver
20/04/14 08:00:03 INFO executor.Executor: Finished task 5.0 in stage 7.0 (TID 67). 1487 bytes result sent to driver
20/04/14 08:00:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 71
20/04/14 08:00:03 INFO executor.Executor: Running task 0.0 in stage 8.1 (TID 71)
20/04/14 08:00:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 73
20/04/14 08:00:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 75
20/04/14 08:00:03 INFO executor.Executor: Running task 4.0 in stage 8.1 (TID 75)
20/04/14 08:00:03 INFO executor.Executor: Running task 2.0 in stage 8.1 (TID 73)
20/04/14 08:00:03 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache
20/04/14 08:00:03 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
20/04/14 08:00:03 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.2 KB, free 3.0 GB)
20/04/14 08:00:03 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 9 ms
20/04/14 08:00:03 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.7 KB, free 3.0 GB)
20/04/14 08:00:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 08:00:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 08:00:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 08:00:03 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.199.118:44353)
20/04/14 08:00:03 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 08:00:03 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 08:00:03 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 08:00:03 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 08:00:03 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 13 ms
20/04/14 08:00:03 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 19 ms
20/04/14 08:00:03 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 20 ms
20/04/14 08:00:03 INFO memory.MemoryStore: Block rdd_7_2 stored as values in memory (estimated size 14.5 MB, free 3.0 GB)
20/04/14 08:00:03 INFO memory.MemoryStore: Block rdd_7_4 stored as values in memory (estimated size 14.9 MB, free 3.0 GB)
20/04/14 08:00:03 INFO memory.MemoryStore: Block rdd_7_0 stored as values in memory (estimated size 15.0 MB, free 3.0 GB)
20/04/14 08:00:03 INFO codegen.CodeGenerator: Code generated in 5.914273 ms
20/04/14 08:00:03 INFO codegen.CodeGenerator: Code generated in 22.985336 ms
20/04/14 08:00:04 INFO codegen.CodeGenerator: Code generated in 13.259715 ms
20/04/14 08:00:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/14 08:00:04 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 3.0 GB)
20/04/14 08:00:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 10 ms
20/04/14 08:00:04 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 3.0 GB)
20/04/14 08:00:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/14 08:00:04 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 3.0 GB)
20/04/14 08:00:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 14 ms
20/04/14 08:00:05 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.4 GB)
20/04/14 08:00:12 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/14 08:00:12 INFO storage.DiskBlockManager: Shutdown hook called
20/04/14 08:00:12 ERROR executor.Executor: Exception in task 2.0 in stage 8.1 (TID 73)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 08:00:12 INFO executor.Executor: Not reporting error to driver during JVM shutdown.
20/04/14 08:00:12 ERROR util.SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 73,5,main]
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 08:00:12 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:124
Log Contents:
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill %p"
#   Executing /bin/sh -c "kill 21749"...
End of LogType:stdout



Container: container_1586849858644_0006_01_000002 on 178.62.198.251_42461
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:32396
Log Contents:
20/04/14 07:58:41 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21122@178.62.198.251
20/04/14 07:58:41 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:58:41 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:58:41 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:58:42 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:58:42 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:58:42 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:58:42 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:58:42 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:58:42 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:39507 after 50 ms (0 ms spent in bootstraps)
20/04/14 07:58:42 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:58:42 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:58:42 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:58:42 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:58:42 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:58:42 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:39507 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:58:43 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-31a8b57d-4f15-42e0-b9e9-2546de50aa68
20/04/14 07:58:43 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 07:58:43 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.198.251:39507
20/04/14 07:58:43 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 07:58:43 INFO executor.Executor: Starting executor ID 1 on host 178.62.198.251
20/04/14 07:58:43 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36075.
20/04/14 07:58:43 INFO netty.NettyBlockTransferService: Server created on 178.62.198.251:36075
20/04/14 07:58:43 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 07:58:43 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, 178.62.198.251, 36075, None)
20/04/14 07:58:43 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, 178.62.198.251, 36075, None)
20/04/14 07:58:43 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, 178.62.198.251, 36075, None)
20/04/14 07:58:45 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0
20/04/14 07:58:45 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
20/04/14 07:58:45 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
20/04/14 07:58:46 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:41011 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:58:46 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.2 KB, free 3.0 GB)
20/04/14 07:58:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 207 ms
20/04/14 07:58:46 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 127.0 KB, free 3.0 GB)
20/04/14 07:58:47 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2496 bytes result sent to driver
20/04/14 07:58:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2
20/04/14 07:58:49 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 2)
20/04/14 07:58:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5
20/04/14 07:58:49 INFO executor.Executor: Running task 4.0 in stage 1.0 (TID 5)
20/04/14 07:58:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8
20/04/14 07:58:49 INFO executor.Executor: Running task 7.0 in stage 1.0 (TID 8)
20/04/14 07:58:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
20/04/14 07:58:49 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.5 KB, free 3.0 GB)
20/04/14 07:58:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 9 ms
20/04/14 07:58:49 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.1 KB, free 3.0 GB)
20/04/14 07:58:49 INFO codegen.CodeGenerator: Code generated in 254.378168 ms
20/04/14 07:58:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00003-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4855590, partition values: [empty row]
20/04/14 07:58:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00005-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4950456, partition values: [empty row]
20/04/14 07:58:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00002-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4735846, partition values: [empty row]
20/04/14 07:58:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/14 07:58:49 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 07:58:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 13 ms
20/04/14 07:58:49 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 540.2 KB, free 3.0 GB)
20/04/14 07:58:49 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:58:49 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:58:49 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:58:49 INFO codegen.CodeGenerator: Code generated in 18.554081 ms
20/04/14 07:58:49 INFO codegen.CodeGenerator: Code generated in 13.952986 ms
20/04/14 07:58:49 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 9550 records.
20/04/14 07:58:49 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 6080 records.
20/04/14 07:58:49 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11838 records.
20/04/14 07:58:49 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:58:49 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:58:49 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:58:49 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:58:49 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:58:49 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:58:49 INFO hadoop.InternalParquetRecordReader: block read in memory in 48 ms. row count = 9550
20/04/14 07:58:49 INFO hadoop.InternalParquetRecordReader: block read in memory in 48 ms. row count = 6080
20/04/14 07:58:49 INFO hadoop.InternalParquetRecordReader: block read in memory in 51 ms. row count = 11838
20/04/14 07:58:51 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 1530 bytes result sent to driver
20/04/14 07:58:51 INFO executor.Executor: Finished task 7.0 in stage 1.0 (TID 8). 1530 bytes result sent to driver
20/04/14 07:58:51 INFO executor.Executor: Finished task 4.0 in stage 1.0 (TID 5). 1487 bytes result sent to driver
20/04/14 07:58:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10
20/04/14 07:58:53 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 10)
20/04/14 07:58:53 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/14 07:58:53 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3
20/04/14 07:58:53 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.6 KB, free 3.0 GB)
20/04/14 07:58:53 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 9 ms
20/04/14 07:58:53 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.1 KB, free 3.0 GB)
20/04/14 07:58:53 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:58:53 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.198.251:39507)
20/04/14 07:58:53 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:58:53 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:53 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:42185 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:58:53 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:39673 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:58:53 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 14 ms
20/04/14 07:58:53 INFO memory.MemoryStore: Block rdd_7_0 stored as values in memory (estimated size 15.0 MB, free 3.0 GB)
20/04/14 07:58:53 INFO codegen.CodeGenerator: Code generated in 6.261639 ms
20/04/14 07:58:53 INFO codegen.CodeGenerator: Code generated in 19.707851 ms
20/04/14 07:58:54 INFO codegen.CodeGenerator: Code generated in 13.460571 ms
20/04/14 07:58:54 INFO executor.Executor: 1 block locks were not released by TID = 10:
[rdd_7_0]
20/04/14 07:58:54 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 10). 4256 bytes result sent to driver
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16
20/04/14 07:58:54 INFO executor.Executor: Running task 2.0 in stage 4.0 (TID 13)
20/04/14 07:58:54 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 11)
20/04/14 07:58:54 INFO executor.Executor: Running task 5.0 in stage 4.0 (TID 16)
20/04/14 07:58:54 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
20/04/14 07:58:54 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.6 KB, free 3.0 GB)
20/04/14 07:58:54 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 9 ms
20/04/14 07:58:54 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 07:58:54 INFO storage.BlockManager: Found block rdd_7_0 locally
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 10 ms
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 10 ms
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_5 stored as values in memory (estimated size 14.7 MB, free 3.0 GB)
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_2 stored as values in memory (estimated size 14.5 MB, free 3.0 GB)
20/04/14 07:58:54 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 11). 1368 bytes result sent to driver
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20
20/04/14 07:58:54 INFO executor.Executor: Running task 9.0 in stage 4.0 (TID 20)
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 2 ms
20/04/14 07:58:54 INFO executor.Executor: Finished task 5.0 in stage 4.0 (TID 16). 1540 bytes result sent to driver
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 21
20/04/14 07:58:54 INFO executor.Executor: Running task 10.0 in stage 4.0 (TID 21)
20/04/14 07:58:54 INFO executor.Executor: Finished task 2.0 in stage 4.0 (TID 13). 1540 bytes result sent to driver
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 4 ms
20/04/14 07:58:54 INFO executor.Executor: Running task 11.0 in stage 4.0 (TID 22)
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_9 stored as values in memory (estimated size 14.6 MB, free 3.0 GB)
20/04/14 07:58:54 INFO executor.Executor: Finished task 9.0 in stage 4.0 (TID 20). 1583 bytes result sent to driver
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_11 stored as values in memory (estimated size 14.4 MB, free 3.0 GB)
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 23
20/04/14 07:58:54 INFO executor.Executor: Running task 12.0 in stage 4.0 (TID 23)
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_10 stored as values in memory (estimated size 14.7 MB, free 2.9 GB)
20/04/14 07:58:54 INFO executor.Executor: Finished task 11.0 in stage 4.0 (TID 22). 1583 bytes result sent to driver
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 24
20/04/14 07:58:54 INFO executor.Executor: Running task 13.0 in stage 4.0 (TID 24)
20/04/14 07:58:54 INFO executor.Executor: Finished task 10.0 in stage 4.0 (TID 21). 1583 bytes result sent to driver
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25
20/04/14 07:58:54 INFO executor.Executor: Running task 14.0 in stage 4.0 (TID 25)
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_12 stored as values in memory (estimated size 14.8 MB, free 2.9 GB)
20/04/14 07:58:54 INFO executor.Executor: Finished task 12.0 in stage 4.0 (TID 23). 1540 bytes result sent to driver
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 26
20/04/14 07:58:54 INFO executor.Executor: Running task 15.0 in stage 4.0 (TID 26)
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_13 stored as values in memory (estimated size 14.1 MB, free 2.9 GB)
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_14 stored as values in memory (estimated size 15.0 MB, free 2.9 GB)
20/04/14 07:58:54 INFO executor.Executor: Finished task 13.0 in stage 4.0 (TID 24). 1540 bytes result sent to driver
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 27
20/04/14 07:58:54 INFO executor.Executor: Running task 16.0 in stage 4.0 (TID 27)
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO executor.Executor: Finished task 14.0 in stage 4.0 (TID 25). 1540 bytes result sent to driver
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 28
20/04/14 07:58:54 INFO executor.Executor: Running task 17.0 in stage 4.0 (TID 28)
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_15 stored as values in memory (estimated size 15.1 MB, free 2.9 GB)
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 120 ms
20/04/14 07:58:54 INFO executor.Executor: Finished task 15.0 in stage 4.0 (TID 26). 1583 bytes result sent to driver
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_16 stored as values in memory (estimated size 15.2 MB, free 2.9 GB)
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_17 stored as values in memory (estimated size 14.7 MB, free 2.9 GB)
20/04/14 07:58:54 INFO executor.Executor: Finished task 17.0 in stage 4.0 (TID 28). 1583 bytes result sent to driver
20/04/14 07:58:55 INFO executor.Executor: Finished task 16.0 in stage 4.0 (TID 27). 1583 bytes result sent to driver
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 29
20/04/14 07:58:55 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 29)
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 35
20/04/14 07:58:55 INFO executor.Executor: Running task 2.0 in stage 6.0 (TID 32)
20/04/14 07:58:55 INFO executor.Executor: Running task 5.0 in stage 6.0 (TID 35)
20/04/14 07:58:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
20/04/14 07:58:55 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.7 KB, free 2.9 GB)
20/04/14 07:58:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 9 ms
20/04/14 07:58:55 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.1 KB, free 2.9 GB)
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_2 locally
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_5 locally
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_0 locally
20/04/14 07:58:55 INFO executor.Executor: Finished task 2.0 in stage 6.0 (TID 32). 8109 bytes result sent to driver
20/04/14 07:58:55 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 29). 4569 bytes result sent to driver
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 38
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 39
20/04/14 07:58:55 INFO executor.Executor: Running task 10.0 in stage 6.0 (TID 39)
20/04/14 07:58:55 INFO executor.Executor: Running task 9.0 in stage 6.0 (TID 38)
20/04/14 07:58:55 INFO executor.Executor: Finished task 5.0 in stage 6.0 (TID 35). 1304 bytes result sent to driver
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 40
20/04/14 07:58:55 INFO executor.Executor: Running task 11.0 in stage 6.0 (TID 40)
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_9 locally
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_10 locally
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_11 locally
20/04/14 07:58:55 INFO executor.Executor: Finished task 9.0 in stage 6.0 (TID 38). 5615 bytes result sent to driver
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 41
20/04/14 07:58:55 INFO executor.Executor: Running task 12.0 in stage 6.0 (TID 41)
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_12 locally
20/04/14 07:58:55 INFO executor.Executor: Finished task 10.0 in stage 6.0 (TID 39). 4310 bytes result sent to driver
20/04/14 07:58:55 INFO executor.Executor: Finished task 11.0 in stage 6.0 (TID 40). 4639 bytes result sent to driver
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 42
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 43
20/04/14 07:58:55 INFO executor.Executor: Running task 13.0 in stage 6.0 (TID 42)
20/04/14 07:58:55 INFO executor.Executor: Running task 14.0 in stage 6.0 (TID 43)
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_13 locally
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_14 locally
20/04/14 07:58:55 INFO executor.Executor: Finished task 12.0 in stage 6.0 (TID 41). 5198 bytes result sent to driver
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 44
20/04/14 07:58:55 INFO executor.Executor: Running task 15.0 in stage 6.0 (TID 44)
20/04/14 07:58:56 INFO storage.BlockManager: Found block rdd_7_15 locally
20/04/14 07:58:56 INFO executor.Executor: Finished task 14.0 in stage 6.0 (TID 43). 3544 bytes result sent to driver
20/04/14 07:58:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 45
20/04/14 07:58:56 INFO executor.Executor: Running task 16.0 in stage 6.0 (TID 45)
20/04/14 07:58:56 INFO executor.Executor: Finished task 13.0 in stage 6.0 (TID 42). 5251 bytes result sent to driver
20/04/14 07:58:56 INFO storage.BlockManager: Found block rdd_7_16 locally
20/04/14 07:58:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 46
20/04/14 07:58:56 INFO executor.Executor: Running task 17.0 in stage 6.0 (TID 46)
20/04/14 07:58:56 INFO storage.BlockManager: Found block rdd_7_17 locally
20/04/14 07:58:56 INFO executor.Executor: Finished task 16.0 in stage 6.0 (TID 45). 1304 bytes result sent to driver
20/04/14 07:58:56 INFO executor.Executor: Finished task 15.0 in stage 6.0 (TID 44). 35161 bytes result sent to driver
20/04/14 07:58:56 INFO executor.Executor: Finished task 17.0 in stage 6.0 (TID 46). 5052 bytes result sent to driver
20/04/14 07:58:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 49
20/04/14 07:58:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 52
20/04/14 07:58:58 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 49)
20/04/14 07:58:58 INFO executor.Executor: Running task 2.0 in stage 8.0 (TID 52)
20/04/14 07:58:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 55
20/04/14 07:58:58 INFO executor.Executor: Running task 5.0 in stage 8.0 (TID 55)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 2.9 GB)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 9 ms
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 19.7 KB, free 2.9 GB)
20/04/14 07:58:58 INFO storage.BlockManager: Found block rdd_7_2 locally
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/14 07:58:58 INFO storage.BlockManager: Found block rdd_7_0 locally
20/04/14 07:58:58 INFO storage.BlockManager: Found block rdd_7_5 locally
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 2.9 GB)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 22 ms
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 2.9 GB)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 2.8 GB)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 18 ms
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.3 GB)
20/04/14 07:59:07 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/14 07:59:07 ERROR executor.Executor: Exception in task 2.0 in stage 8.0 (TID 52)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:07 INFO executor.Executor: Not reporting error to driver during JVM shutdown.
20/04/14 07:59:07 ERROR util.SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 52,5,main]
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:07 INFO storage.DiskBlockManager: Shutdown hook called
20/04/14 07:59:07 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:124
Log Contents:
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill %p"
#   Executing /bin/sh -c "kill 21122"...
End of LogType:stdout



Container: container_1586849858644_0006_01_000001 on 178.62.198.251_42461
===========================================================================
LogType:pyspark.log
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:24265
Log Contents:
[PYTHON] 2020-04-14 07:58:45,362.362 INFO clustering - wrapper: perform_experiment keyword arguments:
[PYTHON] 2020-04-14 07:58:45,363.363 INFO clustering - wrapper: in_files: ['/data/df_3-shingles_sparse-binary-vectors.parquet']
[PYTHON] 2020-04-14 07:58:45,363.363 INFO clustering - wrapper: distances: ['cosine', 'euclidean']
[PYTHON] 2020-04-14 07:58:45,363.363 INFO clustering - wrapper: ks: [2, 4]
[PYTHON] 2020-04-14 07:58:45,363.363 INFO clustering - wrapper: models: [<class 'pyspark.ml.clustering.GaussianMixture'>]
[PYTHON] 2020-04-14 07:58:45,363.363 INFO clustering - wrapper: result_dfs_list: []
[PYTHON] 2020-04-14 07:58:45,365.365 INFO clustering - read_and_repartition: reading /data/df_3-shingles_sparse-binary-vectors.parquet into 18 partitions
[PYTHON] 2020-04-14 07:58:48,429.429 INFO clustering - wrapper: read_and_repartition finished in 3.07s
[PYTHON] 2020-04-14 07:59:26,205.205 INFO java_gateway - send_command: Error while receiving.
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1159, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
[PYTHON] 2020-04-14 07:59:26,207.207 ERROR java_gateway - send_command: Exception while sending command.
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1159, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 985, in send_command
    response = connection.send_command(command)
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1164, in send_command
    "Error while receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while receiving
[PYTHON] 2020-04-14 07:59:26,209.209 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,209.209 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,209.209 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,209.209 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,209.209 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,209.209 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,210.210 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,210.210 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,210.210 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,210.210 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,210.210 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,210.210 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,211.211 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,211.211 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,211.211 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,211.211 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,211.211 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,212.212 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,212.212 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,212.212 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,212.212 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,212.212 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,213.213 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,213.213 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,213.213 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,213.213 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,214.214 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,214.214 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,214.214 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,296.296 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 07:59:26,307.307 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:34773)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
End of LogType:pyspark.log

LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:141936
Log Contents:
20/04/14 07:58:37 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:58:37 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:58:37 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:58:37 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:58:37 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:58:37 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:58:37 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:58:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:58:37 INFO yarn.ApplicationMaster: Preparing Local resources
20/04/14 07:58:38 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1586849858644_0006_000001
20/04/14 07:58:38 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread
20/04/14 07:58:38 INFO yarn.ApplicationMaster: Waiting for spark context initialization...
20/04/14 07:58:38 INFO spark.SparkContext: Running Spark version 2.4.5
20/04/14 07:58:38 INFO spark.SparkContext: Submitted application: ClusteringExperiment
20/04/14 07:58:38 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:58:38 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:58:38 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:58:38 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:58:38 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:58:39 INFO util.Utils: Successfully started service 'sparkDriver' on port 39507.
20/04/14 07:58:39 INFO spark.SparkEnv: Registering MapOutputTracker
20/04/14 07:58:39 INFO spark.SparkEnv: Registering BlockManagerMaster
20/04/14 07:58:39 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/04/14 07:58:39 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/04/14 07:58:39 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-44dacfd3-0e61-4742-b6c8-64f01736a879
20/04/14 07:58:39 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 07:58:39 INFO spark.SparkEnv: Registering OutputCommitCoordinator
20/04/14 07:58:39 INFO util.log: Logging initialized @2652ms
20/04/14 07:58:39 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
20/04/14 07:58:39 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
20/04/14 07:58:39 INFO server.Server: Started @2721ms
20/04/14 07:58:39 INFO server.AbstractConnector: Started ServerConnector@60b3ba3b{HTTP/1.1,[http/1.1]}{0.0.0.0:45627}
20/04/14 07:58:39 INFO util.Utils: Successfully started service 'SparkUI' on port 45627.
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40c3d4cb{/jobs,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30367247{/jobs/json,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7522329d{/jobs/job,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c12b7db{/jobs/job/json,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f355654{/stages,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b6725f5{/stages/json,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@39891e75{/stages/stage,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27746490{/stages/stage/json,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@567fbcbd{/stages/pool,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60d2e7d8{/stages/pool/json,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10369660{/storage,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14622012{/storage/json,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3fd3ec77{/storage/rdd,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48c82072{/storage/rdd/json,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ce92a9d{/environment,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@698ef5a2{/environment/json,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@499e1cb0{/executors,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@40dc7e41{/executors/json,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@65440905{/executors/threadDump,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b79c3ea{/executors/threadDump/json,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@423b2ba9{/static,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c4d1cc1{/,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d3f6f24{/api,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14d3ecd5{/jobs/job/kill,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a76c8a9{/stages/stage/kill,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://178.62.198.251:45627
20/04/14 07:58:39 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler
20/04/14 07:58:39 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1586849858644_0006 and attemptId Some(appattempt_1586849858644_0006_000001)
20/04/14 07:58:39 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41011.
20/04/14 07:58:39 INFO netty.NettyBlockTransferService: Server created on 178.62.198.251:41011
20/04/14 07:58:39 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 07:58:39 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 178.62.198.251, 41011, None)
20/04/14 07:58:39 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.198.251:41011 with 3.0 GB RAM, BlockManagerId(driver, 178.62.198.251, 41011, None)
20/04/14 07:58:39 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 178.62.198.251, 41011, None)
20/04/14 07:58:39 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 178.62.198.251, 41011, None)
20/04/14 07:58:39 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
20/04/14 07:58:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29faad18{/metrics/json,null,AVAILABLE,@Spark}
20/04/14 07:58:39 INFO client.RMProxy: Connecting to ResourceManager at /178.62.197.79:8030
20/04/14 07:58:39 INFO yarn.YarnRMClient: Registering the ApplicationMaster
20/04/14 07:58:39 INFO yarn.ApplicationMaster: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/src/hadoop-2.8.5/etc/hadoop:/usr/src/hadoop-2.8.5/share/hadoop/common/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/common/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/*:/usr/src/hadoop-2.8.5/contrib/capacity-scheduler/*.jar<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__
    SPARK_DIST_CLASSPATH -> /usr/src/hadoop-2.8.5/etc/hadoop:/usr/src/hadoop-2.8.5/share/hadoop/common/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/common/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/*:/usr/src/hadoop-2.8.5/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> hdfs://178.62.197.79:9000/user/root/.sparkStaging/application_1586849858644_0006
    SPARK_USER -> root
    PYTHONPATH -> /usr/src/spark-2.4.5-bin-without-hadoop/python:/usr/src/spark-2.4.5-bin-without-hadoop/python/build:/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/pyspark.zip:/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip

  command:
    {{JAVA_HOME}}/bin/java \ 
      -server \ 
      -Xmx6144m \ 
      -Djava.io.tmpdir={{PWD}}/tmp \ 
      '-Dspark.driver.port=39507' \ 
      '-Dspark.ui.port=0' \ 
      -Dspark.yarn.app.container.log.dir=<LOG_DIR> \ 
      -XX:OnOutOfMemoryError='kill %p' \ 
      org.apache.spark.executor.CoarseGrainedExecutorBackend \ 
      --driver-url \ 
      spark://CoarseGrainedScheduler@178.62.198.251:39507 \ 
      --executor-id \ 
      <executorId> \ 
      --hostname \ 
      <hostname> \ 
      --cores \ 
      3 \ 
      --app-id \ 
      application_1586849858644_0006 \ 
      --user-class-path \ 
      file:$PWD/__app__.jar \ 
      1><LOG_DIR>/stdout \ 
      2><LOG_DIR>/stderr

  resources:
    pyspark.zip -> resource { scheme: "hdfs" host: "178.62.197.79" port: 9000 file: "/user/root/.sparkStaging/application_1586849858644_0006/pyspark.zip" } size: 591945 timestamp: 1586851113752 type: FILE visibility: PRIVATE
    py4j-0.10.7-src.zip -> resource { scheme: "hdfs" host: "178.62.197.79" port: 9000 file: "/user/root/.sparkStaging/application_1586849858644_0006/py4j-0.10.7-src.zip" } size: 42437 timestamp: 1586851113775 type: FILE visibility: PRIVATE
    __spark_libs__ -> resource { scheme: "hdfs" host: "178.62.197.79" port: 9000 file: "/user/root/.sparkStaging/application_1586849858644_0006/__spark_libs__4654896636915899670.zip" } size: 168822862 timestamp: 1586851113598 type: ARCHIVE visibility: PRIVATE
    __spark_conf__ -> resource { scheme: "hdfs" host: "178.62.197.79" port: 9000 file: "/user/root/.sparkStaging/application_1586849858644_0006/__spark_conf__.zip" } size: 233325 timestamp: 1586851113906 type: ARCHIVE visibility: PRIVATE

===============================================================================
20/04/14 07:58:39 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@178.62.198.251:39507)
20/04/14 07:58:39 INFO yarn.YarnAllocator: Will request 3 executor container(s), each with 3 core(s) and 6758 MB memory (including 614 MB of overhead)
20/04/14 07:58:39 INFO yarn.YarnAllocator: Submitted 3 unlocalized container requests.
20/04/14 07:58:39 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals
20/04/14 07:58:41 INFO impl.AMRMClientImpl: Received new token for : 178.62.198.251:42461
20/04/14 07:58:41 INFO impl.AMRMClientImpl: Received new token for : 178.62.199.118:35057
20/04/14 07:58:41 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_01_000002 on host 178.62.198.251 for executor with ID 1
20/04/14 07:58:41 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_01_000003 on host 178.62.199.118 for executor with ID 2
20/04/14 07:58:41 INFO yarn.YarnAllocator: Received 2 containers from YARN, launching executors on 2 of them.
20/04/14 07:58:41 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 07:58:41 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 07:58:42 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_01_000005 on host 178.62.199.118 for executor with ID 3
20/04/14 07:58:42 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/14 07:58:42 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 07:58:43 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.198.251:41436) with ID 1
20/04/14 07:58:43 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.198.251:36075 with 3.0 GB RAM, BlockManagerId(1, 178.62.198.251, 36075, None)
20/04/14 07:58:43 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.199.118:54484) with ID 2
20/04/14 07:58:44 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.199.118:39673 with 3.0 GB RAM, BlockManagerId(2, 178.62.199.118, 39673, None)
20/04/14 07:58:44 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.199.118:54488) with ID 3
20/04/14 07:58:44 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.199.118:42185 with 3.0 GB RAM, BlockManagerId(3, 178.62.199.118, 42185, None)
20/04/14 07:58:44 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/04/14 07:58:44 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
20/04/14 07:58:44 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/container_1586849858644_0006_01_000001/spark-warehouse').
20/04/14 07:58:44 INFO internal.SharedState: Warehouse path is 'file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/container_1586849858644_0006_01_000001/spark-warehouse'.
20/04/14 07:58:44 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.
20/04/14 07:58:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45ee6a7{/SQL,null,AVAILABLE,@Spark}
20/04/14 07:58:44 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.
20/04/14 07:58:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1fd866bd{/SQL/json,null,AVAILABLE,@Spark}
20/04/14 07:58:44 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.
20/04/14 07:58:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6eda9f50{/SQL/execution,null,AVAILABLE,@Spark}
20/04/14 07:58:44 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.
20/04/14 07:58:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63dfe07e{/SQL/execution/json,null,AVAILABLE,@Spark}
20/04/14 07:58:44 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.
20/04/14 07:58:44 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@55ae3ffb{/static/sql,null,AVAILABLE,@Spark}
20/04/14 07:58:45 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/04/14 07:58:45 INFO datasources.InMemoryFileIndex: It took 103 ms to list leaf files for 1 paths.
20/04/14 07:58:45 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
20/04/14 07:58:45 INFO scheduler.DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/04/14 07:58:45 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
20/04/14 07:58:45 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/14 07:58:45 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/14 07:58:45 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/14 07:58:45 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 127.0 KB, free 3.0 GB)
20/04/14 07:58:45 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.2 KB, free 3.0 GB)
20/04/14 07:58:45 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 178.62.198.251:41011 (size: 33.2 KB, free: 3.0 GB)
20/04/14 07:58:45 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1163
20/04/14 07:58:45 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/04/14 07:58:45 INFO cluster.YarnClusterScheduler: Adding task set 0.0 with 1 tasks
20/04/14 07:58:45 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 178.62.198.251, executor 1, partition 0, PROCESS_LOCAL, 8098 bytes)
20/04/14 07:58:46 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 178.62.198.251:36075 (size: 33.2 KB, free: 3.0 GB)
20/04/14 07:58:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1397 ms on 178.62.198.251 (executor 1) (1/1)
20/04/14 07:58:47 INFO cluster.YarnClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/04/14 07:58:47 INFO scheduler.DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.504 s
20/04/14 07:58:47 INFO scheduler.DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.536133 s
20/04/14 07:58:48 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/14 07:58:48 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/14 07:58:48 INFO datasources.FileSourceStrategy: Output Data Schema: struct<entry: string, entry_name: string, features: vector ... 1 more fields>
20/04/14 07:58:48 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/14 07:58:48 INFO codegen.CodeGenerator: Code generated in 222.245065 ms
20/04/14 07:58:48 INFO spark.ContextCleaner: Cleaned accumulator 26
20/04/14 07:58:48 INFO spark.ContextCleaner: Cleaned accumulator 27
20/04/14 07:58:48 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 341.6 KB, free 3.0 GB)
20/04/14 07:58:48 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 07:58:48 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.198.251:41011 (size: 32.5 KB, free: 3.0 GB)
20/04/14 07:58:48 INFO spark.SparkContext: Created broadcast 1 from rdd at GaussianMixture.scala:348
20/04/14 07:58:48 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 8546356 bytes, open cost is considered as scanning 4194304 bytes.
20/04/14 07:58:49 INFO spark.SparkContext: Starting job: first at GaussianMixture.scala:357
20/04/14 07:58:49 INFO scheduler.DAGScheduler: Registering RDD 5 (rdd at GaussianMixture.scala:348) as input to shuffle 0
20/04/14 07:58:49 INFO scheduler.DAGScheduler: Got job 1 (first at GaussianMixture.scala:357) with 1 output partitions
20/04/14 07:58:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (first at GaussianMixture.scala:357)
20/04/14 07:58:49 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
20/04/14 07:58:49 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
20/04/14 07:58:49 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at rdd at GaussianMixture.scala:348), which has no missing parents
20/04/14 07:58:49 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.1 KB, free 3.0 GB)
20/04/14 07:58:49 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.5 KB, free 3.0 GB)
20/04/14 07:58:49 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.198.251:41011 (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:58:49 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1163
20/04/14 07:58:49 INFO scheduler.DAGScheduler: Submitting 9 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at rdd at GaussianMixture.scala:348) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8))
20/04/14 07:58:49 INFO cluster.YarnClusterScheduler: Adding task set 1.0 with 9 tasks
20/04/14 07:58:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 178.62.199.118, executor 3, partition 0, NODE_LOCAL, 8336 bytes)
20/04/14 07:58:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, 178.62.198.251, executor 1, partition 1, NODE_LOCAL, 8336 bytes)
20/04/14 07:58:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, 178.62.199.118, executor 2, partition 2, NODE_LOCAL, 8336 bytes)
20/04/14 07:58:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, 178.62.199.118, executor 3, partition 3, NODE_LOCAL, 8336 bytes)
20/04/14 07:58:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, 178.62.198.251, executor 1, partition 4, NODE_LOCAL, 8336 bytes)
20/04/14 07:58:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, 178.62.199.118, executor 2, partition 5, NODE_LOCAL, 8336 bytes)
20/04/14 07:58:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7, 178.62.199.118, executor 3, partition 6, NODE_LOCAL, 8336 bytes)
20/04/14 07:58:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8, 178.62.198.251, executor 1, partition 7, NODE_LOCAL, 8336 bytes)
20/04/14 07:58:49 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9, 178.62.199.118, executor 2, partition 8, NODE_LOCAL, 8336 bytes)
20/04/14 07:58:49 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.198.251:36075 (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:58:49 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.199.118:39673 (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:58:49 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.199.118:42185 (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:58:49 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.198.251:36075 (size: 32.5 KB, free: 3.0 GB)
20/04/14 07:58:49 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.199.118:39673 (size: 32.5 KB, free: 3.0 GB)
20/04/14 07:58:50 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.199.118:42185 (size: 32.5 KB, free: 3.0 GB)
20/04/14 07:58:51 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 2078 ms on 178.62.198.251 (executor 1) (1/9)
20/04/14 07:58:51 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 2080 ms on 178.62.198.251 (executor 1) (2/9)
20/04/14 07:58:51 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 2126 ms on 178.62.198.251 (executor 1) (3/9)
20/04/14 07:58:52 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 3132 ms on 178.62.199.118 (executor 2) (4/9)
20/04/14 07:58:52 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 3657 ms on 178.62.199.118 (executor 2) (5/9)
20/04/14 07:58:52 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 3712 ms on 178.62.199.118 (executor 2) (6/9)
20/04/14 07:58:52 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 3897 ms on 178.62.199.118 (executor 3) (7/9)
20/04/14 07:58:52 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 3903 ms on 178.62.199.118 (executor 3) (8/9)
20/04/14 07:58:52 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3917 ms on 178.62.199.118 (executor 3) (9/9)
20/04/14 07:58:52 INFO cluster.YarnClusterScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/04/14 07:58:52 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (rdd at GaussianMixture.scala:348) finished in 3.939 s
20/04/14 07:58:52 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/04/14 07:58:52 INFO scheduler.DAGScheduler: running: Set()
20/04/14 07:58:52 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
20/04/14 07:58:52 INFO scheduler.DAGScheduler: failed: Set()
20/04/14 07:58:53 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at map at GaussianMixture.scala:348), which has no missing parents
20/04/14 07:58:53 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.1 KB, free 3.0 GB)
20/04/14 07:58:53 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.6 KB, free 3.0 GB)
20/04/14 07:58:53 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 178.62.198.251:41011 (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:58:53 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163
20/04/14 07:58:53 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at map at GaussianMixture.scala:348) (first 15 tasks are for partitions Vector(0))
20/04/14 07:58:53 INFO cluster.YarnClusterScheduler: Adding task set 2.0 with 1 tasks
20/04/14 07:58:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 10, 178.62.198.251, executor 1, partition 0, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:53 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 178.62.198.251:36075 (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:58:53 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.198.251:41436
20/04/14 07:58:53 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on 178.62.198.251:36075 (size: 15.0 MB, free: 3.0 GB)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 10) in 1157 ms on 178.62.198.251 (executor 1) (1/1)
20/04/14 07:58:54 INFO cluster.YarnClusterScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/04/14 07:58:54 INFO scheduler.DAGScheduler: ResultStage 2 (first at GaussianMixture.scala:357) finished in 1.163 s
20/04/14 07:58:54 INFO scheduler.DAGScheduler: Job 1 finished: first at GaussianMixture.scala:357, took 5.125509 s
20/04/14 07:58:54 INFO util.Instrumentation: [1214304c] Stage class: GaussianMixture
20/04/14 07:58:54 INFO util.Instrumentation: [1214304c] Stage uid: GaussianMixture_47edf4676231
20/04/14 07:58:54 INFO util.Instrumentation: [1214304c] training: numPartitions=18 storageLevel=StorageLevel(1 replicas)
20/04/14 07:58:54 INFO util.Instrumentation: [1214304c] {"featuresCol":"features","predictionCol":"cluster","k":2,"seed":42}
20/04/14 07:58:54 INFO util.Instrumentation: [1214304c] {"numFeatures":8502}
20/04/14 07:58:54 INFO spark.SparkContext: Starting job: takeSample at GaussianMixture.scala:470
20/04/14 07:58:54 INFO scheduler.DAGScheduler: Got job 2 (takeSample at GaussianMixture.scala:470) with 18 output partitions
20/04/14 07:58:54 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (takeSample at GaussianMixture.scala:470)
20/04/14 07:58:54 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
20/04/14 07:58:54 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/14 07:58:54 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at map at GaussianMixture.scala:348), which has no missing parents
20/04/14 07:58:54 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 07:58:54 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.6 KB, free 3.0 GB)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.198.251:41011 (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:58:54 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1163
20/04/14 07:58:54 INFO scheduler.DAGScheduler: Submitting 18 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at map at GaussianMixture.scala:348) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/04/14 07:58:54 INFO cluster.YarnClusterScheduler: Adding task set 4.0 with 18 tasks
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 11, 178.62.198.251, executor 1, partition 0, PROCESS_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 12, 178.62.199.118, executor 2, partition 1, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 13, 178.62.198.251, executor 1, partition 2, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 4.0 (TID 14, 178.62.199.118, executor 3, partition 3, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 4.0 (TID 15, 178.62.199.118, executor 2, partition 4, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 4.0 (TID 16, 178.62.198.251, executor 1, partition 5, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 4.0 (TID 17, 178.62.199.118, executor 3, partition 6, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 4.0 (TID 18, 178.62.199.118, executor 2, partition 7, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 4.0 (TID 19, 178.62.199.118, executor 3, partition 8, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.198.251:36075 (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.199.118:42185 (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.199.118:39673 (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_5 in memory on 178.62.198.251:36075 (size: 14.7 MB, free: 3.0 GB)
20/04/14 07:58:54 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.199.118:54488
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_2 in memory on 178.62.198.251:36075 (size: 14.5 MB, free: 3.0 GB)
20/04/14 07:58:54 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.199.118:54484
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 4.0 (TID 20, 178.62.198.251, executor 1, partition 9, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 11) in 162 ms on 178.62.198.251 (executor 1) (1/18)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 4.0 (TID 21, 178.62.198.251, executor 1, partition 10, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 4.0 (TID 16) in 205 ms on 178.62.198.251 (executor 1) (2/18)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 4.0 (TID 22, 178.62.198.251, executor 1, partition 11, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 13) in 217 ms on 178.62.198.251 (executor 1) (3/18)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_9 in memory on 178.62.198.251:36075 (size: 14.6 MB, free: 3.0 GB)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 4.0 (TID 23, 178.62.198.251, executor 1, partition 12, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_11 in memory on 178.62.198.251:36075 (size: 14.4 MB, free: 3.0 GB)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 4.0 (TID 20) in 162 ms on 178.62.198.251 (executor 1) (4/18)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_10 in memory on 178.62.198.251:36075 (size: 14.7 MB, free: 2.9 GB)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 4.0 (TID 24, 178.62.198.251, executor 1, partition 13, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 4.0 (TID 22) in 165 ms on 178.62.198.251 (executor 1) (5/18)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 4.0 (TID 25, 178.62.198.251, executor 1, partition 14, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 4.0 (TID 21) in 185 ms on 178.62.198.251 (executor 1) (6/18)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_12 in memory on 178.62.198.251:36075 (size: 14.8 MB, free: 2.9 GB)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 4.0 (TID 26, 178.62.198.251, executor 1, partition 15, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 4.0 (TID 23) in 124 ms on 178.62.198.251 (executor 1) (7/18)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_8 in memory on 178.62.199.118:42185 (size: 14.7 MB, free: 3.0 GB)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_13 in memory on 178.62.198.251:36075 (size: 14.1 MB, free: 2.9 GB)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_14 in memory on 178.62.198.251:36075 (size: 15.0 MB, free: 2.9 GB)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_4 in memory on 178.62.199.118:39673 (size: 14.9 MB, free: 3.0 GB)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_3 in memory on 178.62.199.118:42185 (size: 14.8 MB, free: 3.0 GB)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_6 in memory on 178.62.199.118:42185 (size: 15.1 MB, free: 3.0 GB)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_1 in memory on 178.62.199.118:39673 (size: 14.9 MB, free: 3.0 GB)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_7 in memory on 178.62.199.118:39673 (size: 14.6 MB, free: 3.0 GB)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 4.0 (TID 27, 178.62.198.251, executor 1, partition 16, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 4.0 (TID 24) in 135 ms on 178.62.198.251 (executor 1) (8/18)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 4.0 (TID 28, 178.62.198.251, executor 1, partition 17, NODE_LOCAL, 7756 bytes)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 4.0 (TID 25) in 149 ms on 178.62.198.251 (executor 1) (9/18)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_15 in memory on 178.62.198.251:36075 (size: 15.1 MB, free: 2.9 GB)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 4.0 (TID 26) in 279 ms on 178.62.198.251 (executor 1) (10/18)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_16 in memory on 178.62.198.251:36075 (size: 15.2 MB, free: 2.9 GB)
20/04/14 07:58:54 INFO storage.BlockManagerInfo: Added rdd_7_17 in memory on 178.62.198.251:36075 (size: 14.7 MB, free: 2.9 GB)
20/04/14 07:58:54 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 4.0 (TID 28) in 222 ms on 178.62.198.251 (executor 1) (11/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 4.0 (TID 27) in 255 ms on 178.62.198.251 (executor 1) (12/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 4.0 (TID 19) in 1398 ms on 178.62.199.118 (executor 3) (13/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 4.0 (TID 14) in 1414 ms on 178.62.199.118 (executor 3) (14/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 4.0 (TID 17) in 1431 ms on 178.62.199.118 (executor 3) (15/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 4.0 (TID 18) in 1492 ms on 178.62.199.118 (executor 2) (16/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 4.0 (TID 15) in 1492 ms on 178.62.199.118 (executor 2) (17/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 12) in 1504 ms on 178.62.199.118 (executor 2) (18/18)
20/04/14 07:58:55 INFO cluster.YarnClusterScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/04/14 07:58:55 INFO scheduler.DAGScheduler: ResultStage 4 (takeSample at GaussianMixture.scala:470) finished in 1.513 s
20/04/14 07:58:55 INFO scheduler.DAGScheduler: Job 2 finished: takeSample at GaussianMixture.scala:470, took 1.524320 s
20/04/14 07:58:55 INFO spark.SparkContext: Starting job: takeSample at GaussianMixture.scala:470
20/04/14 07:58:55 INFO scheduler.DAGScheduler: Got job 3 (takeSample at GaussianMixture.scala:470) with 18 output partitions
20/04/14 07:58:55 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (takeSample at GaussianMixture.scala:470)
20/04/14 07:58:55 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
20/04/14 07:58:55 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/14 07:58:55 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (PartitionwiseSampledRDD[21] at takeSample at GaussianMixture.scala:470), which has no missing parents
20/04/14 07:58:55 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.1 KB, free 3.0 GB)
20/04/14 07:58:55 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.7 KB, free 3.0 GB)
20/04/14 07:58:55 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.198.251:41011 (size: 13.7 KB, free: 3.0 GB)
20/04/14 07:58:55 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1163
20/04/14 07:58:55 INFO scheduler.DAGScheduler: Submitting 18 missing tasks from ResultStage 6 (PartitionwiseSampledRDD[21] at takeSample at GaussianMixture.scala:470) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/04/14 07:58:55 INFO cluster.YarnClusterScheduler: Adding task set 6.0 with 18 tasks
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 29, 178.62.198.251, executor 1, partition 0, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 30, 178.62.199.118, executor 3, partition 3, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 31, 178.62.199.118, executor 2, partition 1, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 32, 178.62.198.251, executor 1, partition 2, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 33, 178.62.199.118, executor 3, partition 6, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 34, 178.62.199.118, executor 2, partition 4, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 35, 178.62.198.251, executor 1, partition 5, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 36, 178.62.199.118, executor 3, partition 8, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 37, 178.62.199.118, executor 2, partition 7, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.198.251:36075 (size: 13.7 KB, free: 2.9 GB)
20/04/14 07:58:55 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.199.118:42185 (size: 13.7 KB, free: 3.0 GB)
20/04/14 07:58:55 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.199.118:39673 (size: 13.7 KB, free: 3.0 GB)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 6.0 (TID 38, 178.62.198.251, executor 1, partition 9, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 6.0 (TID 39, 178.62.198.251, executor 1, partition 10, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 32) in 97 ms on 178.62.198.251 (executor 1) (1/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 29) in 98 ms on 178.62.198.251 (executor 1) (2/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 6.0 (TID 40, 178.62.198.251, executor 1, partition 11, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 6.0 (TID 35) in 101 ms on 178.62.198.251 (executor 1) (3/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 6.0 (TID 41, 178.62.198.251, executor 1, partition 12, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 6.0 (TID 38) in 47 ms on 178.62.198.251 (executor 1) (4/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 6.0 (TID 42, 178.62.198.251, executor 1, partition 13, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 6.0 (TID 43, 178.62.198.251, executor 1, partition 14, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 6.0 (TID 40) in 70 ms on 178.62.198.251 (executor 1) (5/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 6.0 (TID 39) in 75 ms on 178.62.198.251 (executor 1) (6/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 6.0 (TID 30) in 199 ms on 178.62.199.118 (executor 3) (7/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 6.0 (TID 36) in 200 ms on 178.62.199.118 (executor 3) (8/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 6.0 (TID 33) in 204 ms on 178.62.199.118 (executor 3) (9/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 6.0 (TID 37) in 209 ms on 178.62.199.118 (executor 2) (10/18)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 6.0 (TID 44, 178.62.198.251, executor 1, partition 15, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:55 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 6.0 (TID 41) in 73 ms on 178.62.198.251 (executor 1) (11/18)
20/04/14 07:58:56 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 6.0 (TID 34) in 218 ms on 178.62.199.118 (executor 2) (12/18)
20/04/14 07:58:56 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 31) in 225 ms on 178.62.199.118 (executor 2) (13/18)
20/04/14 07:58:56 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 6.0 (TID 45, 178.62.198.251, executor 1, partition 16, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:56 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 6.0 (TID 43) in 56 ms on 178.62.198.251 (executor 1) (14/18)
20/04/14 07:58:56 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 6.0 (TID 46, 178.62.198.251, executor 1, partition 17, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:58:56 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 6.0 (TID 42) in 66 ms on 178.62.198.251 (executor 1) (15/18)
20/04/14 07:58:56 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 6.0 (TID 45) in 39 ms on 178.62.198.251 (executor 1) (16/18)
20/04/14 07:58:56 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 6.0 (TID 44) in 53 ms on 178.62.198.251 (executor 1) (17/18)
20/04/14 07:58:56 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 6.0 (TID 46) in 36 ms on 178.62.198.251 (executor 1) (18/18)
20/04/14 07:58:56 INFO cluster.YarnClusterScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool 
20/04/14 07:58:56 INFO scheduler.DAGScheduler: ResultStage 6 (takeSample at GaussianMixture.scala:470) finished in 0.280 s
20/04/14 07:58:56 INFO scheduler.DAGScheduler: Job 3 finished: takeSample at GaussianMixture.scala:470, took 0.283373 s
20/04/14 07:58:56 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
20/04/14 07:58:56 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 115
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 91
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 38
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 69
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 85
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 73
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.199.118:39673 in memory (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.198.251:41011 in memory (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.199.118:42185 in memory (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.198.251:36075 in memory (size: 7.6 KB, free: 2.9 GB)
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 123
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 40
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 113
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 104
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 119
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 48
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 61
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 59
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 97
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 66
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 82
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 90
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 52
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 86
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 71
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 51
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 138
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 49
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 76
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 65
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 107
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 68
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 80
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 108
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 74
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 64
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 89
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 114
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 45
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 128
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 129
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 96
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 37
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 125
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 98
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 53
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 60
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 58
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 112
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 81
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 116
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 130
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 77
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 105
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 111
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 178.62.198.251:41011 in memory (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 178.62.198.251:36075 in memory (size: 7.6 KB, free: 2.9 GB)
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 122
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 109
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 94
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 47
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 131
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 42
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.198.251:41011 in memory (size: 13.7 KB, free: 3.0 GB)
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.198.251:36075 in memory (size: 13.7 KB, free: 2.9 GB)
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.199.118:42185 in memory (size: 13.7 KB, free: 3.0 GB)
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.199.118:39673 in memory (size: 13.7 KB, free: 3.0 GB)
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 83
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 106
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 110
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 92
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 99
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 124
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 44
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 135
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 126
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 100
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 133
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 75
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 103
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 56
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 95
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 132
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 41
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 79
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 39
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 50
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 134
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 46
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 136
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 102
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 54
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 57
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 55
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 101
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 93
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 70
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 84
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 78
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 121
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 118
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 67
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 120
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 43
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 137
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 127
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 117
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 72
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.198.251:41011 in memory (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.198.251:36075 in memory (size: 6.5 KB, free: 2.9 GB)
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.199.118:39673 in memory (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:58:56 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.199.118:42185 in memory (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 62
20/04/14 07:58:56 INFO spark.ContextCleaner: Cleaned accumulator 63
20/04/14 07:58:57 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 3.0 GB)
20/04/14 07:58:57 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 3.0 GB)
20/04/14 07:58:57 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.198.251:41011 (size: 85.0 B, free: 3.0 GB)
20/04/14 07:58:57 INFO spark.SparkContext: Created broadcast 6 from broadcast at GaussianMixture.scala:379
20/04/14 07:58:57 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.5 GB)
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 2.5 GB)
20/04/14 07:58:58 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.198.251:41011 (size: 2.7 MB, free: 3.0 GB)
20/04/14 07:58:58 INFO spark.SparkContext: Created broadcast 7 from broadcast at GaussianMixture.scala:380
20/04/14 07:58:58 INFO spark.SparkContext: Starting job: treeAggregate at GaussianMixture.scala:384
20/04/14 07:58:58 INFO scheduler.DAGScheduler: Registering RDD 23 (treeAggregate at GaussianMixture.scala:384) as input to shuffle 1
20/04/14 07:58:58 INFO scheduler.DAGScheduler: Got job 4 (treeAggregate at GaussianMixture.scala:384) with 3 output partitions
20/04/14 07:58:58 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (treeAggregate at GaussianMixture.scala:384)
20/04/14 07:58:58 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
20/04/14 07:58:58 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 8)
20/04/14 07:58:58 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[23] at treeAggregate at GaussianMixture.scala:384), which has no missing parents
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 19.7 KB, free 2.5 GB)
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 2.5 GB)
20/04/14 07:58:58 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.198.251:41011 (size: 9.2 KB, free: 3.0 GB)
20/04/14 07:58:58 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1163
20/04/14 07:58:58 INFO scheduler.DAGScheduler: Submitting 18 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[23] at treeAggregate at GaussianMixture.scala:384) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/04/14 07:58:58 INFO cluster.YarnClusterScheduler: Adding task set 8.0 with 18 tasks
20/04/14 07:58:58 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 8.0 (TID 47, 178.62.199.118, executor 3, partition 3, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:58:58 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 48, 178.62.199.118, executor 2, partition 1, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:58:58 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 49, 178.62.198.251, executor 1, partition 0, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:58:58 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 8.0 (TID 50, 178.62.199.118, executor 3, partition 6, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:58:58 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 8.0 (TID 51, 178.62.199.118, executor 2, partition 4, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:58:58 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 8.0 (TID 52, 178.62.198.251, executor 1, partition 2, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:58:58 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 8.0 (TID 53, 178.62.199.118, executor 3, partition 8, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:58:58 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 8.0 (TID 54, 178.62.199.118, executor 2, partition 7, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:58:58 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 8.0 (TID 55, 178.62.198.251, executor 1, partition 5, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:58:58 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.198.251:36075 (size: 9.2 KB, free: 2.9 GB)
20/04/14 07:58:58 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.199.118:42185 (size: 9.2 KB, free: 3.0 GB)
20/04/14 07:58:58 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.199.118:39673 (size: 9.2 KB, free: 3.0 GB)
20/04/14 07:58:58 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.199.118:39673 (size: 85.0 B, free: 3.0 GB)
20/04/14 07:58:58 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.198.251:36075 (size: 85.0 B, free: 2.9 GB)
20/04/14 07:58:58 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.199.118:42185 (size: 85.0 B, free: 3.0 GB)
20/04/14 07:58:58 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.199.118:39673 (size: 2.7 MB, free: 3.0 GB)
20/04/14 07:58:58 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.198.251:36075 (size: 2.7 MB, free: 2.8 GB)
20/04/14 07:58:58 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.199.118:42185 (size: 2.7 MB, free: 3.0 GB)
20/04/14 07:59:04 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 2.
20/04/14 07:59:04 INFO scheduler.DAGScheduler: Executor lost: 2 (epoch 1)
20/04/14 07:59:04 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
20/04/14 07:59:04 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_4 !
20/04/14 07:59:04 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_7 !
20/04/14 07:59:04 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_1 !
20/04/14 07:59:04 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, 178.62.199.118, 39673, None)
20/04/14 07:59:04 INFO storage.BlockManagerMaster: Removed 2 successfully in removeExecutor
20/04/14 07:59:04 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 2 (epoch 1)
20/04/14 07:59:04 INFO yarn.YarnAllocator: Completed container container_1586849858644_0006_01_000003 on host: 178.62.199.118 (state: COMPLETE, exit status: 143)
20/04/14 07:59:04 WARN yarn.YarnAllocator: Container from a bad node: container_1586849858644_0006_01_000003 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:04 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1586849858644_0006_01_000003 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:04 ERROR cluster.YarnClusterScheduler: Lost executor 2 on 178.62.199.118: Container from a bad node: container_1586849858644_0006_01_000003 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:04 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 8.0 (TID 54, 178.62.199.118, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000003 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:04 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 8.0 (TID 48, 178.62.199.118, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000003 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:04 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 8.0 (TID 51, 178.62.199.118, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000003 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:04 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
20/04/14 07:59:04 INFO storage.BlockManagerMaster: Removal of executor 2 requested
20/04/14 07:59:04 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 2
20/04/14 07:59:06 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 3.
20/04/14 07:59:06 INFO scheduler.DAGScheduler: Executor lost: 3 (epoch 2)
20/04/14 07:59:06 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.
20/04/14 07:59:06 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_8 !
20/04/14 07:59:06 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 3 core(s) and 6758 MB memory (including 614 MB of overhead)
20/04/14 07:59:06 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_3 !
20/04/14 07:59:06 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_6 !
20/04/14 07:59:06 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(3, 178.62.199.118, 42185, None)
20/04/14 07:59:06 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.
20/04/14 07:59:06 INFO storage.BlockManagerMaster: Removed 3 successfully in removeExecutor
20/04/14 07:59:06 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 3 (epoch 2)
20/04/14 07:59:06 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_01_000006 on host 178.62.199.118 for executor with ID 4
20/04/14 07:59:06 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/14 07:59:06 INFO yarn.YarnAllocator: Completed container container_1586849858644_0006_01_000005 on host: 178.62.199.118 (state: COMPLETE, exit status: 143)
20/04/14 07:59:06 WARN yarn.YarnAllocator: Container from a bad node: container_1586849858644_0006_01_000005 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:06 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 3 for reason Container from a bad node: container_1586849858644_0006_01_000005 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:06 ERROR cluster.YarnClusterScheduler: Lost executor 3 on 178.62.199.118: Container from a bad node: container_1586849858644_0006_01_000005 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:06 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 07:59:06 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 8.0 (TID 50, 178.62.199.118, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000005 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:06 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 8.0 (TID 53, 178.62.199.118, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000005 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:06 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 8.0 (TID 47, 178.62.199.118, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000005 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:06 INFO storage.BlockManagerMaster: Removal of executor 3 requested
20/04/14 07:59:06 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 3
20/04/14 07:59:06 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.
20/04/14 07:59:08 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.199.118:54526) with ID 4
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Starting task 3.1 in stage 8.0 (TID 56, 178.62.199.118, executor 4, partition 3, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Starting task 8.1 in stage 8.0 (TID 57, 178.62.199.118, executor 4, partition 8, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Starting task 6.1 in stage 8.0 (TID 58, 178.62.199.118, executor 4, partition 6, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:08 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.199.118:44505 with 3.0 GB RAM, BlockManagerId(4, 178.62.199.118, 44505, None)
20/04/14 07:59:08 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 1.
20/04/14 07:59:08 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 3)
20/04/14 07:59:08 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
20/04/14 07:59:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_13 !
20/04/14 07:59:08 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 3 core(s) and 6758 MB memory (including 614 MB of overhead)
20/04/14 07:59:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_10 !
20/04/14 07:59:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_16 !
20/04/14 07:59:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_14 !
20/04/14 07:59:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_0 !
20/04/14 07:59:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_5 !
20/04/14 07:59:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_2 !
20/04/14 07:59:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_15 !
20/04/14 07:59:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_17 !
20/04/14 07:59:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_12 !
20/04/14 07:59:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_9 !
20/04/14 07:59:08 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_11 !
20/04/14 07:59:08 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 178.62.198.251, 36075, None)
20/04/14 07:59:08 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.
20/04/14 07:59:08 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
20/04/14 07:59:08 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 3)
20/04/14 07:59:08 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.199.118:44505 (size: 9.2 KB, free: 3.0 GB)
20/04/14 07:59:08 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_01_000007 on host 178.62.199.118 for executor with ID 5
20/04/14 07:59:08 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/14 07:59:08 INFO yarn.YarnAllocator: Completed container container_1586849858644_0006_01_000002 on host: 178.62.198.251 (state: COMPLETE, exit status: 143)
20/04/14 07:59:08 WARN yarn.YarnAllocator: Container from a bad node: container_1586849858644_0006_01_000002 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:08 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 07:59:08 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container from a bad node: container_1586849858644_0006_01_000002 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:08 ERROR cluster.YarnClusterScheduler: Lost executor 1 on 178.62.198.251: Container from a bad node: container_1586849858644_0006_01_000002 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:08 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 8.0 (TID 55, 178.62.198.251, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000002 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:08 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 8.0 (TID 49, 178.62.198.251, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000002 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:08 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 8.0 (TID 52, 178.62.198.251, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000002 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:08 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
20/04/14 07:59:08 INFO storage.BlockManagerMaster: Removal of executor 1 requested
20/04/14 07:59:08 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 1
20/04/14 07:59:08 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.199.118:54526
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Starting task 4.1 in stage 8.0 (TID 59, 178.62.199.118, executor 4, partition 4, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Starting task 1.1 in stage 8.0 (TID 60, 178.62.199.118, executor 4, partition 1, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Starting task 7.1 in stage 8.0 (TID 61, 178.62.199.118, executor 4, partition 7, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:08 WARN scheduler.TaskSetManager: Lost task 6.1 in stage 8.0 (TID 58, 178.62.199.118, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=6, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Task 6.1 in stage 8.0 (TID 58) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 07:59:08 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 8 (treeAggregate at GaussianMixture.scala:384) as failed due to a fetch failure from ShuffleMapStage 7 (rdd at GaussianMixture.scala:348)
20/04/14 07:59:08 WARN scheduler.TaskSetManager: Lost task 8.1 in stage 8.0 (TID 57, 178.62.199.118, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=8, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Task 8.1 in stage 8.0 (TID 57) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 07:59:08 WARN scheduler.TaskSetManager: Lost task 3.1 in stage 8.0 (TID 56, 178.62.199.118, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=3, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Task 3.1 in stage 8.0 (TID 56) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 07:59:08 INFO scheduler.DAGScheduler: ShuffleMapStage 8 (treeAggregate at GaussianMixture.scala:384) failed in 10.514 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/04/14 07:59:08 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 7 (rdd at GaussianMixture.scala:348) and ShuffleMapStage 8 (treeAggregate at GaussianMixture.scala:384) due to fetch failure
20/04/14 07:59:08 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.199.118:54526
20/04/14 07:59:08 WARN scheduler.TaskSetManager: Lost task 7.1 in stage 8.0 (TID 61, 178.62.199.118, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=7, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Task 7.1 in stage 8.0 (TID 61) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 07:59:08 WARN scheduler.TaskSetManager: Lost task 1.1 in stage 8.0 (TID 60, 178.62.199.118, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=1, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Task 1.1 in stage 8.0 (TID 60) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 07:59:08 WARN scheduler.TaskSetManager: Lost task 4.1 in stage 8.0 (TID 59, 178.62.199.118, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=4, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Task 4.1 in stage 8.0 (TID 59) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 07:59:08 INFO cluster.YarnClusterScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool 
20/04/14 07:59:08 INFO scheduler.DAGScheduler: Resubmitting failed stages
20/04/14 07:59:08 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[5] at rdd at GaussianMixture.scala:348), which has no missing parents
20/04/14 07:59:08 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 14.9 KB, free 2.5 GB)
20/04/14 07:59:08 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2.5 GB)
20/04/14 07:59:08 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.198.251:41011 (size: 6.9 KB, free: 3.0 GB)
20/04/14 07:59:08 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1163
20/04/14 07:59:08 INFO scheduler.DAGScheduler: Submitting 9 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[5] at rdd at GaussianMixture.scala:348) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8))
20/04/14 07:59:08 INFO cluster.YarnClusterScheduler: Adding task set 7.0 with 9 tasks
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 62, 178.62.199.118, executor 4, partition 0, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 63, 178.62.199.118, executor 4, partition 1, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:08 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 7.0 (TID 64, 178.62.199.118, executor 4, partition 2, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:08 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.199.118:44505 (size: 6.9 KB, free: 3.0 GB)
20/04/14 07:59:09 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.199.118:44505 (size: 32.5 KB, free: 3.0 GB)
20/04/14 07:59:10 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.199.118:54546) with ID 5
20/04/14 07:59:10 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 7.0 (TID 65, 178.62.199.118, executor 5, partition 3, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:10 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 7.0 (TID 66, 178.62.199.118, executor 5, partition 4, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:10 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 7.0 (TID 67, 178.62.199.118, executor 5, partition 5, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:10 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.199.118:35143 with 3.0 GB RAM, BlockManagerId(5, 178.62.199.118, 35143, None)
20/04/14 07:59:11 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.199.118:35143 (size: 6.9 KB, free: 3.0 GB)
20/04/14 07:59:11 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 3 core(s) and 6758 MB memory (including 614 MB of overhead)
20/04/14 07:59:11 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.
20/04/14 07:59:11 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.199.118:35143 (size: 32.5 KB, free: 3.0 GB)
20/04/14 07:59:11 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 7.0 (TID 68, 178.62.199.118, executor 4, partition 6, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:11 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 7.0 (TID 64) in 3020 ms on 178.62.199.118 (executor 4) (1/9)
20/04/14 07:59:11 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 7.0 (TID 69, 178.62.199.118, executor 4, partition 7, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:11 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 62) in 3049 ms on 178.62.199.118 (executor 4) (2/9)
20/04/14 07:59:11 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 7.0 (TID 70, 178.62.199.118, executor 4, partition 8, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:11 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 63) in 3051 ms on 178.62.199.118 (executor 4) (3/9)
20/04/14 07:59:11 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 7.0 (TID 70) in 104 ms on 178.62.199.118 (executor 4) (4/9)
20/04/14 07:59:12 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 7.0 (TID 68) in 721 ms on 178.62.199.118 (executor 4) (5/9)
20/04/14 07:59:12 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 7.0 (TID 69) in 732 ms on 178.62.199.118 (executor 4) (6/9)
20/04/14 07:59:12 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_01_000009 on host 178.62.198.251 for executor with ID 6
20/04/14 07:59:12 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/14 07:59:12 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 07:59:14 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 7.0 (TID 65) in 3683 ms on 178.62.199.118 (executor 5) (7/9)
20/04/14 07:59:14 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 7.0 (TID 67) in 3683 ms on 178.62.199.118 (executor 5) (8/9)
20/04/14 07:59:14 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 7.0 (TID 66) in 3684 ms on 178.62.199.118 (executor 5) (9/9)
20/04/14 07:59:14 INFO cluster.YarnClusterScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool 
20/04/14 07:59:14 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (rdd at GaussianMixture.scala:348) finished in 5.403 s
20/04/14 07:59:14 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/04/14 07:59:14 INFO scheduler.DAGScheduler: running: Set()
20/04/14 07:59:14 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 9, ShuffleMapStage 8)
20/04/14 07:59:14 INFO scheduler.DAGScheduler: failed: Set()
20/04/14 07:59:14 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[23] at treeAggregate at GaussianMixture.scala:384), which has no missing parents
20/04/14 07:59:14 INFO scheduler.OutputCommitCoordinator: Reusing state from previous attempt of stage 8.
20/04/14 07:59:14 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.7 KB, free 2.5 GB)
20/04/14 07:59:14 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.2 KB, free 2.5 GB)
20/04/14 07:59:14 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.198.251:41011 (size: 9.2 KB, free: 3.0 GB)
20/04/14 07:59:14 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1163
20/04/14 07:59:14 INFO scheduler.DAGScheduler: Submitting 18 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[23] at treeAggregate at GaussianMixture.scala:384) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/04/14 07:59:14 INFO cluster.YarnClusterScheduler: Adding task set 8.1 with 18 tasks
20/04/14 07:59:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.1 (TID 71, 178.62.199.118, executor 5, partition 0, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:14 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.1 (TID 72, 178.62.199.118, executor 4, partition 1, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:14 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 8.1 (TID 73, 178.62.199.118, executor 5, partition 2, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:14 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 8.1 (TID 74, 178.62.199.118, executor 4, partition 3, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:14 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 8.1 (TID 75, 178.62.199.118, executor 5, partition 4, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:14 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 8.1 (TID 76, 178.62.199.118, executor 4, partition 5, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:14 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.199.118:44505 (size: 9.2 KB, free: 3.0 GB)
20/04/14 07:59:14 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.199.118:54526
20/04/14 07:59:14 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.199.118:35143 (size: 9.2 KB, free: 3.0 GB)
20/04/14 07:59:14 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.199.118:54546
20/04/14 07:59:14 INFO storage.BlockManagerInfo: Added rdd_7_5 in memory on 178.62.199.118:44505 (size: 14.7 MB, free: 3.0 GB)
20/04/14 07:59:14 INFO storage.BlockManagerInfo: Added rdd_7_1 in memory on 178.62.199.118:44505 (size: 14.9 MB, free: 3.0 GB)
20/04/14 07:59:14 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.198.251:41462) with ID 6
20/04/14 07:59:14 INFO storage.BlockManagerInfo: Added rdd_7_3 in memory on 178.62.199.118:44505 (size: 14.8 MB, free: 3.0 GB)
20/04/14 07:59:14 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.198.251:33143 with 3.0 GB RAM, BlockManagerId(6, 178.62.198.251, 33143, None)
20/04/14 07:59:14 INFO storage.BlockManagerInfo: Added rdd_7_4 in memory on 178.62.199.118:35143 (size: 14.9 MB, free: 3.0 GB)
20/04/14 07:59:14 INFO storage.BlockManagerInfo: Added rdd_7_2 in memory on 178.62.199.118:35143 (size: 14.5 MB, free: 3.0 GB)
20/04/14 07:59:14 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on 178.62.199.118:35143 (size: 15.0 MB, free: 3.0 GB)
20/04/14 07:59:15 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.199.118:44505 (size: 85.0 B, free: 3.0 GB)
20/04/14 07:59:15 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.199.118:44505 (size: 2.7 MB, free: 3.0 GB)
20/04/14 07:59:15 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.199.118:35143 (size: 85.0 B, free: 3.0 GB)
20/04/14 07:59:15 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.199.118:35143 (size: 2.7 MB, free: 3.0 GB)
20/04/14 07:59:17 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 8.1 (TID 77, 178.62.198.251, executor 6, partition 6, RACK_LOCAL, 7745 bytes)
20/04/14 07:59:17 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 8.1 (TID 78, 178.62.198.251, executor 6, partition 7, RACK_LOCAL, 7745 bytes)
20/04/14 07:59:17 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 8.1 (TID 79, 178.62.198.251, executor 6, partition 8, RACK_LOCAL, 7745 bytes)
20/04/14 07:59:17 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.198.251:33143 (size: 9.2 KB, free: 3.0 GB)
20/04/14 07:59:17 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.198.251:41462
20/04/14 07:59:18 INFO storage.BlockManagerInfo: Added rdd_7_7 in memory on 178.62.198.251:33143 (size: 14.6 MB, free: 3.0 GB)
20/04/14 07:59:18 INFO storage.BlockManagerInfo: Added rdd_7_8 in memory on 178.62.198.251:33143 (size: 14.7 MB, free: 3.0 GB)
20/04/14 07:59:18 INFO storage.BlockManagerInfo: Added rdd_7_6 in memory on 178.62.198.251:33143 (size: 15.1 MB, free: 3.0 GB)
20/04/14 07:59:19 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.198.251:33143 (size: 85.0 B, free: 3.0 GB)
20/04/14 07:59:19 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.198.251:33143 (size: 2.7 MB, free: 3.0 GB)
20/04/14 07:59:24 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 8.1 (TID 80, 178.62.199.118, executor 4, partition 9, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:24 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 8.1 (TID 72, 178.62.199.118, executor 4): java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)

20/04/14 07:59:24 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 4.
20/04/14 07:59:24 INFO scheduler.DAGScheduler: Executor lost: 4 (epoch 5)
20/04/14 07:59:24 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 4 from BlockManagerMaster.
20/04/14 07:59:24 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_5 !
20/04/14 07:59:24 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_3 !
20/04/14 07:59:24 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_1 !
20/04/14 07:59:24 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(4, 178.62.199.118, 44505, None)
20/04/14 07:59:24 INFO storage.BlockManagerMaster: Removed 4 successfully in removeExecutor
20/04/14 07:59:24 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 4 (epoch 5)
20/04/14 07:59:24 INFO yarn.YarnAllocator: Completed container container_1586849858644_0006_01_000006 on host: 178.62.199.118 (state: COMPLETE, exit status: 143)
20/04/14 07:59:24 WARN yarn.YarnAllocator: Container from a bad node: container_1586849858644_0006_01_000006 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:24 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container from a bad node: container_1586849858644_0006_01_000006 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:24 ERROR cluster.YarnClusterScheduler: Lost executor 4 on 178.62.199.118: Container from a bad node: container_1586849858644_0006_01_000006 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:24 WARN scheduler.TaskSetManager: Lost task 9.0 in stage 8.1 (TID 80, 178.62.199.118, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000006 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:24 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 8.1 (TID 74, 178.62.199.118, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000006 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:24 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 8.1 (TID 76, 178.62.199.118, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000006 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:24 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 4 from BlockManagerMaster.
20/04/14 07:59:24 INFO storage.BlockManagerMaster: Removal of executor 4 requested
20/04/14 07:59:24 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 4
20/04/14 07:59:25 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 6.
20/04/14 07:59:25 INFO scheduler.DAGScheduler: Executor lost: 6 (epoch 6)
20/04/14 07:59:25 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 3 core(s) and 6758 MB memory (including 614 MB of overhead)
20/04/14 07:59:25 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 6 from BlockManagerMaster.
20/04/14 07:59:25 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_8 !
20/04/14 07:59:25 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_7 !
20/04/14 07:59:25 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_6 !
20/04/14 07:59:25 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(6, 178.62.198.251, 33143, None)
20/04/14 07:59:25 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.
20/04/14 07:59:25 INFO storage.BlockManagerMaster: Removed 6 successfully in removeExecutor
20/04/14 07:59:25 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 6 (epoch 6)
20/04/14 07:59:25 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_01_000010 on host 178.62.198.251 for executor with ID 7
20/04/14 07:59:25 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/14 07:59:25 INFO yarn.YarnAllocator: Completed container container_1586849858644_0006_01_000009 on host: 178.62.198.251 (state: COMPLETE, exit status: 143)
20/04/14 07:59:25 WARN yarn.YarnAllocator: Container from a bad node: container_1586849858644_0006_01_000009 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:25 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 6 for reason Container from a bad node: container_1586849858644_0006_01_000009 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:25 ERROR cluster.YarnClusterScheduler: Lost executor 6 on 178.62.198.251: Container from a bad node: container_1586849858644_0006_01_000009 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:25 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 8.1 (TID 77, 178.62.198.251, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000009 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:25 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 8.1 (TID 79, 178.62.198.251, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000009 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:25 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 8.1 (TID 78, 178.62.198.251, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000009 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:25 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 07:59:25 INFO storage.BlockManagerMaster: Removal of executor 6 requested
20/04/14 07:59:25 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 6
20/04/14 07:59:25 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 6 from BlockManagerMaster.
20/04/14 07:59:25 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 5.
20/04/14 07:59:25 INFO scheduler.DAGScheduler: Executor lost: 5 (epoch 7)
20/04/14 07:59:25 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 5 from BlockManagerMaster.
20/04/14 07:59:25 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_4 !
20/04/14 07:59:25 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_2 !
20/04/14 07:59:25 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_0 !
20/04/14 07:59:25 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(5, 178.62.199.118, 35143, None)
20/04/14 07:59:25 INFO storage.BlockManagerMaster: Removed 5 successfully in removeExecutor
20/04/14 07:59:25 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 5 (epoch 7)
20/04/14 07:59:25 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 3 core(s) and 6758 MB memory (including 614 MB of overhead)
20/04/14 07:59:25 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.
20/04/14 07:59:26 INFO yarn.YarnAllocator: Completed container container_1586849858644_0006_01_000007 on host: 178.62.199.118 (state: COMPLETE, exit status: 143)
20/04/14 07:59:26 WARN yarn.YarnAllocator: Container from a bad node: container_1586849858644_0006_01_000007 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:26 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 5 for reason Container from a bad node: container_1586849858644_0006_01_000007 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:26 ERROR cluster.YarnClusterScheduler: Lost executor 5 on 178.62.199.118: Container from a bad node: container_1586849858644_0006_01_000007 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:26 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 8.1 (TID 71, 178.62.199.118, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000007 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:26 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 8.1 (TID 73, 178.62.199.118, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000007 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:26 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 8.1 (TID 75, 178.62.199.118, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_01_000007 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:26 INFO storage.BlockManagerMaster: Removal of executor 5 requested
20/04/14 07:59:26 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 5 from BlockManagerMaster.
20/04/14 07:59:26 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 5
20/04/14 07:59:26 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 11, (reason: Max number of executor failures (6) reached)
20/04/14 07:59:26 INFO spark.SparkContext: Invoking stop() from shutdown hook
20/04/14 07:59:26 INFO server.AbstractConnector: Stopped Spark@60b3ba3b{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
20/04/14 07:59:26 INFO ui.SparkUI: Stopped Spark web UI at http://178.62.198.251:45627
20/04/14 07:59:26 INFO scheduler.DAGScheduler: ShuffleMapStage 8 (treeAggregate at GaussianMixture.scala:384) failed in 11.992 s due to Stage cancelled because SparkContext was shut down
20/04/14 07:59:26 INFO scheduler.DAGScheduler: Job 4 failed: treeAggregate at GaussianMixture.scala:384, took 28.129904 s
20/04/14 07:59:26 ERROR util.Instrumentation: org.apache.spark.SparkException: Job 4 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:933)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:931)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:931)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2130)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2043)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1137)
	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1.apply(GaussianMixture.scala:384)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1.apply(GaussianMixture.scala:340)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.clustering.GaussianMixture.fit(GaussianMixture.scala:340)
	at org.apache.spark.ml.clustering.GaussianMixture.fit(GaussianMixture.scala:291)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

20/04/14 07:59:26 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s).
20/04/14 07:59:26 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors
20/04/14 07:59:26 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/04/14 07:59:26 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/04/14 07:59:26 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/04/14 07:59:26 INFO memory.MemoryStore: MemoryStore cleared
20/04/14 07:59:26 INFO storage.BlockManager: BlockManager stopped
20/04/14 07:59:26 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
20/04/14 07:59:26 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/04/14 07:59:26 INFO spark.SparkContext: Successfully stopped SparkContext
20/04/14 07:59:26 INFO util.ShutdownHookManager: Shutdown hook called
20/04/14 07:59:26 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/spark-0b56a87f-3422-48b4-8eb1-686661993ba6
20/04/14 07:59:26 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/spark-0b56a87f-3422-48b4-8eb1-686661993ba6/pyspark-3b276363-079a-41ca-93c5-4e7c6d24150d
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_1586849858644_0006_02_000002 on 178.62.198.251_42461
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:31752
Log Contents:
20/04/14 07:59:31 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21461@178.62.198.251
20/04/14 07:59:31 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:59:31 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:59:31 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:59:31 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:31 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:31 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:31 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:31 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:32 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 67 ms (0 ms spent in bootstraps)
20/04/14 07:59:32 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:32 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:32 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:32 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:32 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:32 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 3 ms (0 ms spent in bootstraps)
20/04/14 07:59:32 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-236f8c71-0d21-4710-b1be-a33775dc038c
20/04/14 07:59:32 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 07:59:32 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.199.118:44353
20/04/14 07:59:32 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 07:59:32 INFO executor.Executor: Starting executor ID 1 on host 178.62.198.251
20/04/14 07:59:32 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37949.
20/04/14 07:59:32 INFO netty.NettyBlockTransferService: Server created on 178.62.198.251:37949
20/04/14 07:59:32 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 07:59:32 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, 178.62.198.251, 37949, None)
20/04/14 07:59:32 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, 178.62.198.251, 37949, None)
20/04/14 07:59:32 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, 178.62.198.251, 37949, None)
20/04/14 07:59:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1
20/04/14 07:59:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4
20/04/14 07:59:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7
20/04/14 07:59:37 INFO executor.Executor: Running task 6.0 in stage 1.0 (TID 7)
20/04/14 07:59:37 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
20/04/14 07:59:37 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 4)
20/04/14 07:59:38 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
20/04/14 07:59:38 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:45281 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:59:38 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.5 KB, free 3.0 GB)
20/04/14 07:59:38 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 84 ms
20/04/14 07:59:38 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.1 KB, free 3.0 GB)
20/04/14 07:59:38 INFO codegen.CodeGenerator: Code generated in 226.183469 ms
20/04/14 07:59:38 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00006-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4956444, partition values: [empty row]
20/04/14 07:59:38 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00001-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4785170, partition values: [empty row]
20/04/14 07:59:38 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00004-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4935174, partition values: [empty row]
20/04/14 07:59:38 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/14 07:59:38 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:38045 after 1 ms (0 ms spent in bootstraps)
20/04/14 07:59:38 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 07:59:38 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 19 ms
20/04/14 07:59:38 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 540.2 KB, free 3.0 GB)
20/04/14 07:59:40 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:40 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:40 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:40 INFO codegen.CodeGenerator: Code generated in 22.576757 ms
20/04/14 07:59:40 INFO codegen.CodeGenerator: Code generated in 20.836417 ms
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 5926 records.
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11734 records.
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11557 records.
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:40 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:40 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:40 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: block read in memory in 44 ms. row count = 5926
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: block read in memory in 45 ms. row count = 11557
20/04/14 07:59:40 INFO hadoop.InternalParquetRecordReader: block read in memory in 42 ms. row count = 11734
20/04/14 07:59:41 INFO executor.Executor: Finished task 3.0 in stage 1.0 (TID 4). 1530 bytes result sent to driver
20/04/14 07:59:41 INFO executor.Executor: Finished task 6.0 in stage 1.0 (TID 7). 1530 bytes result sent to driver
20/04/14 07:59:41 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1530 bytes result sent to driver
20/04/14 07:59:41 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10
20/04/14 07:59:41 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 10)
20/04/14 07:59:41 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/14 07:59:41 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3
20/04/14 07:59:41 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.6 KB, free 3.0 GB)
20/04/14 07:59:41 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 9 ms
20/04/14 07:59:41 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.1 KB, free 3.0 GB)
20/04/14 07:59:41 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:41 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.199.118:44353)
20/04/14 07:59:42 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:59:42 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:42 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:41839 after 1 ms (0 ms spent in bootstraps)
20/04/14 07:59:42 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 12 ms
20/04/14 07:59:42 INFO memory.MemoryStore: Block rdd_7_0 stored as values in memory (estimated size 15.0 MB, free 3.0 GB)
20/04/14 07:59:42 INFO codegen.CodeGenerator: Code generated in 5.084818 ms
20/04/14 07:59:42 INFO codegen.CodeGenerator: Code generated in 18.078303 ms
20/04/14 07:59:42 INFO codegen.CodeGenerator: Code generated in 9.997463 ms
20/04/14 07:59:42 INFO executor.Executor: 1 block locks were not released by TID = 10:
[rdd_7_0]
20/04/14 07:59:42 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 10). 2765 bytes result sent to driver
20/04/14 07:59:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11
20/04/14 07:59:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12
20/04/14 07:59:42 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 12)
20/04/14 07:59:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15
20/04/14 07:59:42 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
20/04/14 07:59:42 INFO executor.Executor: Running task 4.0 in stage 4.0 (TID 15)
20/04/14 07:59:42 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 11)
20/04/14 07:59:42 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.6 KB, free 3.0 GB)
20/04/14 07:59:42 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 16 ms
20/04/14 07:59:42 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 07:59:42 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:42 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 2 ms
20/04/14 07:59:42 INFO storage.BlockManager: Found block rdd_7_0 locally
20/04/14 07:59:42 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 14 ms
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_1 stored as values in memory (estimated size 14.9 MB, free 3.0 GB)
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_4 stored as values in memory (estimated size 14.9 MB, free 3.0 GB)
20/04/14 07:59:43 INFO executor.Executor: Finished task 4.0 in stage 4.0 (TID 15). 1626 bytes result sent to driver
20/04/14 07:59:43 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 11). 1368 bytes result sent to driver
20/04/14 07:59:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20
20/04/14 07:59:43 INFO executor.Executor: Running task 9.0 in stage 4.0 (TID 20)
20/04/14 07:59:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 21
20/04/14 07:59:43 INFO executor.Executor: Running task 10.0 in stage 4.0 (TID 21)
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 8 ms
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 9 ms
20/04/14 07:59:43 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 12). 1583 bytes result sent to driver
20/04/14 07:59:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22
20/04/14 07:59:43 INFO executor.Executor: Running task 11.0 in stage 4.0 (TID 22)
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 2 ms
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_11 stored as values in memory (estimated size 14.4 MB, free 3.0 GB)
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_10 stored as values in memory (estimated size 14.7 MB, free 3.0 GB)
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_9 stored as values in memory (estimated size 14.6 MB, free 2.9 GB)
20/04/14 07:59:43 INFO executor.Executor: Finished task 11.0 in stage 4.0 (TID 22). 1540 bytes result sent to driver
20/04/14 07:59:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 23
20/04/14 07:59:43 INFO executor.Executor: Running task 12.0 in stage 4.0 (TID 23)
20/04/14 07:59:43 INFO executor.Executor: Finished task 9.0 in stage 4.0 (TID 20). 1540 bytes result sent to driver
20/04/14 07:59:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 24
20/04/14 07:59:43 INFO executor.Executor: Running task 13.0 in stage 4.0 (TID 24)
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/14 07:59:43 INFO executor.Executor: Finished task 10.0 in stage 4.0 (TID 21). 1540 bytes result sent to driver
20/04/14 07:59:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25
20/04/14 07:59:43 INFO executor.Executor: Running task 14.0 in stage 4.0 (TID 25)
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 2 ms
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_13 stored as values in memory (estimated size 14.1 MB, free 2.9 GB)
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_12 stored as values in memory (estimated size 14.8 MB, free 2.9 GB)
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_14 stored as values in memory (estimated size 15.0 MB, free 2.9 GB)
20/04/14 07:59:43 INFO executor.Executor: Finished task 13.0 in stage 4.0 (TID 24). 1583 bytes result sent to driver
20/04/14 07:59:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 26
20/04/14 07:59:43 INFO executor.Executor: Running task 15.0 in stage 4.0 (TID 26)
20/04/14 07:59:43 INFO executor.Executor: Finished task 12.0 in stage 4.0 (TID 23). 1583 bytes result sent to driver
20/04/14 07:59:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 27
20/04/14 07:59:43 INFO executor.Executor: Running task 16.0 in stage 4.0 (TID 27)
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/14 07:59:43 INFO executor.Executor: Finished task 14.0 in stage 4.0 (TID 25). 1540 bytes result sent to driver
20/04/14 07:59:43 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 28
20/04/14 07:59:43 INFO executor.Executor: Running task 17.0 in stage 4.0 (TID 28)
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_15 stored as values in memory (estimated size 15.1 MB, free 2.9 GB)
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_16 stored as values in memory (estimated size 15.2 MB, free 2.9 GB)
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_17 stored as values in memory (estimated size 14.7 MB, free 2.9 GB)
20/04/14 07:59:43 INFO executor.Executor: Finished task 15.0 in stage 4.0 (TID 26). 1583 bytes result sent to driver
20/04/14 07:59:43 INFO executor.Executor: Finished task 16.0 in stage 4.0 (TID 27). 1583 bytes result sent to driver
20/04/14 07:59:43 INFO executor.Executor: Finished task 17.0 in stage 4.0 (TID 28). 1583 bytes result sent to driver
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 31
20/04/14 07:59:44 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 31)
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 34
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 37
20/04/14 07:59:44 INFO executor.Executor: Running task 4.0 in stage 6.0 (TID 37)
20/04/14 07:59:44 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 34)
20/04/14 07:59:44 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
20/04/14 07:59:44 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.7 KB, free 2.9 GB)
20/04/14 07:59:44 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 8 ms
20/04/14 07:59:44 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.1 KB, free 2.9 GB)
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_0 locally
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_4 locally
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_1 locally
20/04/14 07:59:44 INFO executor.Executor: Finished task 4.0 in stage 6.0 (TID 37). 8901 bytes result sent to driver
20/04/14 07:59:44 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 31). 7037 bytes result sent to driver
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 38
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 39
20/04/14 07:59:44 INFO executor.Executor: Running task 9.0 in stage 6.0 (TID 38)
20/04/14 07:59:44 INFO executor.Executor: Running task 10.0 in stage 6.0 (TID 39)
20/04/14 07:59:44 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 34). 5928 bytes result sent to driver
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 40
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_10 locally
20/04/14 07:59:44 INFO executor.Executor: Running task 11.0 in stage 6.0 (TID 40)
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_9 locally
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_11 locally
20/04/14 07:59:44 INFO executor.Executor: Finished task 11.0 in stage 6.0 (TID 40). 4596 bytes result sent to driver
20/04/14 07:59:44 INFO executor.Executor: Finished task 10.0 in stage 6.0 (TID 39). 3170 bytes result sent to driver
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 41
20/04/14 07:59:44 INFO executor.Executor: Running task 12.0 in stage 6.0 (TID 41)
20/04/14 07:59:44 INFO executor.Executor: Finished task 9.0 in stage 6.0 (TID 38). 8579 bytes result sent to driver
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 42
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 43
20/04/14 07:59:44 INFO executor.Executor: Running task 13.0 in stage 6.0 (TID 42)
20/04/14 07:59:44 INFO executor.Executor: Running task 14.0 in stage 6.0 (TID 43)
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_13 locally
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_12 locally
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_14 locally
20/04/14 07:59:44 INFO executor.Executor: Finished task 13.0 in stage 6.0 (TID 42). 10419 bytes result sent to driver
20/04/14 07:59:44 INFO executor.Executor: Finished task 12.0 in stage 6.0 (TID 41). 3021 bytes result sent to driver
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 44
20/04/14 07:59:44 INFO executor.Executor: Running task 15.0 in stage 6.0 (TID 44)
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 45
20/04/14 07:59:44 INFO executor.Executor: Running task 16.0 in stage 6.0 (TID 45)
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_15 locally
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_16 locally
20/04/14 07:59:44 INFO executor.Executor: Finished task 14.0 in stage 6.0 (TID 43). 3237 bytes result sent to driver
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 46
20/04/14 07:59:44 INFO executor.Executor: Running task 17.0 in stage 6.0 (TID 46)
20/04/14 07:59:45 INFO executor.Executor: Finished task 15.0 in stage 6.0 (TID 44). 19397 bytes result sent to driver
20/04/14 07:59:45 INFO storage.BlockManager: Found block rdd_7_17 locally
20/04/14 07:59:45 INFO executor.Executor: Finished task 16.0 in stage 6.0 (TID 45). 1304 bytes result sent to driver
20/04/14 07:59:45 INFO executor.Executor: Finished task 17.0 in stage 6.0 (TID 46). 2738 bytes result sent to driver
20/04/14 07:59:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 48
20/04/14 07:59:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 51
20/04/14 07:59:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 54
20/04/14 07:59:47 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 48)
20/04/14 07:59:47 INFO executor.Executor: Running task 4.0 in stage 8.0 (TID 54)
20/04/14 07:59:47 INFO executor.Executor: Running task 1.0 in stage 8.0 (TID 51)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 2.9 GB)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 17 ms
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 19.7 KB, free 2.9 GB)
20/04/14 07:59:47 INFO storage.BlockManager: Found block rdd_7_1 locally
20/04/14 07:59:47 INFO storage.BlockManager: Found block rdd_7_0 locally
20/04/14 07:59:47 INFO storage.BlockManager: Found block rdd_7_4 locally
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 2.9 GB)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 8 ms
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 2.9 GB)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 2.8 GB)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 18 ms
20/04/14 07:59:48 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.3 GB)
20/04/14 07:59:53 ERROR executor.Executor: Exception in task 4.0 in stage 8.0 (TID 54)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:53 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/14 07:59:53 INFO storage.DiskBlockManager: Shutdown hook called
20/04/14 07:59:53 ERROR util.SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 54,5,main]
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:53 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 56
20/04/14 07:59:53 INFO executor.Executor: Running task 9.0 in stage 8.0 (TID 56)
20/04/14 07:59:53 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:124
Log Contents:
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill %p"
#   Executing /bin/sh -c "kill 21461"...
End of LogType:stdout



Container: container_1586849858644_0006_01_000007 on 178.62.199.118_35057
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:16163
Log Contents:
20/04/14 07:59:09 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21394@178.62.199.118
20/04/14 07:59:09 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:59:09 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:59:09 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:59:09 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:09 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:09 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:09 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:09 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:10 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:39507 after 50 ms (0 ms spent in bootstraps)
20/04/14 07:59:10 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:10 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:10 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:10 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:10 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:10 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:39507 after 4 ms (0 ms spent in bootstraps)
20/04/14 07:59:10 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-da705fd1-f0d1-414e-94a5-d5d9fd31189f
20/04/14 07:59:10 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 07:59:10 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.198.251:39507
20/04/14 07:59:10 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 07:59:10 INFO executor.Executor: Starting executor ID 5 on host 178.62.199.118
20/04/14 07:59:10 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35143.
20/04/14 07:59:10 INFO netty.NettyBlockTransferService: Server created on 178.62.199.118:35143
20/04/14 07:59:10 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 07:59:10 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(5, 178.62.199.118, 35143, None)
20/04/14 07:59:10 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(5, 178.62.199.118, 35143, None)
20/04/14 07:59:10 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(5, 178.62.199.118, 35143, None)
20/04/14 07:59:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 65
20/04/14 07:59:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 66
20/04/14 07:59:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 67
20/04/14 07:59:10 INFO executor.Executor: Running task 3.0 in stage 7.0 (TID 65)
20/04/14 07:59:10 INFO executor.Executor: Running task 4.0 in stage 7.0 (TID 66)
20/04/14 07:59:10 INFO executor.Executor: Running task 5.0 in stage 7.0 (TID 67)
20/04/14 07:59:10 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache
20/04/14 07:59:10 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9
20/04/14 07:59:10 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44505 after 17 ms (0 ms spent in bootstraps)
20/04/14 07:59:11 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.9 KB, free 3.0 GB)
20/04/14 07:59:11 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 358 ms
20/04/14 07:59:11 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 07:59:11 INFO codegen.CodeGenerator: Code generated in 194.248368 ms
20/04/14 07:59:11 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00003-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4855590, partition values: [empty row]
20/04/14 07:59:11 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00000-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4791031, partition values: [empty row]
20/04/14 07:59:11 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00004-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4935174, partition values: [empty row]
20/04/14 07:59:11 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/14 07:59:11 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 07:59:11 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 9 ms
20/04/14 07:59:11 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 540.2 KB, free 3.0 GB)
20/04/14 07:59:12 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:12 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:12 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:13 INFO codegen.CodeGenerator: Code generated in 16.323882 ms
20/04/14 07:59:13 INFO codegen.CodeGenerator: Code generated in 11.24664 ms
20/04/14 07:59:13 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 9550 records.
20/04/14 07:59:13 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11557 records.
20/04/14 07:59:13 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 5980 records.
20/04/14 07:59:13 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:13 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:13 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:13 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:13 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:13 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:13 INFO hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 5980
20/04/14 07:59:13 INFO hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 11557
20/04/14 07:59:13 INFO hadoop.InternalParquetRecordReader: block read in memory in 37 ms. row count = 9550
20/04/14 07:59:14 INFO executor.Executor: Finished task 3.0 in stage 7.0 (TID 65). 1530 bytes result sent to driver
20/04/14 07:59:14 INFO executor.Executor: Finished task 5.0 in stage 7.0 (TID 67). 1487 bytes result sent to driver
20/04/14 07:59:14 INFO executor.Executor: Finished task 4.0 in stage 7.0 (TID 66). 1487 bytes result sent to driver
20/04/14 07:59:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 71
20/04/14 07:59:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 73
20/04/14 07:59:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 75
20/04/14 07:59:14 INFO executor.Executor: Running task 0.0 in stage 8.1 (TID 71)
20/04/14 07:59:14 INFO executor.Executor: Running task 4.0 in stage 8.1 (TID 75)
20/04/14 07:59:14 INFO executor.Executor: Running task 2.0 in stage 8.1 (TID 73)
20/04/14 07:59:14 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache
20/04/14 07:59:14 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
20/04/14 07:59:14 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.2 KB, free 3.0 GB)
20/04/14 07:59:14 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 23 ms
20/04/14 07:59:14 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.7 KB, free 3.0 GB)
20/04/14 07:59:14 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:14 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:14 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.198.251:39507)
20/04/14 07:59:14 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:14 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:59:14 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:14 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:14 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 15 ms
20/04/14 07:59:14 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:14 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 19 ms
20/04/14 07:59:14 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 30 ms
20/04/14 07:59:14 INFO memory.MemoryStore: Block rdd_7_4 stored as values in memory (estimated size 14.9 MB, free 3.0 GB)
20/04/14 07:59:14 INFO memory.MemoryStore: Block rdd_7_2 stored as values in memory (estimated size 14.5 MB, free 3.0 GB)
20/04/14 07:59:14 INFO codegen.CodeGenerator: Code generated in 5.763649 ms
20/04/14 07:59:14 INFO memory.MemoryStore: Block rdd_7_0 stored as values in memory (estimated size 15.0 MB, free 3.0 GB)
20/04/14 07:59:14 INFO codegen.CodeGenerator: Code generated in 16.857369 ms
20/04/14 07:59:15 INFO codegen.CodeGenerator: Code generated in 15.747118 ms
20/04/14 07:59:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/14 07:59:15 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 3.0 GB)
20/04/14 07:59:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 59 ms
20/04/14 07:59:15 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 3.0 GB)
20/04/14 07:59:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/14 07:59:15 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 3.0 GB)
20/04/14 07:59:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 45 ms
20/04/14 07:59:16 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.4 GB)
20/04/14 07:59:25 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/14 07:59:25 INFO storage.DiskBlockManager: Shutdown hook called
20/04/14 07:59:25 INFO util.ShutdownHookManager: Shutdown hook called
20/04/14 07:59:25 ERROR executor.Executor: Exception in task 4.0 in stage 8.1 (TID 75)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:25 INFO executor.Executor: Not reporting error to driver during JVM shutdown.
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:124
Log Contents:
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill %p"
#   Executing /bin/sh -c "kill 21394"...
End of LogType:stdout



Container: container_1586849858644_0006_01_000003 on 178.62.199.118_35057
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:22134
Log Contents:
20/04/14 07:58:42 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21113@178.62.199.118
20/04/14 07:58:42 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:58:42 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:58:42 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:58:43 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:58:43 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:58:43 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:58:43 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:58:43 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:58:43 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:39507 after 64 ms (0 ms spent in bootstraps)
20/04/14 07:58:43 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:58:43 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:58:43 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:58:43 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:58:43 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:58:43 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:39507 after 9 ms (0 ms spent in bootstraps)
20/04/14 07:58:43 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-6ffe74e3-09fe-4553-9ec1-bc495adfd59b
20/04/14 07:58:43 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 07:58:43 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.198.251:39507
20/04/14 07:58:43 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 07:58:43 INFO executor.Executor: Starting executor ID 2 on host 178.62.199.118
20/04/14 07:58:44 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39673.
20/04/14 07:58:44 INFO netty.NettyBlockTransferService: Server created on 178.62.199.118:39673
20/04/14 07:58:44 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 07:58:44 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(2, 178.62.199.118, 39673, None)
20/04/14 07:58:44 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(2, 178.62.199.118, 39673, None)
20/04/14 07:58:44 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(2, 178.62.199.118, 39673, None)
20/04/14 07:58:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3
20/04/14 07:58:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6
20/04/14 07:58:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9
20/04/14 07:58:49 INFO executor.Executor: Running task 5.0 in stage 1.0 (TID 6)
20/04/14 07:58:49 INFO executor.Executor: Running task 8.0 in stage 1.0 (TID 9)
20/04/14 07:58:49 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 3)
20/04/14 07:58:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
20/04/14 07:58:49 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:41011 after 4 ms (0 ms spent in bootstraps)
20/04/14 07:58:49 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.5 KB, free 3.0 GB)
20/04/14 07:58:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 84 ms
20/04/14 07:58:49 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.1 KB, free 3.0 GB)
20/04/14 07:58:49 INFO codegen.CodeGenerator: Code generated in 218.180933 ms
20/04/14 07:58:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00008-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-213844, partition values: [empty row]
20/04/14 07:58:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00007-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4944913, partition values: [empty row]
20/04/14 07:58:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00000-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4791031, partition values: [empty row]
20/04/14 07:58:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/14 07:58:49 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 07:58:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 18 ms
20/04/14 07:58:50 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 540.2 KB, free 3.0 GB)
20/04/14 07:58:51 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:58:51 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:58:51 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:58:51 INFO codegen.CodeGenerator: Code generated in 30.665174 ms
20/04/14 07:58:51 INFO codegen.CodeGenerator: Code generated in 19.205078 ms
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 5980 records.
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11674 records.
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 472 records.
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:58:51 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:58:51 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:58:51 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: block read in memory in 50 ms. row count = 472
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: block read in memory in 54 ms. row count = 5980
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: block read in memory in 52 ms. row count = 11674
20/04/14 07:58:52 INFO executor.Executor: Finished task 8.0 in stage 1.0 (TID 9). 1530 bytes result sent to driver
20/04/14 07:58:52 INFO executor.Executor: Finished task 5.0 in stage 1.0 (TID 6). 1487 bytes result sent to driver
20/04/14 07:58:52 INFO executor.Executor: Finished task 2.0 in stage 1.0 (TID 3). 1487 bytes result sent to driver
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12
20/04/14 07:58:54 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 12)
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18
20/04/14 07:58:54 INFO executor.Executor: Running task 4.0 in stage 4.0 (TID 15)
20/04/14 07:58:54 INFO executor.Executor: Running task 7.0 in stage 4.0 (TID 18)
20/04/14 07:58:54 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/14 07:58:54 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
20/04/14 07:58:54 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:36075 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:58:54 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.6 KB, free 3.0 GB)
20/04/14 07:58:54 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 45 ms
20/04/14 07:58:54 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 07:58:54 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:58:54 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:58:54 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.198.251:39507)
20/04/14 07:58:54 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:58:54 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:42185 after 4 ms (0 ms spent in bootstraps)
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 26 ms
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 26 ms
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 35 ms
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_4 stored as values in memory (estimated size 14.9 MB, free 3.0 GB)
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_1 stored as values in memory (estimated size 14.9 MB, free 3.0 GB)
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_7 stored as values in memory (estimated size 14.6 MB, free 3.0 GB)
20/04/14 07:58:54 INFO codegen.CodeGenerator: Code generated in 20.166154 ms
20/04/14 07:58:54 INFO codegen.CodeGenerator: Code generated in 24.50729 ms
20/04/14 07:58:55 INFO codegen.CodeGenerator: Code generated in 19.390322 ms
20/04/14 07:58:55 INFO executor.Executor: Finished task 7.0 in stage 4.0 (TID 18). 1583 bytes result sent to driver
20/04/14 07:58:55 INFO executor.Executor: Finished task 4.0 in stage 4.0 (TID 15). 1583 bytes result sent to driver
20/04/14 07:58:55 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 12). 1583 bytes result sent to driver
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 31
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 34
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 37
20/04/14 07:58:55 INFO executor.Executor: Running task 7.0 in stage 6.0 (TID 37)
20/04/14 07:58:55 INFO executor.Executor: Running task 4.0 in stage 6.0 (TID 34)
20/04/14 07:58:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
20/04/14 07:58:55 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 31)
20/04/14 07:58:55 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.7 KB, free 3.0 GB)
20/04/14 07:58:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 12 ms
20/04/14 07:58:55 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.1 KB, free 3.0 GB)
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_7 locally
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_4 locally
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_1 locally
20/04/14 07:58:55 INFO executor.Executor: Finished task 7.0 in stage 6.0 (TID 37). 1347 bytes result sent to driver
20/04/14 07:58:56 INFO executor.Executor: Finished task 4.0 in stage 6.0 (TID 34). 9448 bytes result sent to driver
20/04/14 07:58:56 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 31). 2913 bytes result sent to driver
20/04/14 07:58:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 48
20/04/14 07:58:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 51
20/04/14 07:58:58 INFO executor.Executor: Running task 1.0 in stage 8.0 (TID 48)
20/04/14 07:58:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 54
20/04/14 07:58:58 INFO executor.Executor: Running task 4.0 in stage 8.0 (TID 51)
20/04/14 07:58:58 INFO executor.Executor: Running task 7.0 in stage 8.0 (TID 54)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 3.0 GB)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 11 ms
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 19.7 KB, free 3.0 GB)
20/04/14 07:58:58 INFO storage.BlockManager: Found block rdd_7_4 locally
20/04/14 07:58:58 INFO storage.BlockManager: Found block rdd_7_1 locally
20/04/14 07:58:58 INFO storage.BlockManager: Found block rdd_7_7 locally
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 3.0 GB)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 9 ms
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 3.0 GB)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 3.0 GB)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 25 ms
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.4 GB)
20/04/14 07:59:02 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/14 07:59:02 ERROR executor.Executor: Exception in task 1.0 in stage 8.0 (TID 48)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:02 INFO executor.Executor: Not reporting error to driver during JVM shutdown.
20/04/14 07:59:03 ERROR util.SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 48,5,main]
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:03 INFO storage.DiskBlockManager: Shutdown hook called
20/04/14 07:59:03 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:124
Log Contents:
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill %p"
#   Executing /bin/sh -c "kill 21113"...
End of LogType:stdout



Container: container_1586849858644_0006_01_000006 on 178.62.199.118_35057
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:28789
Log Contents:
20/04/14 07:59:06 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21330@178.62.199.118
20/04/14 07:59:06 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:59:06 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:59:06 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:59:07 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:07 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:07 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:07 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:07 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:07 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:39507 after 43 ms (0 ms spent in bootstraps)
20/04/14 07:59:07 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:07 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:07 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:07 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:07 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:07 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:39507 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:59:07 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-51d6e58a-18f6-4bee-ba26-1f2ebf2b00ba
20/04/14 07:59:07 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 07:59:08 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.198.251:39507
20/04/14 07:59:08 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 07:59:08 INFO executor.Executor: Starting executor ID 4 on host 178.62.199.118
20/04/14 07:59:08 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44505.
20/04/14 07:59:08 INFO netty.NettyBlockTransferService: Server created on 178.62.199.118:44505
20/04/14 07:59:08 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 07:59:08 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(4, 178.62.199.118, 44505, None)
20/04/14 07:59:08 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(4, 178.62.199.118, 44505, None)
20/04/14 07:59:08 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(4, 178.62.199.118, 44505, None)
20/04/14 07:59:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 56
20/04/14 07:59:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 57
20/04/14 07:59:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 58
20/04/14 07:59:08 INFO executor.Executor: Running task 3.1 in stage 8.0 (TID 56)
20/04/14 07:59:08 INFO executor.Executor: Running task 8.1 in stage 8.0 (TID 57)
20/04/14 07:59:08 INFO executor.Executor: Running task 6.1 in stage 8.0 (TID 58)
20/04/14 07:59:08 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/14 07:59:08 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/14 07:59:08 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:41011 after 1 ms (0 ms spent in bootstraps)
20/04/14 07:59:08 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 3.0 GB)
20/04/14 07:59:08 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 72 ms
20/04/14 07:59:08 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 19.7 KB, free 3.0 GB)
20/04/14 07:59:08 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:08 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:08 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:08 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.198.251:39507)
20/04/14 07:59:08 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:59:08 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 07:59:08 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 07:59:08 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 07:59:08 WARN storage.BlockManager: Putting block rdd_7_6 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 07:59:08 WARN storage.BlockManager: Putting block rdd_7_3 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 07:59:08 WARN storage.BlockManager: Putting block rdd_7_8 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 07:59:08 WARN storage.BlockManager: Block rdd_7_8 could not be removed as it was not found on disk or in memory
20/04/14 07:59:08 WARN storage.BlockManager: Block rdd_7_3 could not be removed as it was not found on disk or in memory
20/04/14 07:59:08 WARN storage.BlockManager: Block rdd_7_6 could not be removed as it was not found on disk or in memory
20/04/14 07:59:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 59
20/04/14 07:59:08 INFO executor.Executor: Running task 4.1 in stage 8.0 (TID 59)
20/04/14 07:59:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 60
20/04/14 07:59:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 61
20/04/14 07:59:08 INFO executor.Executor: Running task 1.1 in stage 8.0 (TID 60)
20/04/14 07:59:08 INFO executor.Executor: Running task 7.1 in stage 8.0 (TID 61)
20/04/14 07:59:08 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:08 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.198.251:39507)
20/04/14 07:59:08 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:08 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:08 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:59:08 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 07:59:08 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 07:59:08 ERROR spark.MapOutputTracker: Missing an output location for shuffle 0
20/04/14 07:59:08 WARN storage.BlockManager: Putting block rdd_7_1 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 07:59:08 WARN storage.BlockManager: Putting block rdd_7_4 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 07:59:08 WARN storage.BlockManager: Putting block rdd_7_7 failed due to exception org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0.
20/04/14 07:59:08 WARN storage.BlockManager: Block rdd_7_7 could not be removed as it was not found on disk or in memory
20/04/14 07:59:08 WARN storage.BlockManager: Block rdd_7_1 could not be removed as it was not found on disk or in memory
20/04/14 07:59:08 WARN storage.BlockManager: Block rdd_7_4 could not be removed as it was not found on disk or in memory
20/04/14 07:59:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 62
20/04/14 07:59:08 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 62)
20/04/14 07:59:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 63
20/04/14 07:59:08 INFO executor.Executor: Running task 1.0 in stage 7.0 (TID 63)
20/04/14 07:59:08 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 64
20/04/14 07:59:08 INFO executor.Executor: Running task 2.0 in stage 7.0 (TID 64)
20/04/14 07:59:08 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache
20/04/14 07:59:08 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9
20/04/14 07:59:08 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.9 KB, free 3.0 GB)
20/04/14 07:59:08 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 10 ms
20/04/14 07:59:08 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 07:59:09 INFO codegen.CodeGenerator: Code generated in 276.601112 ms
20/04/14 07:59:09 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00007-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4944913, partition values: [empty row]
20/04/14 07:59:09 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00005-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4950456, partition values: [empty row]
20/04/14 07:59:09 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00006-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4956444, partition values: [empty row]
20/04/14 07:59:09 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/14 07:59:09 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 07:59:09 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 8 ms
20/04/14 07:59:09 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 540.2 KB, free 3.0 GB)
20/04/14 07:59:10 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:10 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:10 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:10 INFO codegen.CodeGenerator: Code generated in 15.638768 ms
20/04/14 07:59:10 INFO codegen.CodeGenerator: Code generated in 15.929995 ms
20/04/14 07:59:10 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11734 records.
20/04/14 07:59:10 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11674 records.
20/04/14 07:59:10 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11838 records.
20/04/14 07:59:10 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:10 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:10 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:10 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:10 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:10 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:10 INFO hadoop.InternalParquetRecordReader: block read in memory in 35 ms. row count = 11674
20/04/14 07:59:10 INFO hadoop.InternalParquetRecordReader: block read in memory in 35 ms. row count = 11734
20/04/14 07:59:10 INFO hadoop.InternalParquetRecordReader: block read in memory in 35 ms. row count = 11838
20/04/14 07:59:11 INFO executor.Executor: Finished task 2.0 in stage 7.0 (TID 64). 1530 bytes result sent to driver
20/04/14 07:59:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68
20/04/14 07:59:11 INFO executor.Executor: Running task 6.0 in stage 7.0 (TID 68)
20/04/14 07:59:11 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 62). 1487 bytes result sent to driver
20/04/14 07:59:11 INFO executor.Executor: Finished task 1.0 in stage 7.0 (TID 63). 1487 bytes result sent to driver
20/04/14 07:59:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 69
20/04/14 07:59:11 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 70
20/04/14 07:59:11 INFO executor.Executor: Running task 7.0 in stage 7.0 (TID 69)
20/04/14 07:59:11 INFO executor.Executor: Running task 8.0 in stage 7.0 (TID 70)
20/04/14 07:59:11 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00001-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4785170, partition values: [empty row]
20/04/14 07:59:11 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00002-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4735846, partition values: [empty row]
20/04/14 07:59:11 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00008-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-213844, partition values: [empty row]
20/04/14 07:59:11 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:11 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:11 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:11 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 472 records.
20/04/14 07:59:11 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:11 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 6080 records.
20/04/14 07:59:11 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:11 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 5926 records.
20/04/14 07:59:11 INFO hadoop.InternalParquetRecordReader: block read in memory in 5 ms. row count = 472
20/04/14 07:59:11 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:11 INFO hadoop.InternalParquetRecordReader: block read in memory in 11 ms. row count = 5926
20/04/14 07:59:11 INFO hadoop.InternalParquetRecordReader: block read in memory in 25 ms. row count = 6080
20/04/14 07:59:11 INFO executor.Executor: Finished task 8.0 in stage 7.0 (TID 70). 1444 bytes result sent to driver
20/04/14 07:59:12 INFO executor.Executor: Finished task 6.0 in stage 7.0 (TID 68). 1487 bytes result sent to driver
20/04/14 07:59:12 INFO executor.Executor: Finished task 7.0 in stage 7.0 (TID 69). 1487 bytes result sent to driver
20/04/14 07:59:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 72
20/04/14 07:59:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 74
20/04/14 07:59:14 INFO executor.Executor: Running task 3.0 in stage 8.1 (TID 74)
20/04/14 07:59:14 INFO executor.Executor: Running task 1.0 in stage 8.1 (TID 72)
20/04/14 07:59:14 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache
20/04/14 07:59:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 76
20/04/14 07:59:14 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
20/04/14 07:59:14 INFO executor.Executor: Running task 5.0 in stage 8.1 (TID 76)
20/04/14 07:59:14 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.2 KB, free 3.0 GB)
20/04/14 07:59:14 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 10 ms
20/04/14 07:59:14 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.7 KB, free 3.0 GB)
20/04/14 07:59:14 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:14 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.198.251:39507)
20/04/14 07:59:14 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:14 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:14 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:59:14 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 6 local blocks and 3 remote blocks
20/04/14 07:59:14 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 6 local blocks and 3 remote blocks
20/04/14 07:59:14 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 6 local blocks and 3 remote blocks
20/04/14 07:59:14 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:35143 after 3 ms (0 ms spent in bootstraps)
20/04/14 07:59:14 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 25 ms
20/04/14 07:59:14 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 25 ms
20/04/14 07:59:14 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 27 ms
20/04/14 07:59:14 INFO memory.MemoryStore: Block rdd_7_5 stored as values in memory (estimated size 14.7 MB, free 3.0 GB)
20/04/14 07:59:14 INFO memory.MemoryStore: Block rdd_7_1 stored as values in memory (estimated size 14.9 MB, free 3.0 GB)
20/04/14 07:59:14 INFO memory.MemoryStore: Block rdd_7_3 stored as values in memory (estimated size 14.8 MB, free 3.0 GB)
20/04/14 07:59:14 INFO codegen.CodeGenerator: Code generated in 10.24639 ms
20/04/14 07:59:14 INFO codegen.CodeGenerator: Code generated in 61.155337 ms
20/04/14 07:59:15 INFO codegen.CodeGenerator: Code generated in 9.145489 ms
20/04/14 07:59:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/14 07:59:15 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 3.0 GB)
20/04/14 07:59:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 8 ms
20/04/14 07:59:15 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 3.0 GB)
20/04/14 07:59:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/14 07:59:15 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 3.0 GB)
20/04/14 07:59:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 21 ms
20/04/14 07:59:16 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.4 GB)
20/04/14 07:59:23 ERROR executor.Executor: Exception in task 1.0 in stage 8.1 (TID 72)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:23 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/14 07:59:24 INFO storage.DiskBlockManager: Shutdown hook called
20/04/14 07:59:24 INFO util.ShutdownHookManager: Shutdown hook called
20/04/14 07:59:24 ERROR util.SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 72,5,main]
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 80
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:124
Log Contents:
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill %p"
#   Executing /bin/sh -c "kill 21330"...
End of LogType:stdout



Container: container_1586849858644_0006_02_000007 on 178.62.199.118_35057
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:7518
Log Contents:
20/04/14 08:00:01 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21825@178.62.199.118
20/04/14 08:00:01 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 08:00:01 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 08:00:01 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 08:00:01 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 08:00:01 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 08:00:01 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 08:00:01 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 08:00:01 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 08:00:01 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 45 ms (0 ms spent in bootstraps)
20/04/14 08:00:02 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 08:00:02 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 08:00:02 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 08:00:02 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 08:00:02 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 08:00:02 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 2 ms (0 ms spent in bootstraps)
20/04/14 08:00:02 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-686e28e8-c134-4b2f-bd3d-fd9fc90937d2
20/04/14 08:00:02 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 08:00:02 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.199.118:44353
20/04/14 08:00:02 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 08:00:02 INFO executor.Executor: Starting executor ID 6 on host 178.62.199.118
20/04/14 08:00:02 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39973.
20/04/14 08:00:02 INFO netty.NettyBlockTransferService: Server created on 178.62.199.118:39973
20/04/14 08:00:02 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 08:00:02 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(6, 178.62.199.118, 39973, None)
20/04/14 08:00:02 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(6, 178.62.199.118, 39973, None)
20/04/14 08:00:02 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(6, 178.62.199.118, 39973, None)
20/04/14 08:00:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 77
20/04/14 08:00:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 78
20/04/14 08:00:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 79
20/04/14 08:00:07 INFO executor.Executor: Running task 6.0 in stage 8.1 (TID 77)
20/04/14 08:00:07 INFO executor.Executor: Running task 8.0 in stage 8.1 (TID 79)
20/04/14 08:00:07 INFO executor.Executor: Running task 7.0 in stage 8.1 (TID 78)
20/04/14 08:00:07 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache
20/04/14 08:00:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
20/04/14 08:00:07 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:45281 after 2 ms (0 ms spent in bootstraps)
20/04/14 08:00:07 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.2 KB, free 3.0 GB)
20/04/14 08:00:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 68 ms
20/04/14 08:00:07 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.7 KB, free 3.0 GB)
20/04/14 08:00:07 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 08:00:07 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 08:00:07 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 08:00:07 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.199.118:44353)
20/04/14 08:00:07 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 08:00:07 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 0 local blocks and 9 remote blocks
20/04/14 08:00:07 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 0 local blocks and 9 remote blocks
20/04/14 08:00:07 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 0 local blocks and 9 remote blocks
20/04/14 08:00:07 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:45589 after 2 ms (0 ms spent in bootstraps)
20/04/14 08:00:07 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:35331 after 2 ms (0 ms spent in bootstraps)
20/04/14 08:00:07 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 11 ms
20/04/14 08:00:07 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 19 ms
20/04/14 08:00:07 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 19 ms
20/04/14 08:00:08 INFO memory.MemoryStore: Block rdd_7_7 stored as values in memory (estimated size 14.6 MB, free 3.0 GB)
20/04/14 08:00:08 INFO memory.MemoryStore: Block rdd_7_6 stored as values in memory (estimated size 15.1 MB, free 3.0 GB)
20/04/14 08:00:08 INFO memory.MemoryStore: Block rdd_7_8 stored as values in memory (estimated size 14.7 MB, free 3.0 GB)
20/04/14 08:00:08 INFO codegen.CodeGenerator: Code generated in 121.171978 ms
20/04/14 08:00:08 INFO codegen.CodeGenerator: Code generated in 24.294056 ms
20/04/14 08:00:09 INFO codegen.CodeGenerator: Code generated in 11.858415 ms
20/04/14 08:00:09 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/14 08:00:09 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 3.0 GB)
20/04/14 08:00:09 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 9 ms
20/04/14 08:00:09 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 3.0 GB)
20/04/14 08:00:09 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/14 08:00:09 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 3.0 GB)
20/04/14 08:00:09 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 15 ms
20/04/14 08:00:10 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.4 GB)
20/04/14 08:00:14 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/14 08:00:14 INFO storage.DiskBlockManager: Shutdown hook called
20/04/14 08:00:14 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:124
Log Contents:
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill %p"
#   Executing /bin/sh -c "kill 21825"...
End of LogType:stdout



Container: container_1586849858644_0006_01_000005 on 178.62.199.118_35057
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:22141
Log Contents:
20/04/14 07:58:43 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21143@178.62.199.118
20/04/14 07:58:43 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:58:43 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:58:43 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:58:44 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:58:44 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:58:44 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:58:44 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:58:44 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:58:44 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:39507 after 51 ms (0 ms spent in bootstraps)
20/04/14 07:58:44 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:58:44 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:58:44 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:58:44 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:58:44 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:58:44 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:39507 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:58:44 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-63cbcf54-79e8-4191-8dbd-fcfa1c77e14e
20/04/14 07:58:44 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 07:58:44 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.198.251:39507
20/04/14 07:58:44 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 07:58:44 INFO executor.Executor: Starting executor ID 3 on host 178.62.199.118
20/04/14 07:58:44 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42185.
20/04/14 07:58:44 INFO netty.NettyBlockTransferService: Server created on 178.62.199.118:42185
20/04/14 07:58:44 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 07:58:44 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(3, 178.62.199.118, 42185, None)
20/04/14 07:58:44 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(3, 178.62.199.118, 42185, None)
20/04/14 07:58:44 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(3, 178.62.199.118, 42185, None)
20/04/14 07:58:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1
20/04/14 07:58:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4
20/04/14 07:58:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7
20/04/14 07:58:49 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
20/04/14 07:58:49 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 4)
20/04/14 07:58:49 INFO executor.Executor: Running task 6.0 in stage 1.0 (TID 7)
20/04/14 07:58:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
20/04/14 07:58:49 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:41011 after 1 ms (0 ms spent in bootstraps)
20/04/14 07:58:49 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.5 KB, free 3.0 GB)
20/04/14 07:58:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 75 ms
20/04/14 07:58:49 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.1 KB, free 3.0 GB)
20/04/14 07:58:49 INFO codegen.CodeGenerator: Code generated in 229.187259 ms
20/04/14 07:58:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00006-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4956444, partition values: [empty row]
20/04/14 07:58:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/14 07:58:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00004-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4935174, partition values: [empty row]
20/04/14 07:58:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00001-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4785170, partition values: [empty row]
20/04/14 07:58:49 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:39673 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:58:50 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 07:58:50 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 125 ms
20/04/14 07:58:50 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 540.2 KB, free 3.0 GB)
20/04/14 07:58:51 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:58:51 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:58:51 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:58:51 INFO codegen.CodeGenerator: Code generated in 52.998765 ms
20/04/14 07:58:51 INFO codegen.CodeGenerator: Code generated in 14.238106 ms
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 5926 records.
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11734 records.
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11557 records.
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:58:51 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:58:51 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:58:51 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: block read in memory in 68 ms. row count = 11734
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: block read in memory in 56 ms. row count = 11557
20/04/14 07:58:51 INFO hadoop.InternalParquetRecordReader: block read in memory in 71 ms. row count = 5926
20/04/14 07:58:52 INFO executor.Executor: Finished task 3.0 in stage 1.0 (TID 4). 1530 bytes result sent to driver
20/04/14 07:58:52 INFO executor.Executor: Finished task 6.0 in stage 1.0 (TID 7). 1487 bytes result sent to driver
20/04/14 07:58:53 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1487 bytes result sent to driver
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17
20/04/14 07:58:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19
20/04/14 07:58:54 INFO executor.Executor: Running task 6.0 in stage 4.0 (TID 17)
20/04/14 07:58:54 INFO executor.Executor: Running task 3.0 in stage 4.0 (TID 14)
20/04/14 07:58:54 INFO executor.Executor: Running task 8.0 in stage 4.0 (TID 19)
20/04/14 07:58:54 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/14 07:58:54 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
20/04/14 07:58:54 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.6 KB, free 3.0 GB)
20/04/14 07:58:54 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 9 ms
20/04/14 07:58:54 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 07:58:54 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:58:54 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:58:54 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:58:54 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.198.251:39507)
20/04/14 07:58:54 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:58:54 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:36075 after 4 ms (0 ms spent in bootstraps)
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 33 ms
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 44 ms
20/04/14 07:58:54 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 45 ms
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_8 stored as values in memory (estimated size 14.7 MB, free 3.0 GB)
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_3 stored as values in memory (estimated size 14.8 MB, free 3.0 GB)
20/04/14 07:58:54 INFO memory.MemoryStore: Block rdd_7_6 stored as values in memory (estimated size 15.1 MB, free 3.0 GB)
20/04/14 07:58:54 INFO codegen.CodeGenerator: Code generated in 13.047586 ms
20/04/14 07:58:54 INFO codegen.CodeGenerator: Code generated in 40.424687 ms
20/04/14 07:58:55 INFO codegen.CodeGenerator: Code generated in 13.775475 ms
20/04/14 07:58:55 INFO executor.Executor: Finished task 8.0 in stage 4.0 (TID 19). 1583 bytes result sent to driver
20/04/14 07:58:55 INFO executor.Executor: Finished task 3.0 in stage 4.0 (TID 14). 1583 bytes result sent to driver
20/04/14 07:58:55 INFO executor.Executor: Finished task 6.0 in stage 4.0 (TID 17). 1583 bytes result sent to driver
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 30
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 33
20/04/14 07:58:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 36
20/04/14 07:58:55 INFO executor.Executor: Running task 6.0 in stage 6.0 (TID 33)
20/04/14 07:58:55 INFO executor.Executor: Running task 3.0 in stage 6.0 (TID 30)
20/04/14 07:58:55 INFO executor.Executor: Running task 8.0 in stage 6.0 (TID 36)
20/04/14 07:58:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
20/04/14 07:58:55 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.7 KB, free 3.0 GB)
20/04/14 07:58:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 10 ms
20/04/14 07:58:55 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.1 KB, free 3.0 GB)
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_3 locally
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_8 locally
20/04/14 07:58:55 INFO storage.BlockManager: Found block rdd_7_6 locally
20/04/14 07:58:55 INFO executor.Executor: Finished task 3.0 in stage 6.0 (TID 30). 7134 bytes result sent to driver
20/04/14 07:58:55 INFO executor.Executor: Finished task 8.0 in stage 6.0 (TID 36). 2267 bytes result sent to driver
20/04/14 07:58:55 INFO executor.Executor: Finished task 6.0 in stage 6.0 (TID 33). 2514 bytes result sent to driver
20/04/14 07:58:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 47
20/04/14 07:58:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 50
20/04/14 07:58:58 INFO executor.Executor: Running task 6.0 in stage 8.0 (TID 50)
20/04/14 07:58:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 53
20/04/14 07:58:58 INFO executor.Executor: Running task 3.0 in stage 8.0 (TID 47)
20/04/14 07:58:58 INFO executor.Executor: Running task 8.0 in stage 8.0 (TID 53)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 3.0 GB)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 15 ms
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 19.7 KB, free 3.0 GB)
20/04/14 07:58:58 INFO storage.BlockManager: Found block rdd_7_8 locally
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/14 07:58:58 INFO storage.BlockManager: Found block rdd_7_3 locally
20/04/14 07:58:58 INFO storage.BlockManager: Found block rdd_7_6 locally
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 3.0 GB)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 16 ms
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 3.0 GB)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 3.0 GB)
20/04/14 07:58:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 22 ms
20/04/14 07:58:58 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.4 GB)
20/04/14 07:59:05 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/14 07:59:05 INFO storage.DiskBlockManager: Shutdown hook called
20/04/14 07:59:05 ERROR executor.Executor: Exception in task 3.0 in stage 8.0 (TID 47)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:05 INFO executor.Executor: Not reporting error to driver during JVM shutdown.
20/04/14 07:59:05 ERROR util.SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 47,5,main]
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:05 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:124
Log Contents:
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill %p"
#   Executing /bin/sh -c "kill 21143"...
End of LogType:stdout



Container: container_1586849858644_0006_02_000001 on 178.62.199.118_35057
===========================================================================
LogType:pyspark.log
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:24265
Log Contents:
[PYTHON] 2020-04-14 07:59:34,306.306 INFO clustering - wrapper: perform_experiment keyword arguments:
[PYTHON] 2020-04-14 07:59:34,306.306 INFO clustering - wrapper: in_files: ['/data/df_3-shingles_sparse-binary-vectors.parquet']
[PYTHON] 2020-04-14 07:59:34,306.306 INFO clustering - wrapper: distances: ['cosine', 'euclidean']
[PYTHON] 2020-04-14 07:59:34,306.306 INFO clustering - wrapper: ks: [2, 4]
[PYTHON] 2020-04-14 07:59:34,306.306 INFO clustering - wrapper: models: [<class 'pyspark.ml.clustering.GaussianMixture'>]
[PYTHON] 2020-04-14 07:59:34,306.306 INFO clustering - wrapper: result_dfs_list: []
[PYTHON] 2020-04-14 07:59:34,308.308 INFO clustering - read_and_repartition: reading /data/df_3-shingles_sparse-binary-vectors.parquet into 18 partitions
[PYTHON] 2020-04-14 07:59:37,324.324 INFO clustering - wrapper: read_and_repartition finished in 3.02s
[PYTHON] 2020-04-14 08:00:19,252.252 INFO java_gateway - send_command: Error while receiving.
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1159, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
[PYTHON] 2020-04-14 08:00:19,254.254 ERROR java_gateway - send_command: Exception while sending command.
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1159, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 985, in send_command
    response = connection.send_command(command)
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1164, in send_command
    "Error while receiving", e, proto.ERROR_ON_RECEIVE)
py4j.protocol.Py4JNetworkError: Error while receiving
[PYTHON] 2020-04-14 08:00:19,256.256 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,256.256 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,256.256 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,257.257 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,257.257 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,257.257 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,257.257 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,257.257 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,257.257 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,258.258 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,258.258 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,258.258 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,258.258 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,258.258 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,258.258 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,258.258 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,259.259 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,259.259 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,259.259 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,259.259 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,260.260 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,260.260 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,260.260 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,261.261 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,261.261 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,261.261 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,261.261 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,261.261 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,261.261 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,348.348 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
[PYTHON] 2020-04-14 08:00:19,354.354 ERROR java_gateway - start: An error occurred while trying to connect to the Java server (127.0.0.1:41565)
Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 929, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1067, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
End of LogType:pyspark.log

LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:172447
Log Contents:
20/04/14 07:59:27 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:59:27 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:59:27 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:59:27 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:27 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:27 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:27 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:27 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:28 INFO yarn.ApplicationMaster: Preparing Local resources
20/04/14 07:59:28 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1586849858644_0006_000002
20/04/14 07:59:28 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread
20/04/14 07:59:28 INFO yarn.ApplicationMaster: Waiting for spark context initialization...
20/04/14 07:59:29 INFO spark.SparkContext: Running Spark version 2.4.5
20/04/14 07:59:29 INFO spark.SparkContext: Submitted application: ClusteringExperiment
20/04/14 07:59:29 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:29 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:29 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:29 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:29 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:29 INFO util.Utils: Successfully started service 'sparkDriver' on port 44353.
20/04/14 07:59:29 INFO spark.SparkEnv: Registering MapOutputTracker
20/04/14 07:59:29 INFO spark.SparkEnv: Registering BlockManagerMaster
20/04/14 07:59:29 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/04/14 07:59:29 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/04/14 07:59:29 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-22a07eef-d0e4-46b6-a583-9cbbc6f06bed
20/04/14 07:59:29 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 07:59:29 INFO spark.SparkEnv: Registering OutputCommitCoordinator
20/04/14 07:59:29 INFO util.log: Logging initialized @2757ms
20/04/14 07:59:29 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
20/04/14 07:59:29 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
20/04/14 07:59:29 INFO server.Server: Started @2824ms
20/04/14 07:59:29 INFO server.AbstractConnector: Started ServerConnector@24b8f8e9{HTTP/1.1,[http/1.1]}{0.0.0.0:36681}
20/04/14 07:59:29 INFO util.Utils: Successfully started service 'SparkUI' on port 36681.
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b2af124{/jobs,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@480feeef{/jobs/json,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4324536b{/jobs/job,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6dbcd2b5{/jobs/job/json,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a73317b{/stages,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3bad9e55{/stages/json,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@246d8ddd{/stages/stage,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3bfcaa19{/stages/stage/json,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@73b7cbde{/stages/pool,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@227dfc15{/stages/pool/json,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68b415d0{/storage,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36c6c6bd{/storage/json,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7be21280{/storage/rdd,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3632c4bd{/storage/rdd/json,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e605d3e{/environment,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e939f61{/environment/json,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@150ee421{/executors,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3539dd11{/executors/json,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@494cff25{/executors/threadDump,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b4619fa{/executors/threadDump/json,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@490bcf5f{/static,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@567fc5c{/,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35dbfd4a{/api,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62408a62{/jobs/job/kill,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@66d43c12{/stages/stage/kill,null,AVAILABLE,@Spark}
20/04/14 07:59:29 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://178.62.199.118:36681
20/04/14 07:59:30 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler
20/04/14 07:59:30 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1586849858644_0006 and attemptId Some(appattempt_1586849858644_0006_000002)
20/04/14 07:59:30 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45281.
20/04/14 07:59:30 INFO netty.NettyBlockTransferService: Server created on 178.62.199.118:45281
20/04/14 07:59:30 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 07:59:30 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 178.62.199.118, 45281, None)
20/04/14 07:59:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.199.118:45281 with 3.0 GB RAM, BlockManagerId(driver, 178.62.199.118, 45281, None)
20/04/14 07:59:30 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 178.62.199.118, 45281, None)
20/04/14 07:59:30 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 178.62.199.118, 45281, None)
20/04/14 07:59:30 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
20/04/14 07:59:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@31a76e89{/metrics/json,null,AVAILABLE,@Spark}
20/04/14 07:59:30 INFO client.RMProxy: Connecting to ResourceManager at /178.62.197.79:8030
20/04/14 07:59:30 INFO yarn.YarnRMClient: Registering the ApplicationMaster
20/04/14 07:59:30 INFO yarn.ApplicationMaster: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/src/hadoop-2.8.5/etc/hadoop:/usr/src/hadoop-2.8.5/share/hadoop/common/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/common/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/*:/usr/src/hadoop-2.8.5/contrib/capacity-scheduler/*.jar<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__
    SPARK_DIST_CLASSPATH -> /usr/src/hadoop-2.8.5/etc/hadoop:/usr/src/hadoop-2.8.5/share/hadoop/common/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/common/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/*:/usr/src/hadoop-2.8.5/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> hdfs://178.62.197.79:9000/user/root/.sparkStaging/application_1586849858644_0006
    SPARK_USER -> root
    PYTHONPATH -> /usr/src/spark-2.4.5-bin-without-hadoop/python:/usr/src/spark-2.4.5-bin-without-hadoop/python/build:/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/pyspark.zip:/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip

  command:
    {{JAVA_HOME}}/bin/java \ 
      -server \ 
      -Xmx6144m \ 
      -Djava.io.tmpdir={{PWD}}/tmp \ 
      '-Dspark.ui.port=0' \ 
      '-Dspark.driver.port=44353' \ 
      -Dspark.yarn.app.container.log.dir=<LOG_DIR> \ 
      -XX:OnOutOfMemoryError='kill %p' \ 
      org.apache.spark.executor.CoarseGrainedExecutorBackend \ 
      --driver-url \ 
      spark://CoarseGrainedScheduler@178.62.199.118:44353 \ 
      --executor-id \ 
      <executorId> \ 
      --hostname \ 
      <hostname> \ 
      --cores \ 
      3 \ 
      --app-id \ 
      application_1586849858644_0006 \ 
      --user-class-path \ 
      file:$PWD/__app__.jar \ 
      1><LOG_DIR>/stdout \ 
      2><LOG_DIR>/stderr

  resources:
    pyspark.zip -> resource { scheme: "hdfs" host: "178.62.197.79" port: 9000 file: "/user/root/.sparkStaging/application_1586849858644_0006/pyspark.zip" } size: 591945 timestamp: 1586851113752 type: FILE visibility: PRIVATE
    py4j-0.10.7-src.zip -> resource { scheme: "hdfs" host: "178.62.197.79" port: 9000 file: "/user/root/.sparkStaging/application_1586849858644_0006/py4j-0.10.7-src.zip" } size: 42437 timestamp: 1586851113775 type: FILE visibility: PRIVATE
    __spark_libs__ -> resource { scheme: "hdfs" host: "178.62.197.79" port: 9000 file: "/user/root/.sparkStaging/application_1586849858644_0006/__spark_libs__4654896636915899670.zip" } size: 168822862 timestamp: 1586851113598 type: ARCHIVE visibility: PRIVATE
    __spark_conf__ -> resource { scheme: "hdfs" host: "178.62.197.79" port: 9000 file: "/user/root/.sparkStaging/application_1586849858644_0006/__spark_conf__.zip" } size: 233325 timestamp: 1586851113906 type: ARCHIVE visibility: PRIVATE

===============================================================================
20/04/14 07:59:30 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@178.62.199.118:44353)
20/04/14 07:59:30 INFO yarn.YarnAllocator: Will request 3 executor container(s), each with 3 core(s) and 6758 MB memory (including 614 MB of overhead)
20/04/14 07:59:30 INFO yarn.YarnAllocator: Submitted 3 unlocalized container requests.
20/04/14 07:59:30 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals
20/04/14 07:59:30 INFO impl.AMRMClientImpl: Received new token for : 178.62.198.251:42461
20/04/14 07:59:30 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_02_000002 on host 178.62.198.251 for executor with ID 1
20/04/14 07:59:30 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/14 07:59:30 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 07:59:31 INFO impl.AMRMClientImpl: Received new token for : 178.62.199.118:35057
20/04/14 07:59:31 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_02_000003 on host 178.62.199.118 for executor with ID 2
20/04/14 07:59:31 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/14 07:59:31 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 07:59:31 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_02_000004 on host 178.62.198.251 for executor with ID 3
20/04/14 07:59:31 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/14 07:59:31 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 07:59:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.198.251:36236) with ID 1
20/04/14 07:59:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.199.118:53892) with ID 2
20/04/14 07:59:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.198.251:37949 with 3.0 GB RAM, BlockManagerId(1, 178.62.198.251, 37949, None)
20/04/14 07:59:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.199.118:38045 with 3.0 GB RAM, BlockManagerId(2, 178.62.199.118, 38045, None)
20/04/14 07:59:33 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.198.251:36240) with ID 3
20/04/14 07:59:33 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/04/14 07:59:33 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
20/04/14 07:59:33 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.198.251:41839 with 3.0 GB RAM, BlockManagerId(3, 178.62.198.251, 41839, None)
20/04/14 07:59:33 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/container_1586849858644_0006_02_000001/spark-warehouse').
20/04/14 07:59:33 INFO internal.SharedState: Warehouse path is 'file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/container_1586849858644_0006_02_000001/spark-warehouse'.
20/04/14 07:59:33 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.
20/04/14 07:59:33 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ef9a2e9{/SQL,null,AVAILABLE,@Spark}
20/04/14 07:59:33 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.
20/04/14 07:59:33 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b111296{/SQL/json,null,AVAILABLE,@Spark}
20/04/14 07:59:33 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.
20/04/14 07:59:33 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b24c330{/SQL/execution,null,AVAILABLE,@Spark}
20/04/14 07:59:33 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.
20/04/14 07:59:33 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10697292{/SQL/execution/json,null,AVAILABLE,@Spark}
20/04/14 07:59:33 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.
20/04/14 07:59:33 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c82f659{/static/sql,null,AVAILABLE,@Spark}
20/04/14 07:59:34 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/04/14 07:59:34 INFO datasources.InMemoryFileIndex: It took 86 ms to list leaf files for 1 paths.
20/04/14 07:59:34 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
20/04/14 07:59:34 INFO scheduler.DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/04/14 07:59:34 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
20/04/14 07:59:34 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/14 07:59:34 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/14 07:59:34 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/14 07:59:34 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 127.0 KB, free 3.0 GB)
20/04/14 07:59:34 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.2 KB, free 3.0 GB)
20/04/14 07:59:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 178.62.199.118:45281 (size: 33.2 KB, free: 3.0 GB)
20/04/14 07:59:34 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1163
20/04/14 07:59:34 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/04/14 07:59:34 INFO cluster.YarnClusterScheduler: Adding task set 0.0 with 1 tasks
20/04/14 07:59:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 178.62.199.118, executor 2, partition 0, PROCESS_LOCAL, 8098 bytes)
20/04/14 07:59:35 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 178.62.199.118:38045 (size: 33.2 KB, free: 3.0 GB)
20/04/14 07:59:36 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1333 ms on 178.62.199.118 (executor 2) (1/1)
20/04/14 07:59:36 INFO cluster.YarnClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/04/14 07:59:36 INFO scheduler.DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.456 s
20/04/14 07:59:36 INFO scheduler.DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.494458 s
20/04/14 07:59:37 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/14 07:59:37 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/14 07:59:37 INFO datasources.FileSourceStrategy: Output Data Schema: struct<entry: string, entry_name: string, features: vector ... 1 more fields>
20/04/14 07:59:37 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/14 07:59:37 INFO spark.ContextCleaner: Cleaned accumulator 27
20/04/14 07:59:37 INFO spark.ContextCleaner: Cleaned accumulator 26
20/04/14 07:59:37 INFO codegen.CodeGenerator: Code generated in 161.880487 ms
20/04/14 07:59:37 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 341.6 KB, free 3.0 GB)
20/04/14 07:59:37 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 07:59:37 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.199.118:45281 (size: 32.5 KB, free: 3.0 GB)
20/04/14 07:59:37 INFO spark.SparkContext: Created broadcast 1 from rdd at GaussianMixture.scala:348
20/04/14 07:59:37 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 8546356 bytes, open cost is considered as scanning 4194304 bytes.
20/04/14 07:59:37 INFO spark.SparkContext: Starting job: first at GaussianMixture.scala:357
20/04/14 07:59:37 INFO scheduler.DAGScheduler: Registering RDD 5 (rdd at GaussianMixture.scala:348) as input to shuffle 0
20/04/14 07:59:37 INFO scheduler.DAGScheduler: Got job 1 (first at GaussianMixture.scala:357) with 1 output partitions
20/04/14 07:59:37 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (first at GaussianMixture.scala:357)
20/04/14 07:59:37 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
20/04/14 07:59:37 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 1)
20/04/14 07:59:37 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at rdd at GaussianMixture.scala:348), which has no missing parents
20/04/14 07:59:37 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.1 KB, free 3.0 GB)
20/04/14 07:59:37 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.5 KB, free 3.0 GB)
20/04/14 07:59:37 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.199.118:45281 (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:59:37 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1163
20/04/14 07:59:37 INFO scheduler.DAGScheduler: Submitting 9 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at rdd at GaussianMixture.scala:348) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8))
20/04/14 07:59:37 INFO cluster.YarnClusterScheduler: Adding task set 1.0 with 9 tasks
20/04/14 07:59:37 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 178.62.198.251, executor 1, partition 0, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:37 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, 178.62.199.118, executor 2, partition 1, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:37 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, 178.62.198.251, executor 3, partition 2, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:37 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, 178.62.198.251, executor 1, partition 3, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:37 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, 178.62.199.118, executor 2, partition 4, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:37 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, 178.62.198.251, executor 3, partition 5, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:37 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7, 178.62.198.251, executor 1, partition 6, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:37 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8, 178.62.199.118, executor 2, partition 7, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:37 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9, 178.62.198.251, executor 3, partition 8, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:37 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.199.118:38045 (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:59:38 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.198.251:37949 (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:59:38 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.198.251:41839 (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:59:38 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.199.118:38045 (size: 32.5 KB, free: 3.0 GB)
20/04/14 07:59:38 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.198.251:37949 (size: 32.5 KB, free: 3.0 GB)
20/04/14 07:59:38 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.198.251:41839 (size: 32.5 KB, free: 3.0 GB)
20/04/14 07:59:39 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1808 ms on 178.62.199.118 (executor 2) (1/9)
20/04/14 07:59:39 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 1851 ms on 178.62.199.118 (executor 2) (2/9)
20/04/14 07:59:39 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 1879 ms on 178.62.199.118 (executor 2) (3/9)
20/04/14 07:59:41 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 3227 ms on 178.62.198.251 (executor 3) (4/9)
20/04/14 07:59:41 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 3816 ms on 178.62.198.251 (executor 1) (5/9)
20/04/14 07:59:41 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 3817 ms on 178.62.198.251 (executor 1) (6/9)
20/04/14 07:59:41 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3821 ms on 178.62.198.251 (executor 1) (7/9)
20/04/14 07:59:41 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 4016 ms on 178.62.198.251 (executor 3) (8/9)
20/04/14 07:59:41 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 4022 ms on 178.62.198.251 (executor 3) (9/9)
20/04/14 07:59:41 INFO cluster.YarnClusterScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/04/14 07:59:41 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (rdd at GaussianMixture.scala:348) finished in 4.044 s
20/04/14 07:59:41 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/04/14 07:59:41 INFO scheduler.DAGScheduler: running: Set()
20/04/14 07:59:41 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 2)
20/04/14 07:59:41 INFO scheduler.DAGScheduler: failed: Set()
20/04/14 07:59:41 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at map at GaussianMixture.scala:348), which has no missing parents
20/04/14 07:59:41 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.1 KB, free 3.0 GB)
20/04/14 07:59:41 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.6 KB, free 3.0 GB)
20/04/14 07:59:41 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 178.62.199.118:45281 (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:59:41 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163
20/04/14 07:59:41 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at map at GaussianMixture.scala:348) (first 15 tasks are for partitions Vector(0))
20/04/14 07:59:41 INFO cluster.YarnClusterScheduler: Adding task set 2.0 with 1 tasks
20/04/14 07:59:41 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 10, 178.62.198.251, executor 1, partition 0, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:41 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 178.62.198.251:37949 (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:59:42 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.198.251:36236
20/04/14 07:59:42 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on 178.62.198.251:37949 (size: 15.0 MB, free: 3.0 GB)
20/04/14 07:59:42 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 10) in 992 ms on 178.62.198.251 (executor 1) (1/1)
20/04/14 07:59:42 INFO cluster.YarnClusterScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/04/14 07:59:42 INFO scheduler.DAGScheduler: ResultStage 2 (first at GaussianMixture.scala:357) finished in 0.998 s
20/04/14 07:59:42 INFO scheduler.DAGScheduler: Job 1 finished: first at GaussianMixture.scala:357, took 5.065080 s
20/04/14 07:59:42 INFO util.Instrumentation: [f33e0edc] Stage class: GaussianMixture
20/04/14 07:59:42 INFO util.Instrumentation: [f33e0edc] Stage uid: GaussianMixture_7de8b76a6037
20/04/14 07:59:42 INFO util.Instrumentation: [f33e0edc] training: numPartitions=18 storageLevel=StorageLevel(1 replicas)
20/04/14 07:59:42 INFO util.Instrumentation: [f33e0edc] {"featuresCol":"features","predictionCol":"cluster","k":2,"seed":42}
20/04/14 07:59:42 INFO util.Instrumentation: [f33e0edc] {"numFeatures":8502}
20/04/14 07:59:42 INFO spark.SparkContext: Starting job: takeSample at GaussianMixture.scala:470
20/04/14 07:59:42 INFO scheduler.DAGScheduler: Got job 2 (takeSample at GaussianMixture.scala:470) with 18 output partitions
20/04/14 07:59:42 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (takeSample at GaussianMixture.scala:470)
20/04/14 07:59:42 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
20/04/14 07:59:42 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/14 07:59:42 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at map at GaussianMixture.scala:348), which has no missing parents
20/04/14 07:59:42 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 07:59:42 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.6 KB, free 3.0 GB)
20/04/14 07:59:42 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.199.118:45281 (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:59:42 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1163
20/04/14 07:59:42 INFO scheduler.DAGScheduler: Submitting 18 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at map at GaussianMixture.scala:348) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/04/14 07:59:42 INFO cluster.YarnClusterScheduler: Adding task set 4.0 with 18 tasks
20/04/14 07:59:42 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 11, 178.62.198.251, executor 1, partition 0, PROCESS_LOCAL, 7756 bytes)
20/04/14 07:59:42 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 12, 178.62.198.251, executor 1, partition 1, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:42 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 13, 178.62.198.251, executor 3, partition 2, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:42 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 4.0 (TID 14, 178.62.199.118, executor 2, partition 3, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:42 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 4.0 (TID 15, 178.62.198.251, executor 1, partition 4, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:42 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 4.0 (TID 16, 178.62.198.251, executor 3, partition 5, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:42 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 4.0 (TID 17, 178.62.199.118, executor 2, partition 6, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:42 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 4.0 (TID 18, 178.62.198.251, executor 3, partition 7, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:42 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 4.0 (TID 19, 178.62.199.118, executor 2, partition 8, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:42 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.198.251:37949 (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:59:42 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.199.118:38045 (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.198.251:41839 (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:59:43 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.199.118:53892
20/04/14 07:59:43 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.198.251:36240
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_1 in memory on 178.62.198.251:37949 (size: 14.9 MB, free: 3.0 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_4 in memory on 178.62.198.251:37949 (size: 14.9 MB, free: 3.0 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_3 in memory on 178.62.199.118:38045 (size: 14.8 MB, free: 3.0 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_6 in memory on 178.62.199.118:38045 (size: 15.1 MB, free: 3.0 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_8 in memory on 178.62.199.118:38045 (size: 14.7 MB, free: 3.0 GB)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 4.0 (TID 20, 178.62.198.251, executor 1, partition 9, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 4.0 (TID 15) in 371 ms on 178.62.198.251 (executor 1) (1/18)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 4.0 (TID 21, 178.62.198.251, executor 1, partition 10, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 11) in 374 ms on 178.62.198.251 (executor 1) (2/18)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 4.0 (TID 22, 178.62.198.251, executor 1, partition 11, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 12) in 433 ms on 178.62.198.251 (executor 1) (3/18)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_7 in memory on 178.62.198.251:41839 (size: 14.6 MB, free: 3.0 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_5 in memory on 178.62.198.251:41839 (size: 14.7 MB, free: 3.0 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_2 in memory on 178.62.198.251:41839 (size: 14.5 MB, free: 3.0 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_11 in memory on 178.62.198.251:37949 (size: 14.4 MB, free: 3.0 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_10 in memory on 178.62.198.251:37949 (size: 14.7 MB, free: 3.0 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_9 in memory on 178.62.198.251:37949 (size: 14.6 MB, free: 2.9 GB)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 4.0 (TID 23, 178.62.198.251, executor 1, partition 12, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 4.0 (TID 22) in 201 ms on 178.62.198.251 (executor 1) (4/18)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 4.0 (TID 24, 178.62.198.251, executor 1, partition 13, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 4.0 (TID 20) in 266 ms on 178.62.198.251 (executor 1) (5/18)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 4.0 (TID 25, 178.62.198.251, executor 1, partition 14, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 4.0 (TID 21) in 334 ms on 178.62.198.251 (executor 1) (6/18)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_13 in memory on 178.62.198.251:37949 (size: 14.1 MB, free: 2.9 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_12 in memory on 178.62.198.251:37949 (size: 14.8 MB, free: 2.9 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_14 in memory on 178.62.198.251:37949 (size: 15.0 MB, free: 2.9 GB)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 4.0 (TID 26, 178.62.198.251, executor 1, partition 15, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 4.0 (TID 24) in 148 ms on 178.62.198.251 (executor 1) (7/18)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 4.0 (TID 27, 178.62.198.251, executor 1, partition 16, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 4.0 (TID 23) in 160 ms on 178.62.198.251 (executor 1) (8/18)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 4.0 (TID 28, 178.62.198.251, executor 1, partition 17, NODE_LOCAL, 7756 bytes)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 4.0 (TID 25) in 108 ms on 178.62.198.251 (executor 1) (9/18)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_15 in memory on 178.62.198.251:37949 (size: 15.1 MB, free: 2.9 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_16 in memory on 178.62.198.251:37949 (size: 15.2 MB, free: 2.9 GB)
20/04/14 07:59:43 INFO storage.BlockManagerInfo: Added rdd_7_17 in memory on 178.62.198.251:37949 (size: 14.7 MB, free: 2.9 GB)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 4.0 (TID 26) in 166 ms on 178.62.198.251 (executor 1) (10/18)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 4.0 (TID 27) in 167 ms on 178.62.198.251 (executor 1) (11/18)
20/04/14 07:59:43 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 4.0 (TID 28) in 145 ms on 178.62.198.251 (executor 1) (12/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 4.0 (TID 17) in 1235 ms on 178.62.199.118 (executor 2) (13/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 4.0 (TID 19) in 1240 ms on 178.62.199.118 (executor 2) (14/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 4.0 (TID 14) in 1244 ms on 178.62.199.118 (executor 2) (15/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 13) in 1589 ms on 178.62.198.251 (executor 3) (16/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 4.0 (TID 18) in 1588 ms on 178.62.198.251 (executor 3) (17/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 4.0 (TID 16) in 1593 ms on 178.62.198.251 (executor 3) (18/18)
20/04/14 07:59:44 INFO cluster.YarnClusterScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/04/14 07:59:44 INFO scheduler.DAGScheduler: ResultStage 4 (takeSample at GaussianMixture.scala:470) finished in 1.602 s
20/04/14 07:59:44 INFO scheduler.DAGScheduler: Job 2 finished: takeSample at GaussianMixture.scala:470, took 1.608054 s
20/04/14 07:59:44 INFO spark.SparkContext: Starting job: takeSample at GaussianMixture.scala:470
20/04/14 07:59:44 INFO scheduler.DAGScheduler: Got job 3 (takeSample at GaussianMixture.scala:470) with 18 output partitions
20/04/14 07:59:44 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (takeSample at GaussianMixture.scala:470)
20/04/14 07:59:44 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
20/04/14 07:59:44 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/14 07:59:44 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (PartitionwiseSampledRDD[21] at takeSample at GaussianMixture.scala:470), which has no missing parents
20/04/14 07:59:44 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.1 KB, free 3.0 GB)
20/04/14 07:59:44 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.7 KB, free 3.0 GB)
20/04/14 07:59:44 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.199.118:45281 (size: 13.7 KB, free: 3.0 GB)
20/04/14 07:59:44 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1163
20/04/14 07:59:44 INFO scheduler.DAGScheduler: Submitting 18 missing tasks from ResultStage 6 (PartitionwiseSampledRDD[21] at takeSample at GaussianMixture.scala:470) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/04/14 07:59:44 INFO cluster.YarnClusterScheduler: Adding task set 6.0 with 18 tasks
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 29, 178.62.199.118, executor 2, partition 3, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 30, 178.62.198.251, executor 3, partition 2, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 31, 178.62.198.251, executor 1, partition 0, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 32, 178.62.199.118, executor 2, partition 6, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 33, 178.62.198.251, executor 3, partition 5, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 34, 178.62.198.251, executor 1, partition 1, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 35, 178.62.199.118, executor 2, partition 8, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 36, 178.62.198.251, executor 3, partition 7, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 37, 178.62.198.251, executor 1, partition 4, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.199.118:38045 (size: 13.7 KB, free: 3.0 GB)
20/04/14 07:59:44 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.198.251:37949 (size: 13.7 KB, free: 2.9 GB)
20/04/14 07:59:44 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.198.251:41839 (size: 13.7 KB, free: 3.0 GB)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 6.0 (TID 35) in 86 ms on 178.62.199.118 (executor 2) (1/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 6.0 (TID 29) in 90 ms on 178.62.199.118 (executor 2) (2/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 6.0 (TID 32) in 88 ms on 178.62.199.118 (executor 2) (3/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 6.0 (TID 38, 178.62.198.251, executor 1, partition 9, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 6.0 (TID 37) in 158 ms on 178.62.198.251 (executor 1) (4/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 6.0 (TID 39, 178.62.198.251, executor 1, partition 10, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 31) in 160 ms on 178.62.198.251 (executor 1) (5/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 6.0 (TID 33) in 161 ms on 178.62.198.251 (executor 3) (6/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 6.0 (TID 40, 178.62.198.251, executor 1, partition 11, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 34) in 164 ms on 178.62.198.251 (executor 1) (7/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 6.0 (TID 36) in 203 ms on 178.62.198.251 (executor 3) (8/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 30) in 220 ms on 178.62.198.251 (executor 3) (9/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 6.0 (TID 41, 178.62.198.251, executor 1, partition 12, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 6.0 (TID 40) in 69 ms on 178.62.198.251 (executor 1) (10/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 6.0 (TID 42, 178.62.198.251, executor 1, partition 13, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 6.0 (TID 39) in 74 ms on 178.62.198.251 (executor 1) (11/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 6.0 (TID 43, 178.62.198.251, executor 1, partition 14, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 6.0 (TID 38) in 78 ms on 178.62.198.251 (executor 1) (12/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 6.0 (TID 44, 178.62.198.251, executor 1, partition 15, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 6.0 (TID 42) in 173 ms on 178.62.198.251 (executor 1) (13/18)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 6.0 (TID 45, 178.62.198.251, executor 1, partition 16, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:44 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 6.0 (TID 41) in 178 ms on 178.62.198.251 (executor 1) (14/18)
20/04/14 07:59:45 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 6.0 (TID 46, 178.62.198.251, executor 1, partition 17, PROCESS_LOCAL, 7865 bytes)
20/04/14 07:59:45 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 6.0 (TID 43) in 184 ms on 178.62.198.251 (executor 1) (15/18)
20/04/14 07:59:45 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 6.0 (TID 44) in 23 ms on 178.62.198.251 (executor 1) (16/18)
20/04/14 07:59:45 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 6.0 (TID 45) in 27 ms on 178.62.198.251 (executor 1) (17/18)
20/04/14 07:59:45 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 6.0 (TID 46) in 28 ms on 178.62.198.251 (executor 1) (18/18)
20/04/14 07:59:45 INFO cluster.YarnClusterScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool 
20/04/14 07:59:45 INFO scheduler.DAGScheduler: ResultStage 6 (takeSample at GaussianMixture.scala:470) finished in 0.453 s
20/04/14 07:59:45 INFO scheduler.DAGScheduler: Job 3 finished: takeSample at GaussianMixture.scala:470, took 0.457948 s
20/04/14 07:59:45 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
20/04/14 07:59:45 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 38
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 59
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 72
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 89
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 83
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 106
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 130
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 126
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 65
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 138
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 123
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 42
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 82
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 90
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 45
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 55
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 53
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 61
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 51
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 78
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 95
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 108
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 48
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 44
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 115
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.199.118:45281 in memory (size: 13.7 KB, free: 3.0 GB)
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.199.118:38045 in memory (size: 13.7 KB, free: 3.0 GB)
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.198.251:41839 in memory (size: 13.7 KB, free: 3.0 GB)
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.198.251:37949 in memory (size: 13.7 KB, free: 2.9 GB)
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 66
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 99
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 56
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 85
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 118
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 40
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 60
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 86
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 52
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 63
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 98
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 121
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 68
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 94
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 46
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 74
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 134
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 93
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 91
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 107
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 80
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 110
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 69
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 111
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 124
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 137
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.199.118:45281 in memory (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.198.251:41839 in memory (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.198.251:37949 in memory (size: 6.5 KB, free: 2.9 GB)
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.199.118:38045 in memory (size: 6.5 KB, free: 3.0 GB)
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 96
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 101
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 50
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 119
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 37
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 135
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 178.62.199.118:45281 in memory (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 178.62.198.251:37949 in memory (size: 7.6 KB, free: 2.9 GB)
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 120
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 133
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 128
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 109
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 73
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 132
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 79
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 47
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 57
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 131
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 71
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 102
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 49
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 136
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 58
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 117
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 39
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 64
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 125
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 62
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 76
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 92
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 112
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 43
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 103
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 75
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 122
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.199.118:45281 in memory (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.198.251:37949 in memory (size: 7.6 KB, free: 2.9 GB)
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.199.118:38045 in memory (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:59:45 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.198.251:41839 in memory (size: 7.6 KB, free: 3.0 GB)
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 114
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 104
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 81
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 70
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 116
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 41
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 77
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 97
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 105
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 127
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 67
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 84
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 129
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 113
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 54
20/04/14 07:59:45 INFO spark.ContextCleaner: Cleaned accumulator 100
20/04/14 07:59:46 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 3.0 GB)
20/04/14 07:59:46 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 3.0 GB)
20/04/14 07:59:46 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.199.118:45281 (size: 85.0 B, free: 3.0 GB)
20/04/14 07:59:46 INFO spark.SparkContext: Created broadcast 6 from broadcast at GaussianMixture.scala:379
20/04/14 07:59:46 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.5 GB)
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 2.5 GB)
20/04/14 07:59:47 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.199.118:45281 (size: 2.7 MB, free: 3.0 GB)
20/04/14 07:59:47 INFO spark.SparkContext: Created broadcast 7 from broadcast at GaussianMixture.scala:380
20/04/14 07:59:47 INFO spark.SparkContext: Starting job: treeAggregate at GaussianMixture.scala:384
20/04/14 07:59:47 INFO scheduler.DAGScheduler: Registering RDD 23 (treeAggregate at GaussianMixture.scala:384) as input to shuffle 1
20/04/14 07:59:47 INFO scheduler.DAGScheduler: Got job 4 (treeAggregate at GaussianMixture.scala:384) with 3 output partitions
20/04/14 07:59:47 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (treeAggregate at GaussianMixture.scala:384)
20/04/14 07:59:47 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
20/04/14 07:59:47 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 8)
20/04/14 07:59:47 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[23] at treeAggregate at GaussianMixture.scala:384), which has no missing parents
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 19.7 KB, free 2.5 GB)
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 2.5 GB)
20/04/14 07:59:47 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.199.118:45281 (size: 9.2 KB, free: 3.0 GB)
20/04/14 07:59:47 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1163
20/04/14 07:59:47 INFO scheduler.DAGScheduler: Submitting 18 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[23] at treeAggregate at GaussianMixture.scala:384) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/04/14 07:59:47 INFO cluster.YarnClusterScheduler: Adding task set 8.0 with 18 tasks
20/04/14 07:59:47 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 8.0 (TID 47, 178.62.198.251, executor 3, partition 2, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:59:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 48, 178.62.198.251, executor 1, partition 0, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:59:47 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 8.0 (TID 49, 178.62.199.118, executor 2, partition 3, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:59:47 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 8.0 (TID 50, 178.62.198.251, executor 3, partition 5, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:59:47 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 51, 178.62.198.251, executor 1, partition 1, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:59:47 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 8.0 (TID 52, 178.62.199.118, executor 2, partition 6, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:59:47 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 8.0 (TID 53, 178.62.198.251, executor 3, partition 7, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:59:47 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 8.0 (TID 54, 178.62.198.251, executor 1, partition 4, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:59:47 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 8.0 (TID 55, 178.62.199.118, executor 2, partition 8, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:59:47 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.199.118:38045 (size: 9.2 KB, free: 3.0 GB)
20/04/14 07:59:47 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.198.251:41839 (size: 9.2 KB, free: 3.0 GB)
20/04/14 07:59:47 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.198.251:37949 (size: 9.2 KB, free: 2.9 GB)
20/04/14 07:59:47 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.199.118:38045 (size: 85.0 B, free: 3.0 GB)
20/04/14 07:59:47 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.198.251:37949 (size: 85.0 B, free: 2.9 GB)
20/04/14 07:59:47 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.199.118:38045 (size: 2.7 MB, free: 3.0 GB)
20/04/14 07:59:47 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.198.251:41839 (size: 85.0 B, free: 3.0 GB)
20/04/14 07:59:47 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.198.251:37949 (size: 2.7 MB, free: 2.8 GB)
20/04/14 07:59:47 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.198.251:41839 (size: 2.7 MB, free: 3.0 GB)
20/04/14 07:59:52 INFO spark.ContextCleaner: Cleaned accumulator 18
20/04/14 07:59:52 INFO spark.ContextCleaner: Cleaned accumulator 23
20/04/14 07:59:52 INFO spark.ContextCleaner: Cleaned accumulator 8
20/04/14 07:59:52 INFO spark.ContextCleaner: Cleaned accumulator 15
20/04/14 07:59:52 INFO spark.ContextCleaner: Cleaned accumulator 5
20/04/14 07:59:52 INFO spark.ContextCleaner: Cleaned accumulator 21
20/04/14 07:59:52 INFO spark.ContextCleaner: Cleaned accumulator 7
20/04/14 07:59:52 INFO spark.ContextCleaner: Cleaned accumulator 17
20/04/14 07:59:52 INFO spark.ContextCleaner: Cleaned accumulator 11
20/04/14 07:59:52 INFO spark.ContextCleaner: Cleaned accumulator 9
20/04/14 07:59:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 2.
20/04/14 07:59:52 ERROR client.TransportClient: Failed to send RPC RPC 6489725225721263265 to /178.62.199.118:53892: java.nio.channels.ClosedChannelException
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:958)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:866)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:716)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:708)
	at io.netty.channel.AbstractChannelHandlerContext.access$1700(AbstractChannelHandlerContext.java:56)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1102)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1149)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1073)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:510)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:518)
	at io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1044)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
20/04/14 07:59:52 WARN storage.BlockManagerMasterEndpoint: Error trying to remove broadcast 0 from block manager BlockManagerId(2, 178.62.199.118, 38045, None)
java.io.IOException: Failed to send RPC RPC 6489725225721263265 to /178.62.199.118:53892: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:362)
	at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:339)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)
	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)
	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)
	at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)
	at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:994)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:866)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:716)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:708)
	at io.netty.channel.AbstractChannelHandlerContext.access$1700(AbstractChannelHandlerContext.java:56)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write(AbstractChannelHandlerContext.java:1102)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:1149)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:1073)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:510)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:518)
	at io.netty.util.concurrent.SingleThreadEventExecutor$6.run(SingleThreadEventExecutor.java:1044)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:958)
	... 15 more
20/04/14 07:59:52 INFO scheduler.DAGScheduler: Executor lost: 2 (epoch 1)
20/04/14 07:59:52 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
20/04/14 07:59:52 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_8 !
20/04/14 07:59:52 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_3 !
20/04/14 07:59:52 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_6 !
20/04/14 07:59:52 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(2, 178.62.199.118, 38045, None)
20/04/14 07:59:52 INFO storage.BlockManagerMaster: Removed 2 successfully in removeExecutor
20/04/14 07:59:52 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 2 (epoch 1)
20/04/14 07:59:52 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 178.62.199.118:45281 in memory (size: 33.2 KB, free: 3.0 GB)
20/04/14 07:59:52 INFO yarn.YarnAllocator: Completed container container_1586849858644_0006_02_000003 on host: 178.62.199.118 (state: COMPLETE, exit status: 143)
20/04/14 07:59:52 WARN yarn.YarnAllocator: Container from a bad node: container_1586849858644_0006_02_000003 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:52 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1586849858644_0006_02_000003 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:52 ERROR cluster.YarnClusterScheduler: Lost executor 2 on 178.62.199.118: Container from a bad node: container_1586849858644_0006_02_000003 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:52 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 8.0 (TID 55, 178.62.199.118, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000003 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:52 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 8.0 (TID 49, 178.62.199.118, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000003 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:52 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 8.0 (TID 52, 178.62.199.118, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000003 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:52 INFO storage.BlockManagerMaster: Removal of executor 2 requested
20/04/14 07:59:52 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
20/04/14 07:59:52 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 2
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 2
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 3
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 6
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 25
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 16
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 24
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 13
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 19
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 1
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 10
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 4
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 22
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 12
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 14
20/04/14 07:59:53 INFO spark.ContextCleaner: Cleaned accumulator 20
20/04/14 07:59:53 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 8.0 (TID 56, 178.62.198.251, executor 1, partition 9, PROCESS_LOCAL, 7745 bytes)
20/04/14 07:59:53 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 8.0 (TID 54, 178.62.198.251, executor 1): java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)

20/04/14 07:59:54 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 1.
20/04/14 07:59:54 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 2)
20/04/14 07:59:54 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 3 core(s) and 6758 MB memory (including 614 MB of overhead)
20/04/14 07:59:54 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
20/04/14 07:59:54 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.
20/04/14 07:59:54 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_13 !
20/04/14 07:59:54 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_10 !
20/04/14 07:59:54 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_16 !
20/04/14 07:59:54 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_14 !
20/04/14 07:59:54 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_4 !
20/04/14 07:59:54 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_0 !
20/04/14 07:59:54 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_15 !
20/04/14 07:59:54 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_17 !
20/04/14 07:59:54 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_12 !
20/04/14 07:59:54 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_9 !
20/04/14 07:59:54 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_1 !
20/04/14 07:59:54 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_11 !
20/04/14 07:59:54 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, 178.62.198.251, 37949, None)
20/04/14 07:59:54 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
20/04/14 07:59:54 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 1 (epoch 2)
20/04/14 07:59:54 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_02_000005 on host 178.62.198.251 for executor with ID 4
20/04/14 07:59:54 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/14 07:59:54 INFO yarn.YarnAllocator: Completed container container_1586849858644_0006_02_000002 on host: 178.62.198.251 (state: COMPLETE, exit status: 143)
20/04/14 07:59:54 WARN yarn.YarnAllocator: Container from a bad node: container_1586849858644_0006_02_000002 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:54 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 07:59:54 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container from a bad node: container_1586849858644_0006_02_000002 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:54 ERROR cluster.YarnClusterScheduler: Lost executor 1 on 178.62.198.251: Container from a bad node: container_1586849858644_0006_02_000002 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:54 WARN scheduler.TaskSetManager: Lost task 9.0 in stage 8.0 (TID 56, 178.62.198.251, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000002 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:54 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 8.0 (TID 48, 178.62.198.251, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000002 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:54 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 8.0 (TID 51, 178.62.198.251, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000002 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:54 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
20/04/14 07:59:54 INFO storage.BlockManagerMaster: Removal of executor 1 requested
20/04/14 07:59:54 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 1
20/04/14 07:59:56 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 3.
20/04/14 07:59:56 INFO scheduler.DAGScheduler: Executor lost: 3 (epoch 3)
20/04/14 07:59:56 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.
20/04/14 07:59:56 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_5 !
20/04/14 07:59:56 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_2 !
20/04/14 07:59:56 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 3 core(s) and 6758 MB memory (including 614 MB of overhead)
20/04/14 07:59:56 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_7 !
20/04/14 07:59:56 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(3, 178.62.198.251, 41839, None)
20/04/14 07:59:56 INFO storage.BlockManagerMaster: Removed 3 successfully in removeExecutor
20/04/14 07:59:56 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.
20/04/14 07:59:56 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 3 (epoch 3)
20/04/14 07:59:56 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.198.251:36272) with ID 4
20/04/14 07:59:56 INFO scheduler.TaskSetManager: Starting task 1.1 in stage 8.0 (TID 57, 178.62.198.251, executor 4, partition 1, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:56 INFO scheduler.TaskSetManager: Starting task 0.1 in stage 8.0 (TID 58, 178.62.198.251, executor 4, partition 0, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:56 INFO scheduler.TaskSetManager: Starting task 9.1 in stage 8.0 (TID 59, 178.62.198.251, executor 4, partition 9, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:57 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.198.251:35331 with 3.0 GB RAM, BlockManagerId(4, 178.62.198.251, 35331, None)
20/04/14 07:59:57 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_02_000006 on host 178.62.198.251 for executor with ID 5
20/04/14 07:59:57 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/14 07:59:57 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 07:59:57 INFO yarn.YarnAllocator: Completed container container_1586849858644_0006_02_000004 on host: 178.62.198.251 (state: COMPLETE, exit status: 143)
20/04/14 07:59:57 WARN yarn.YarnAllocator: Container from a bad node: container_1586849858644_0006_02_000004 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:57 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 3 for reason Container from a bad node: container_1586849858644_0006_02_000004 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:57 ERROR cluster.YarnClusterScheduler: Lost executor 3 on 178.62.198.251: Container from a bad node: container_1586849858644_0006_02_000004 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:57 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 8.0 (TID 50, 178.62.198.251, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000004 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:57 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 8.0 (TID 53, 178.62.198.251, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000004 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:57 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 8.0 (TID 47, 178.62.198.251, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000004 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 07:59:57 INFO storage.BlockManagerMaster: Removal of executor 3 requested
20/04/14 07:59:57 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 3
20/04/14 07:59:57 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.
20/04/14 07:59:57 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.198.251:35331 (size: 9.2 KB, free: 3.0 GB)
20/04/14 07:59:57 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.198.251:36272
20/04/14 07:59:57 INFO scheduler.TaskSetManager: Starting task 2.1 in stage 8.0 (TID 60, 178.62.198.251, executor 4, partition 2, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:57 INFO scheduler.TaskSetManager: Starting task 7.1 in stage 8.0 (TID 61, 178.62.198.251, executor 4, partition 7, NODE_LOCAL, 7745 bytes)
20/04/14 07:59:57 WARN scheduler.TaskSetManager: Lost task 1.1 in stage 8.0 (TID 57, 178.62.198.251, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=1, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 07:59:57 INFO scheduler.TaskSetManager: Task 1.1 in stage 8.0 (TID 57) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 07:59:57 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 8 (treeAggregate at GaussianMixture.scala:384) as failed due to a fetch failure from ShuffleMapStage 7 (rdd at GaussianMixture.scala:348)
20/04/14 07:59:57 INFO scheduler.DAGScheduler: ShuffleMapStage 8 (treeAggregate at GaussianMixture.scala:384) failed in 10.371 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/04/14 07:59:57 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 8.0 (TID 58, 178.62.198.251, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=0, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 07:59:57 INFO scheduler.TaskSetManager: Task 0.1 in stage 8.0 (TID 58) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 07:59:57 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 7 (rdd at GaussianMixture.scala:348) and ShuffleMapStage 8 (treeAggregate at GaussianMixture.scala:384) due to fetch failure
20/04/14 07:59:57 WARN scheduler.TaskSetManager: Lost task 9.1 in stage 8.0 (TID 59, 178.62.198.251, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=9, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 07:59:57 INFO scheduler.TaskSetManager: Task 9.1 in stage 8.0 (TID 59) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 07:59:57 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.198.251:36272
20/04/14 07:59:57 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.198.251:36272
20/04/14 07:59:57 WARN scheduler.TaskSetManager: Lost task 2.1 in stage 8.0 (TID 60, 178.62.198.251, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=2, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 07:59:57 INFO scheduler.TaskSetManager: Task 2.1 in stage 8.0 (TID 60) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 07:59:57 WARN scheduler.TaskSetManager: Lost task 7.1 in stage 8.0 (TID 61, 178.62.198.251, executor 4): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=7, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 07:59:57 INFO scheduler.TaskSetManager: Task 7.1 in stage 8.0 (TID 61) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 07:59:57 INFO cluster.YarnClusterScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool 
20/04/14 07:59:57 INFO scheduler.DAGScheduler: Resubmitting failed stages
20/04/14 07:59:57 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[5] at rdd at GaussianMixture.scala:348), which has no missing parents
20/04/14 07:59:57 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 14.9 KB, free 2.5 GB)
20/04/14 07:59:57 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2.5 GB)
20/04/14 07:59:57 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.199.118:45281 (size: 6.9 KB, free: 3.0 GB)
20/04/14 07:59:57 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1163
20/04/14 07:59:57 INFO scheduler.DAGScheduler: Submitting 9 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[5] at rdd at GaussianMixture.scala:348) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8))
20/04/14 07:59:57 INFO cluster.YarnClusterScheduler: Adding task set 7.0 with 9 tasks
20/04/14 07:59:57 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 62, 178.62.198.251, executor 4, partition 0, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:57 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 63, 178.62.198.251, executor 4, partition 1, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:57 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 7.0 (TID 64, 178.62.198.251, executor 4, partition 2, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:57 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.198.251:35331 (size: 6.9 KB, free: 3.0 GB)
20/04/14 07:59:58 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.198.251:35331 (size: 32.5 KB, free: 3.0 GB)
20/04/14 07:59:59 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.198.251:36280) with ID 5
20/04/14 07:59:59 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 7.0 (TID 65, 178.62.198.251, executor 5, partition 3, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:59 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 7.0 (TID 66, 178.62.198.251, executor 5, partition 4, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:59 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 7.0 (TID 67, 178.62.198.251, executor 5, partition 5, NODE_LOCAL, 8336 bytes)
20/04/14 07:59:59 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.198.251:45589 with 3.0 GB RAM, BlockManagerId(5, 178.62.198.251, 45589, None)
20/04/14 07:59:59 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.198.251:45589 (size: 6.9 KB, free: 3.0 GB)
20/04/14 08:00:00 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 3 core(s) and 6758 MB memory (including 614 MB of overhead)
20/04/14 08:00:00 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.
20/04/14 08:00:00 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.198.251:45589 (size: 32.5 KB, free: 3.0 GB)
20/04/14 08:00:00 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_02_000007 on host 178.62.199.118 for executor with ID 6
20/04/14 08:00:00 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/14 08:00:00 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 08:00:01 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 7.0 (TID 68, 178.62.198.251, executor 4, partition 6, NODE_LOCAL, 8336 bytes)
20/04/14 08:00:01 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 62) in 3357 ms on 178.62.198.251 (executor 4) (1/9)
20/04/14 08:00:01 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 7.0 (TID 69, 178.62.198.251, executor 4, partition 7, NODE_LOCAL, 8336 bytes)
20/04/14 08:00:01 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 7.0 (TID 64) in 3358 ms on 178.62.198.251 (executor 4) (2/9)
20/04/14 08:00:01 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 7.0 (TID 70, 178.62.198.251, executor 4, partition 8, NODE_LOCAL, 8336 bytes)
20/04/14 08:00:01 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 63) in 3377 ms on 178.62.198.251 (executor 4) (3/9)
20/04/14 08:00:01 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 7.0 (TID 70) in 132 ms on 178.62.198.251 (executor 4) (4/9)
20/04/14 08:00:01 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 7.0 (TID 68) in 745 ms on 178.62.198.251 (executor 4) (5/9)
20/04/14 08:00:01 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 7.0 (TID 69) in 833 ms on 178.62.198.251 (executor 4) (6/9)
20/04/14 08:00:02 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.199.118:53918) with ID 6
20/04/14 08:00:02 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.199.118:39973 with 3.0 GB RAM, BlockManagerId(6, 178.62.199.118, 39973, None)
20/04/14 08:00:03 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 7.0 (TID 66) in 3863 ms on 178.62.198.251 (executor 5) (7/9)
20/04/14 08:00:03 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 7.0 (TID 65) in 3877 ms on 178.62.198.251 (executor 5) (8/9)
20/04/14 08:00:03 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 7.0 (TID 67) in 3898 ms on 178.62.198.251 (executor 5) (9/9)
20/04/14 08:00:03 INFO cluster.YarnClusterScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool 
20/04/14 08:00:03 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (rdd at GaussianMixture.scala:348) finished in 5.448 s
20/04/14 08:00:03 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/04/14 08:00:03 INFO scheduler.DAGScheduler: running: Set()
20/04/14 08:00:03 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 9, ShuffleMapStage 8)
20/04/14 08:00:03 INFO scheduler.DAGScheduler: failed: Set()
20/04/14 08:00:03 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[23] at treeAggregate at GaussianMixture.scala:384), which has no missing parents
20/04/14 08:00:03 INFO scheduler.OutputCommitCoordinator: Reusing state from previous attempt of stage 8.
20/04/14 08:00:03 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 19.7 KB, free 2.5 GB)
20/04/14 08:00:03 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.2 KB, free 2.5 GB)
20/04/14 08:00:03 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.199.118:45281 (size: 9.2 KB, free: 3.0 GB)
20/04/14 08:00:03 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1163
20/04/14 08:00:03 INFO scheduler.DAGScheduler: Submitting 18 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[23] at treeAggregate at GaussianMixture.scala:384) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
20/04/14 08:00:03 INFO cluster.YarnClusterScheduler: Adding task set 8.1 with 18 tasks
20/04/14 08:00:03 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.1 (TID 71, 178.62.198.251, executor 5, partition 0, NODE_LOCAL, 7745 bytes)
20/04/14 08:00:03 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.1 (TID 72, 178.62.198.251, executor 4, partition 1, NODE_LOCAL, 7745 bytes)
20/04/14 08:00:03 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 8.1 (TID 73, 178.62.198.251, executor 5, partition 2, NODE_LOCAL, 7745 bytes)
20/04/14 08:00:03 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 8.1 (TID 74, 178.62.198.251, executor 4, partition 3, NODE_LOCAL, 7745 bytes)
20/04/14 08:00:03 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 8.1 (TID 75, 178.62.198.251, executor 5, partition 4, NODE_LOCAL, 7745 bytes)
20/04/14 08:00:03 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 8.1 (TID 76, 178.62.198.251, executor 4, partition 5, NODE_LOCAL, 7745 bytes)
20/04/14 08:00:03 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.198.251:35331 (size: 9.2 KB, free: 3.0 GB)
20/04/14 08:00:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.198.251:36272
20/04/14 08:00:03 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.198.251:45589 (size: 9.2 KB, free: 3.0 GB)
20/04/14 08:00:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.198.251:36280
20/04/14 08:00:03 INFO storage.BlockManagerInfo: Added rdd_7_1 in memory on 178.62.198.251:35331 (size: 14.9 MB, free: 3.0 GB)
20/04/14 08:00:03 INFO storage.BlockManagerInfo: Added rdd_7_5 in memory on 178.62.198.251:35331 (size: 14.7 MB, free: 3.0 GB)
20/04/14 08:00:03 INFO storage.BlockManagerInfo: Added rdd_7_3 in memory on 178.62.198.251:35331 (size: 14.8 MB, free: 3.0 GB)
20/04/14 08:00:03 INFO storage.BlockManagerInfo: Added rdd_7_2 in memory on 178.62.198.251:45589 (size: 14.5 MB, free: 3.0 GB)
20/04/14 08:00:03 INFO storage.BlockManagerInfo: Added rdd_7_4 in memory on 178.62.198.251:45589 (size: 14.9 MB, free: 3.0 GB)
20/04/14 08:00:03 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on 178.62.198.251:45589 (size: 15.0 MB, free: 3.0 GB)
20/04/14 08:00:04 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.198.251:35331 (size: 85.0 B, free: 3.0 GB)
20/04/14 08:00:04 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.198.251:35331 (size: 2.7 MB, free: 3.0 GB)
20/04/14 08:00:04 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.198.251:45589 (size: 85.0 B, free: 3.0 GB)
20/04/14 08:00:04 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.198.251:45589 (size: 2.7 MB, free: 3.0 GB)
20/04/14 08:00:07 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 8.1 (TID 77, 178.62.199.118, executor 6, partition 6, RACK_LOCAL, 7745 bytes)
20/04/14 08:00:07 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 8.1 (TID 78, 178.62.199.118, executor 6, partition 7, RACK_LOCAL, 7745 bytes)
20/04/14 08:00:07 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 8.1 (TID 79, 178.62.199.118, executor 6, partition 8, RACK_LOCAL, 7745 bytes)
20/04/14 08:00:07 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.199.118:39973 (size: 9.2 KB, free: 3.0 GB)
20/04/14 08:00:07 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.199.118:53918
20/04/14 08:00:08 INFO storage.BlockManagerInfo: Added rdd_7_7 in memory on 178.62.199.118:39973 (size: 14.6 MB, free: 3.0 GB)
20/04/14 08:00:08 INFO storage.BlockManagerInfo: Added rdd_7_6 in memory on 178.62.199.118:39973 (size: 15.1 MB, free: 3.0 GB)
20/04/14 08:00:08 INFO storage.BlockManagerInfo: Added rdd_7_8 in memory on 178.62.199.118:39973 (size: 14.7 MB, free: 3.0 GB)
20/04/14 08:00:09 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.199.118:39973 (size: 85.0 B, free: 3.0 GB)
20/04/14 08:00:09 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.199.118:39973 (size: 2.7 MB, free: 3.0 GB)
20/04/14 08:00:12 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 4.
20/04/14 08:00:12 INFO scheduler.DAGScheduler: Executor lost: 4 (epoch 5)
20/04/14 08:00:12 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 4 from BlockManagerMaster.
20/04/14 08:00:12 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_5 !
20/04/14 08:00:12 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_3 !
20/04/14 08:00:12 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_1 !
20/04/14 08:00:12 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(4, 178.62.198.251, 35331, None)
20/04/14 08:00:12 INFO storage.BlockManagerMaster: Removed 4 successfully in removeExecutor
20/04/14 08:00:12 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 4 (epoch 5)
20/04/14 08:00:12 INFO yarn.YarnAllocator: Completed container container_1586849858644_0006_02_000005 on host: 178.62.198.251 (state: COMPLETE, exit status: 143)
20/04/14 08:00:12 WARN yarn.YarnAllocator: Container from a bad node: container_1586849858644_0006_02_000005 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:12 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 4 for reason Container from a bad node: container_1586849858644_0006_02_000005 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:12 ERROR cluster.YarnClusterScheduler: Lost executor 4 on 178.62.198.251: Container from a bad node: container_1586849858644_0006_02_000005 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:12 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 8.1 (TID 74, 178.62.198.251, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000005 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:12 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 8.1 (TID 76, 178.62.198.251, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000005 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:12 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 8.1 (TID 72, 178.62.198.251, executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000005 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:12 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 4 from BlockManagerMaster.
20/04/14 08:00:12 INFO storage.BlockManagerMaster: Removal of executor 4 requested
20/04/14 08:00:12 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 4
20/04/14 08:00:12 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 5.
20/04/14 08:00:12 INFO scheduler.DAGScheduler: Executor lost: 5 (epoch 6)
20/04/14 08:00:12 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 5 from BlockManagerMaster.
20/04/14 08:00:12 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_4 !
20/04/14 08:00:12 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_2 !
20/04/14 08:00:12 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_0 !
20/04/14 08:00:12 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(5, 178.62.198.251, 45589, None)
20/04/14 08:00:12 INFO storage.BlockManagerMaster: Removed 5 successfully in removeExecutor
20/04/14 08:00:12 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 5 (epoch 6)
20/04/14 08:00:12 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 3 core(s) and 6758 MB memory (including 614 MB of overhead)
20/04/14 08:00:12 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.
20/04/14 08:00:12 INFO yarn.YarnAllocator: Completed container container_1586849858644_0006_02_000006 on host: 178.62.198.251 (state: COMPLETE, exit status: 143)
20/04/14 08:00:12 WARN yarn.YarnAllocator: Container from a bad node: container_1586849858644_0006_02_000006 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:12 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 5 for reason Container from a bad node: container_1586849858644_0006_02_000006 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:12 ERROR cluster.YarnClusterScheduler: Lost executor 5 on 178.62.198.251: Container from a bad node: container_1586849858644_0006_02_000006 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:12 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 8.1 (TID 71, 178.62.198.251, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000006 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:12 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 8.1 (TID 73, 178.62.198.251, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000006 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:12 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 8.1 (TID 75, 178.62.198.251, executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000006 on host: 178.62.198.251. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:13 INFO storage.BlockManagerMaster: Removal of executor 5 requested
20/04/14 08:00:13 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 5
20/04/14 08:00:13 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 5 from BlockManagerMaster.
20/04/14 08:00:13 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 3 core(s) and 6758 MB memory (including 614 MB of overhead)
20/04/14 08:00:13 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.
20/04/14 08:00:14 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_02_000009 on host 178.62.198.251 for executor with ID 7
20/04/14 08:00:14 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/14 08:00:14 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 08:00:16 INFO yarn.YarnAllocator: Launching container container_1586849858644_0006_02_000010 on host 178.62.198.251 for executor with ID 8
20/04/14 08:00:16 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/14 08:00:16 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/14 08:00:16 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Disabling executor 6.
20/04/14 08:00:16 INFO scheduler.DAGScheduler: Executor lost: 6 (epoch 7)
20/04/14 08:00:16 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 6 from BlockManagerMaster.
20/04/14 08:00:16 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_8 !
20/04/14 08:00:16 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_7 !
20/04/14 08:00:16 WARN storage.BlockManagerMasterEndpoint: No more replicas available for rdd_7_6 !
20/04/14 08:00:16 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(6, 178.62.199.118, 39973, None)
20/04/14 08:00:16 INFO storage.BlockManagerMaster: Removed 6 successfully in removeExecutor
20/04/14 08:00:16 INFO scheduler.DAGScheduler: Shuffle files lost for executor: 6 (epoch 7)
20/04/14 08:00:16 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 0 of them.
20/04/14 08:00:16 INFO yarn.YarnAllocator: Completed container container_1586849858644_0006_02_000007 on host: 178.62.199.118 (state: COMPLETE, exit status: 143)
20/04/14 08:00:16 WARN yarn.YarnAllocator: Container from a bad node: container_1586849858644_0006_02_000007 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:16 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 6 for reason Container from a bad node: container_1586849858644_0006_02_000007 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:16 ERROR cluster.YarnClusterScheduler: Lost executor 6 on 178.62.199.118: Container from a bad node: container_1586849858644_0006_02_000007 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:16 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 8.1 (TID 77, 178.62.199.118, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000007 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:16 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 8.1 (TID 79, 178.62.199.118, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000007 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:16 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 8.1 (TID 78, 178.62.199.118, executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Container from a bad node: container_1586849858644_0006_02_000007 on host: 178.62.199.118. Exit status: 143. Diagnostics: Container killed on request. Exit code is 143
Container exited with a non-zero exit code 143
Killed by external signal
.
20/04/14 08:00:16 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 6 from BlockManagerMaster.
20/04/14 08:00:16 INFO storage.BlockManagerMaster: Removal of executor 6 requested
20/04/14 08:00:16 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 6
20/04/14 08:00:16 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.198.251:36304) with ID 7
20/04/14 08:00:16 INFO scheduler.TaskSetManager: Starting task 7.1 in stage 8.1 (TID 80, 178.62.198.251, executor 7, partition 7, NODE_LOCAL, 7745 bytes)
20/04/14 08:00:16 INFO scheduler.TaskSetManager: Starting task 8.1 in stage 8.1 (TID 81, 178.62.198.251, executor 7, partition 8, NODE_LOCAL, 7745 bytes)
20/04/14 08:00:16 INFO scheduler.TaskSetManager: Starting task 6.1 in stage 8.1 (TID 82, 178.62.198.251, executor 7, partition 6, NODE_LOCAL, 7745 bytes)
20/04/14 08:00:16 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.198.251:41743 with 3.0 GB RAM, BlockManagerId(7, 178.62.198.251, 41743, None)
20/04/14 08:00:16 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.198.251:41743 (size: 9.2 KB, free: 3.0 GB)
20/04/14 08:00:16 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.198.251:36304
20/04/14 08:00:16 INFO scheduler.TaskSetManager: Starting task 4.1 in stage 8.1 (TID 83, 178.62.198.251, executor 7, partition 4, NODE_LOCAL, 7745 bytes)
20/04/14 08:00:16 WARN scheduler.TaskSetManager: Lost task 7.1 in stage 8.1 (TID 80, 178.62.198.251, executor 7): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=7, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 08:00:16 INFO scheduler.TaskSetManager: Task 7.1 in stage 8.1 (TID 80) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 08:00:16 INFO scheduler.DAGScheduler: Marking ShuffleMapStage 8 (treeAggregate at GaussianMixture.scala:384) as failed due to a fetch failure from ShuffleMapStage 7 (rdd at GaussianMixture.scala:348)
20/04/14 08:00:16 INFO scheduler.DAGScheduler: ShuffleMapStage 8 (treeAggregate at GaussianMixture.scala:384) failed in 13.767 s due to org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

20/04/14 08:00:16 WARN scheduler.TaskSetManager: Lost task 6.1 in stage 8.1 (TID 82, 178.62.198.251, executor 7): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=6, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 08:00:16 INFO scheduler.DAGScheduler: Resubmitting ShuffleMapStage 7 (rdd at GaussianMixture.scala:348) and ShuffleMapStage 8 (treeAggregate at GaussianMixture.scala:384) due to fetch failure
20/04/14 08:00:16 INFO scheduler.TaskSetManager: Task 6.1 in stage 8.1 (TID 82) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 08:00:16 WARN scheduler.TaskSetManager: Lost task 8.1 in stage 8.1 (TID 81, 178.62.198.251, executor 7): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=8, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 08:00:16 INFO scheduler.TaskSetManager: Task 8.1 in stage 8.1 (TID 81) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 08:00:16 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.198.251:36304
20/04/14 08:00:16 WARN scheduler.TaskSetManager: Lost task 4.1 in stage 8.1 (TID 83, 178.62.198.251, executor 7): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=4, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:165)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:359)
	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:357)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.execution.SQLExecutionRDD$$anonfun$compute$1.apply(SQLExecutionRDD.scala:52)
	at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:92)
	at org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:51)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

)
20/04/14 08:00:16 INFO scheduler.TaskSetManager: Task 4.1 in stage 8.1 (TID 83) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded).
20/04/14 08:00:16 INFO cluster.YarnClusterScheduler: Removed TaskSet 8.1, whose tasks have all completed, from pool 
20/04/14 08:00:17 INFO scheduler.DAGScheduler: Resubmitting failed stages
20/04/14 08:00:17 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[5] at rdd at GaussianMixture.scala:348), which has no missing parents
20/04/14 08:00:17 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 14.9 KB, free 2.5 GB)
20/04/14 08:00:17 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.9 KB, free 2.5 GB)
20/04/14 08:00:17 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 178.62.199.118:45281 (size: 6.9 KB, free: 3.0 GB)
20/04/14 08:00:17 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1163
20/04/14 08:00:17 INFO scheduler.DAGScheduler: Submitting 9 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[5] at rdd at GaussianMixture.scala:348) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8))
20/04/14 08:00:17 INFO cluster.YarnClusterScheduler: Adding task set 7.1 with 9 tasks
20/04/14 08:00:17 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.1 (TID 84, 178.62.198.251, executor 7, partition 0, NODE_LOCAL, 8336 bytes)
20/04/14 08:00:17 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.1 (TID 85, 178.62.198.251, executor 7, partition 1, NODE_LOCAL, 8336 bytes)
20/04/14 08:00:17 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 7.1 (TID 86, 178.62.198.251, executor 7, partition 2, NODE_LOCAL, 8336 bytes)
20/04/14 08:00:17 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 178.62.198.251:41743 (size: 6.9 KB, free: 3.0 GB)
20/04/14 08:00:17 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.198.251:41743 (size: 32.5 KB, free: 3.0 GB)
20/04/14 08:00:18 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.198.251:36310) with ID 8
20/04/14 08:00:18 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 7.1 (TID 87, 178.62.198.251, executor 8, partition 3, NODE_LOCAL, 8336 bytes)
20/04/14 08:00:18 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 7.1 (TID 88, 178.62.198.251, executor 8, partition 4, NODE_LOCAL, 8336 bytes)
20/04/14 08:00:18 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 7.1 (TID 89, 178.62.198.251, executor 8, partition 5, NODE_LOCAL, 8336 bytes)
20/04/14 08:00:18 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.198.251:44973 with 3.0 GB RAM, BlockManagerId(8, 178.62.198.251, 44973, None)
20/04/14 08:00:18 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 178.62.198.251:44973 (size: 6.9 KB, free: 3.0 GB)
20/04/14 08:00:19 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.198.251:44973 (size: 32.5 KB, free: 3.0 GB)
20/04/14 08:00:19 INFO yarn.ApplicationMaster: Final app status: FAILED, exitCode: 11, (reason: Max number of executor failures (6) reached)
20/04/14 08:00:19 INFO spark.SparkContext: Invoking stop() from shutdown hook
20/04/14 08:00:19 INFO server.AbstractConnector: Stopped Spark@24b8f8e9{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
20/04/14 08:00:19 INFO ui.SparkUI: Stopped Spark web UI at http://178.62.199.118:36681
20/04/14 08:00:19 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (rdd at GaussianMixture.scala:348) failed in 2.096 s due to Stage cancelled because SparkContext was shut down
20/04/14 08:00:19 INFO scheduler.DAGScheduler: Job 4 failed: treeAggregate at GaussianMixture.scala:384, took 32.102710 s
20/04/14 08:00:19 ERROR util.Instrumentation: org.apache.spark.SparkException: Job 4 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:933)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:931)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:931)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2130)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2043)
	at org.apache.spark.SparkContext$$anonfun$stop$6.apply$mcV$sp(SparkContext.scala:1949)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1948)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1143)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1137)
	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1206)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1182)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1.apply(GaussianMixture.scala:384)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1.apply(GaussianMixture.scala:340)
	at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)
	at org.apache.spark.ml.clustering.GaussianMixture.fit(GaussianMixture.scala:340)
	at org.apache.spark.ml.clustering.GaussianMixture.fit(GaussianMixture.scala:291)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

20/04/14 08:00:19 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s).
20/04/14 08:00:19 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors
20/04/14 08:00:19 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/04/14 08:00:19 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/04/14 08:00:19 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/04/14 08:00:19 INFO memory.MemoryStore: MemoryStore cleared
20/04/14 08:00:19 INFO storage.BlockManager: BlockManager stopped
20/04/14 08:00:19 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
20/04/14 08:00:19 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/04/14 08:00:19 INFO spark.SparkContext: Successfully stopped SparkContext
20/04/14 08:00:19 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: Max number of executor failures (6) reached)
20/04/14 08:00:19 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.
20/04/14 08:00:19 INFO yarn.ApplicationMaster: Deleting staging directory hdfs://178.62.197.79:9000/user/root/.sparkStaging/application_1586849858644_0006
20/04/14 08:00:19 INFO util.ShutdownHookManager: Shutdown hook called
20/04/14 08:00:19 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/spark-45ac57ce-aebd-46b9-88af-8945ce614f1c/pyspark-82ef6cb8-cc59-4a86-801c-4bef367f8583
20/04/14 08:00:19 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/spark-45ac57ce-aebd-46b9-88af-8945ce614f1c
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:1474
Log Contents:
Traceback (most recent call last):
  File "clustering.py", line 307, in <module>
    result_dfs_list=result_dfs,
  File "clustering.py", line 54, in wrapper
    ret = func(*args, **kwargs)
  File "clustering.py", line 72, in wrapper
    ret = func(*args, **kwargs)
  File "clustering.py", line 267, in perform_experiment
    result_df = perform_clustering_gaussian(in_file=in_file, k=k)
  File "clustering.py", line 203, in perform_clustering_gaussian
    model = fit(model_algo, df)
  File "clustering.py", line 54, in wrapper
    ret = func(*args, **kwargs)
  File "clustering.py", line 94, in fit
    model = model_algo.fit(df)
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/pyspark/ml/base.py", line 132, in fit
    return self._fit(dataset)
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/pyspark/ml/wrapper.py", line 295, in _fit
    java_model = self._fit_java(dataset)
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/pyspark/ml/wrapper.py", line 292, in _fit_java
    return self._java_obj.fit(dataset._jdf)
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 336, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling o50.fit
End of LogType:stdout



Container: container_1586849858644_0006_02_000003 on 178.62.199.118_35057
===========================================================================
LogType:stderr
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:22854
Log Contents:
20/04/14 07:59:31 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 21618@178.62.199.118
20/04/14 07:59:31 INFO util.SignalUtils: Registered signal handler for TERM
20/04/14 07:59:31 INFO util.SignalUtils: Registered signal handler for HUP
20/04/14 07:59:31 INFO util.SignalUtils: Registered signal handler for INT
20/04/14 07:59:32 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:32 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:32 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:32 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:32 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:32 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 45 ms (0 ms spent in bootstraps)
20/04/14 07:59:32 INFO spark.SecurityManager: Changing view acls to: root
20/04/14 07:59:32 INFO spark.SecurityManager: Changing modify acls to: root
20/04/14 07:59:32 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/14 07:59:32 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/14 07:59:32 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/14 07:59:32 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:44353 after 1 ms (0 ms spent in bootstraps)
20/04/14 07:59:32 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586849858644_0006/blockmgr-ecc0544a-89bb-47c7-b2ff-9da95fe8e242
20/04/14 07:59:32 INFO memory.MemoryStore: MemoryStore started with capacity 3.0 GB
20/04/14 07:59:32 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.199.118:44353
20/04/14 07:59:32 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/14 07:59:32 INFO executor.Executor: Starting executor ID 2 on host 178.62.199.118
20/04/14 07:59:32 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38045.
20/04/14 07:59:32 INFO netty.NettyBlockTransferService: Server created on 178.62.199.118:38045
20/04/14 07:59:32 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/14 07:59:32 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(2, 178.62.199.118, 38045, None)
20/04/14 07:59:32 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(2, 178.62.199.118, 38045, None)
20/04/14 07:59:32 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(2, 178.62.199.118, 38045, None)
20/04/14 07:59:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0
20/04/14 07:59:34 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
20/04/14 07:59:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
20/04/14 07:59:35 INFO client.TransportClientFactory: Successfully created connection to /178.62.199.118:45281 after 2 ms (0 ms spent in bootstraps)
20/04/14 07:59:35 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.2 KB, free 3.0 GB)
20/04/14 07:59:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 254 ms
20/04/14 07:59:35 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 127.0 KB, free 3.0 GB)
20/04/14 07:59:36 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2496 bytes result sent to driver
20/04/14 07:59:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2
20/04/14 07:59:37 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 2)
20/04/14 07:59:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5
20/04/14 07:59:37 INFO executor.Executor: Running task 4.0 in stage 1.0 (TID 5)
20/04/14 07:59:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8
20/04/14 07:59:37 INFO executor.Executor: Running task 7.0 in stage 1.0 (TID 8)
20/04/14 07:59:37 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
20/04/14 07:59:37 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.5 KB, free 3.0 GB)
20/04/14 07:59:37 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 10 ms
20/04/14 07:59:37 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 14.1 KB, free 3.0 GB)
20/04/14 07:59:38 INFO codegen.CodeGenerator: Code generated in 173.380654 ms
20/04/14 07:59:38 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00003-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4855590, partition values: [empty row]
20/04/14 07:59:38 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00002-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4735846, partition values: [empty row]
20/04/14 07:59:38 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.197.79:9000/data/df_3-shingles_sparse-binary-vectors.parquet/part-00005-c42f3e91-9947-42f1-ac3e-b810b403f36b-c000.snappy.parquet, range: 0-4950456, partition values: [empty row]
20/04/14 07:59:38 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/14 07:59:38 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 32.5 KB, free 3.0 GB)
20/04/14 07:59:38 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 8 ms
20/04/14 07:59:38 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 540.2 KB, free 3.0 GB)
20/04/14 07:59:38 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:38 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:38 INFO parquet.ParquetReadSupport: Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

Catalyst form:
StructType(StructField(entry,StringType,true), StructField(entry_name,StringType,true), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true))
       
20/04/14 07:59:38 INFO codegen.CodeGenerator: Code generated in 18.193582 ms
20/04/14 07:59:38 INFO codegen.CodeGenerator: Code generated in 13.336903 ms
20/04/14 07:59:38 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 9550 records.
20/04/14 07:59:38 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 11838 records.
20/04/14 07:59:38 INFO hadoop.InternalParquetRecordReader: RecordReader initialized will read a total of 6080 records.
20/04/14 07:59:38 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:38 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:38 INFO hadoop.InternalParquetRecordReader: at row 0. reading next block
20/04/14 07:59:38 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:38 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:38 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
20/04/14 07:59:38 INFO hadoop.InternalParquetRecordReader: block read in memory in 69 ms. row count = 9550
20/04/14 07:59:38 INFO hadoop.InternalParquetRecordReader: block read in memory in 40 ms. row count = 11838
20/04/14 07:59:38 INFO hadoop.InternalParquetRecordReader: block read in memory in 66 ms. row count = 6080
20/04/14 07:59:39 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 1487 bytes result sent to driver
20/04/14 07:59:39 INFO executor.Executor: Finished task 7.0 in stage 1.0 (TID 8). 1487 bytes result sent to driver
20/04/14 07:59:39 INFO executor.Executor: Finished task 4.0 in stage 1.0 (TID 5). 1487 bytes result sent to driver
20/04/14 07:59:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14
20/04/14 07:59:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17
20/04/14 07:59:42 INFO executor.Executor: Running task 6.0 in stage 4.0 (TID 17)
20/04/14 07:59:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19
20/04/14 07:59:42 INFO executor.Executor: Running task 3.0 in stage 4.0 (TID 14)
20/04/14 07:59:42 INFO executor.Executor: Running task 8.0 in stage 4.0 (TID 19)
20/04/14 07:59:42 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/14 07:59:42 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
20/04/14 07:59:42 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.6 KB, free 3.0 GB)
20/04/14 07:59:42 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 16 ms
20/04/14 07:59:42 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.9 KB, free 3.0 GB)
20/04/14 07:59:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:43 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/14 07:59:43 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.199.118:44353)
20/04/14 07:59:43 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 3 local blocks and 6 remote blocks
20/04/14 07:59:43 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:37949 after 10 ms (0 ms spent in bootstraps)
20/04/14 07:59:43 INFO client.TransportClientFactory: Successfully created connection to /178.62.198.251:41839 after 10 ms (0 ms spent in bootstraps)
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 20 ms
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 29 ms
20/04/14 07:59:43 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 30 ms
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_3 stored as values in memory (estimated size 14.8 MB, free 3.0 GB)
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_6 stored as values in memory (estimated size 15.1 MB, free 3.0 GB)
20/04/14 07:59:43 INFO memory.MemoryStore: Block rdd_7_8 stored as values in memory (estimated size 14.7 MB, free 3.0 GB)
20/04/14 07:59:43 INFO codegen.CodeGenerator: Code generated in 6.536373 ms
20/04/14 07:59:43 INFO codegen.CodeGenerator: Code generated in 23.720425 ms
20/04/14 07:59:44 INFO codegen.CodeGenerator: Code generated in 13.164524 ms
20/04/14 07:59:44 INFO executor.Executor: Finished task 6.0 in stage 4.0 (TID 17). 1583 bytes result sent to driver
20/04/14 07:59:44 INFO executor.Executor: Finished task 8.0 in stage 4.0 (TID 19). 1583 bytes result sent to driver
20/04/14 07:59:44 INFO executor.Executor: Finished task 3.0 in stage 4.0 (TID 14). 1583 bytes result sent to driver
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 29
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32
20/04/14 07:59:44 INFO executor.Executor: Running task 3.0 in stage 6.0 (TID 29)
20/04/14 07:59:44 INFO executor.Executor: Running task 6.0 in stage 6.0 (TID 32)
20/04/14 07:59:44 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 35
20/04/14 07:59:44 INFO executor.Executor: Running task 8.0 in stage 6.0 (TID 35)
20/04/14 07:59:44 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
20/04/14 07:59:44 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.7 KB, free 3.0 GB)
20/04/14 07:59:44 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 9 ms
20/04/14 07:59:44 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.1 KB, free 3.0 GB)
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_3 locally
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_8 locally
20/04/14 07:59:44 INFO storage.BlockManager: Found block rdd_7_6 locally
20/04/14 07:59:44 INFO executor.Executor: Finished task 8.0 in stage 6.0 (TID 35). 3189 bytes result sent to driver
20/04/14 07:59:44 INFO executor.Executor: Finished task 3.0 in stage 6.0 (TID 29). 15567 bytes result sent to driver
20/04/14 07:59:44 INFO executor.Executor: Finished task 6.0 in stage 6.0 (TID 32). 4612 bytes result sent to driver
20/04/14 07:59:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 49
20/04/14 07:59:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 52
20/04/14 07:59:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 55
20/04/14 07:59:47 INFO executor.Executor: Running task 6.0 in stage 8.0 (TID 52)
20/04/14 07:59:47 INFO executor.Executor: Running task 3.0 in stage 8.0 (TID 49)
20/04/14 07:59:47 INFO executor.Executor: Running task 8.0 in stage 8.0 (TID 55)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.2 KB, free 3.0 GB)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 12 ms
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 19.7 KB, free 3.0 GB)
20/04/14 07:59:47 INFO storage.BlockManager: Found block rdd_7_8 locally
20/04/14 07:59:47 INFO storage.BlockManager: Found block rdd_7_6 locally
20/04/14 07:59:47 INFO storage.BlockManager: Found block rdd_7_3 locally
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 85.0 B, free 3.0 GB)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 6 ms
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 56.0 B, free 3.0 GB)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/14 07:59:47 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.7 MB, free 3.0 GB)
20/04/14 07:59:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 14 ms
20/04/14 07:59:48 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 551.7 MB, free 2.4 GB)
20/04/14 07:59:52 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
20/04/14 07:59:52 INFO storage.DiskBlockManager: Shutdown hook called
20/04/14 07:59:52 ERROR executor.Executor: Exception in task 3.0 in stage 8.0 (TID 49)
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:52 INFO executor.Executor: Not reporting error to driver during JVM shutdown.
20/04/14 07:59:52 ERROR util.SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 49,5,main]
java.lang.OutOfMemoryError: Java heap space
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)
	at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:345)
	at breeze.linalg.DenseMatrix$.zeros(DenseMatrix.scala:333)
	at breeze.linalg.MatrixConstructors$class.tabulate(Matrix.scala:228)
	at breeze.linalg.DenseMatrix$.tabulate(DenseMatrix.scala:333)
	at breeze.linalg.package$.lowerTriangular(package.scala:205)
	at breeze.linalg.eigSym$.breeze$linalg$eigSym$$doEigSym(eig.scala:124)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:96)
	at breeze.linalg.eigSym$EigSym_DM_Impl$.apply(eig.scala:94)
	at breeze.generic.UFunc$class.apply(UFunc.scala:48)
	at breeze.linalg.eigSym$.apply(eig.scala:91)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.calculateCovarianceConstants(MultivariateGaussian.scala:117)
	at org.apache.spark.ml.stat.distribution.MultivariateGaussian.<init>(MultivariateGaussian.scala:58)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:610)
	at org.apache.spark.ml.clustering.ExpectationAggregator$$anonfun$oldGaussians$1.apply(GaussianMixture.scala:608)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians$lzycompute(GaussianMixture.scala:608)
	at org.apache.spark.ml.clustering.ExpectationAggregator.oldGaussians(GaussianMixture.scala:607)
	at org.apache.spark.ml.clustering.ExpectationAggregator.add(GaussianMixture.scala:633)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:386)
	at org.apache.spark.ml.clustering.GaussianMixture$$anonfun$fit$1$$anonfun$11.apply(GaussianMixture.scala:385)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
20/04/14 07:59:52 INFO util.ShutdownHookManager: Shutdown hook called
End of LogType:stderr

LogType:stdout
Log Upload Time:Tue Apr 14 08:00:20 +0000 2020
LogLength:124
Log Contents:
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill %p"
#   Executing /bin/sh -c "kill 21618"...
End of LogType:stdout

