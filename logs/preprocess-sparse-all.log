

Container: container_1586780871303_0007_01_000002 on 178.62.200.211_37461
===========================================================================
LogType:stderr
Log Upload Time:Mon Apr 13 15:29:16 +0000 2020
LogLength:118297
Log Contents:
20/04/13 15:24:40 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 22265@178.62.200.211
20/04/13 15:24:40 INFO util.SignalUtils: Registered signal handler for TERM
20/04/13 15:24:40 INFO util.SignalUtils: Registered signal handler for HUP
20/04/13 15:24:40 INFO util.SignalUtils: Registered signal handler for INT
20/04/13 15:24:41 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 15:24:41 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 15:24:41 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 15:24:41 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 15:24:41 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 15:24:42 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:39481 after 88 ms (0 ms spent in bootstraps)
20/04/13 15:24:42 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 15:24:42 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 15:24:42 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 15:24:42 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 15:24:42 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 15:24:42 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:39481 after 6 ms (0 ms spent in bootstraps)
20/04/13 15:24:42 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0007/blockmgr-a762928b-1d3f-44db-b717-bce06f47641c
20/04/13 15:24:42 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
20/04/13 15:24:43 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.210.13:39481
20/04/13 15:24:43 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/13 15:24:43 INFO executor.Executor: Starting executor ID 1 on host 178.62.200.211
20/04/13 15:24:43 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36927.
20/04/13 15:24:43 INFO netty.NettyBlockTransferService: Server created on 178.62.200.211:36927
20/04/13 15:24:43 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/13 15:24:43 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, 178.62.200.211, 36927, None)
20/04/13 15:24:43 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, 178.62.200.211, 36927, None)
20/04/13 15:24:43 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, 178.62.200.211, 36927, None)
20/04/13 15:24:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1
20/04/13 15:24:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4
20/04/13 15:24:49 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)
20/04/13 15:24:49 INFO executor.Executor: Running task 3.0 in stage 1.0 (TID 4)
20/04/13 15:24:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3
20/04/13 15:24:49 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:36375 after 6 ms (0 ms spent in bootstraps)
20/04/13 15:24:49 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1458.6 MB)
20/04/13 15:24:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 160 ms
20/04/13 15:24:50 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.0 KB, free 1458.6 MB)
20/04/13 15:24:51 INFO codegen.CodeGenerator: Code generated in 529.898616 ms
20/04/13 15:24:53 INFO codegen.CodeGenerator: Code generated in 31.135196 ms
20/04/13 15:24:53 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:24:53 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:24:53 INFO codegen.CodeGenerator: Code generated in 38.641563 ms
20/04/13 15:24:53 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
20/04/13 15:24:53 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:35537 after 9 ms (0 ms spent in bootstraps)
20/04/13 15:24:53 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.5 MB)
20/04/13 15:24:53 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 68 ms
20/04/13 15:24:53 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 536.2 KB, free 1458.0 MB)
20/04/13 15:24:55 INFO executor.Executor: Finished task 3.0 in stage 1.0 (TID 4). 1640 bytes result sent to driver
20/04/13 15:24:55 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 1640 bytes result sent to driver
20/04/13 15:24:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8
20/04/13 15:24:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11
20/04/13 15:24:56 INFO executor.Executor: Running task 1.0 in stage 2.0 (TID 8)
20/04/13 15:24:56 INFO executor.Executor: Running task 4.0 in stage 2.0 (TID 11)
20/04/13 15:24:56 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
20/04/13 15:24:56 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1458.0 MB)
20/04/13 15:24:56 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 16 ms
20/04/13 15:24:56 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 27.4 KB, free 1458.0 MB)
20/04/13 15:24:56 INFO codegen.CodeGenerator: Code generated in 27.218732 ms
20/04/13 15:24:57 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:24:57 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
20/04/13 15:24:57 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:24:57 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.0 MB)
20/04/13 15:24:57 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 27 ms
20/04/13 15:24:57 INFO codegen.CodeGenerator: Code generated in 86.54083 ms
20/04/13 15:24:57 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 536.2 KB, free 1457.4 MB)
20/04/13 15:24:57 INFO codegen.CodeGenerator: Code generated in 63.194702 ms
20/04/13 15:24:57 INFO codegen.CodeGenerator: Code generated in 118.065208 ms
20/04/13 15:25:13 INFO python.PythonUDFRunner: Times: total = 7692, boot = 393, init = 433, finish = 6866
20/04/13 15:25:13 INFO executor.Executor: Finished task 4.0 in stage 2.0 (TID 11). 2464 bytes result sent to driver
20/04/13 15:25:13 INFO python.PythonUDFRunner: Times: total = 5620, boot = 437, init = 359, finish = 4824
20/04/13 15:25:13 INFO executor.Executor: Finished task 1.0 in stage 2.0 (TID 8). 2421 bytes result sent to driver
20/04/13 15:25:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13
20/04/13 15:25:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16
20/04/13 15:25:13 INFO executor.Executor: Running task 3.0 in stage 3.0 (TID 16)
20/04/13 15:25:13 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 13)
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/13 15:25:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/13 15:25:13 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:44585 after 5 ms (0 ms spent in bootstraps)
20/04/13 15:25:13 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1976.0 B, free 1457.4 MB)
20/04/13 15:25:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 38 ms
20/04/13 15:25:13 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 1457.4 MB)
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:25:13 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:25:13 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:25:13 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 32 ms
20/04/13 15:25:13 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 35 ms
20/04/13 15:25:14 INFO memory.MemoryStore: Block rdd_21_0 stored as values in memory (estimated size 147.7 KB, free 1457.3 MB)
20/04/13 15:25:14 INFO memory.MemoryStore: Block rdd_21_3 stored as values in memory (estimated size 147.4 KB, free 1457.1 MB)
20/04/13 15:25:14 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 13). 1176 bytes result sent to driver
20/04/13 15:25:14 INFO executor.Executor: Finished task 3.0 in stage 3.0 (TID 16). 1176 bytes result sent to driver
20/04/13 15:25:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20
20/04/13 15:25:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 23
20/04/13 15:25:14 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 20)
20/04/13 15:25:14 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/13 15:25:14 INFO executor.Executor: Running task 3.0 in stage 5.0 (TID 23)
20/04/13 15:25:14 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1457.1 MB)
20/04/13 15:25:14 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 21 ms
20/04/13 15:25:14 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.2 KB, free 1457.1 MB)
20/04/13 15:25:14 INFO storage.BlockManager: Found block rdd_21_0 locally
20/04/13 15:25:14 INFO storage.BlockManager: Found block rdd_21_3 locally
20/04/13 15:25:14 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 20). 40880 bytes result sent to driver
20/04/13 15:25:14 INFO executor.Executor: Finished task 3.0 in stage 5.0 (TID 23). 41008 bytes result sent to driver
20/04/13 15:25:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25
20/04/13 15:25:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 28
20/04/13 15:25:15 INFO executor.Executor: Running task 3.0 in stage 6.0 (TID 28)
20/04/13 15:25:15 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 25)
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 124.8 KB, free 1457.0 MB)
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 15 ms
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 347.5 KB, free 1456.7 MB)
20/04/13 15:25:15 INFO codegen.CodeGenerator: Code generated in 25.881863 ms
20/04/13 15:25:15 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:25:15 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:25:15 INFO codegen.CodeGenerator: Code generated in 35.761142 ms
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9
20/04/13 15:25:15 INFO codegen.CodeGenerator: Code generated in 45.129045 ms
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.6 MB)
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 26 ms
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 536.2 KB, free 1456.1 MB)
20/04/13 15:25:15 INFO codegen.CodeGenerator: Code generated in 114.069994 ms
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 77.2 KB, free 1392.0 MB)
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 21 ms
20/04/13 15:25:15 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:15 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:15 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:25:15 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:25:15 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:25:15 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:25:15 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:25:15 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:25:15 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:25:15 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:25:15 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:25:15 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 987.3 KB, free 1391.1 MB)
20/04/13 15:25:16 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:16 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:25:16 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:25:16 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:25:16 INFO compress.CodecPool: Got brand-new compressor [.snappy]
20/04/13 15:25:16 INFO compress.CodecPool: Got brand-new compressor [.snappy]
20/04/13 15:25:31 INFO python.PythonUDFRunner: Times: total = 1209, boot = -13127, init = 13250, finish = 1086
20/04/13 15:25:31 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8690439
20/04/13 15:25:31 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152515_0006_m_000003_28' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152515_0006_m_000003
20/04/13 15:25:31 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152515_0006_m_000003_28: Committed
20/04/13 15:25:31 INFO executor.Executor: Finished task 3.0 in stage 6.0 (TID 28). 3241 bytes result sent to driver
20/04/13 15:25:32 INFO python.PythonUDFRunner: Times: total = 7236, boot = -11050, init = 11201, finish = 7085
20/04/13 15:25:32 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7933678
20/04/13 15:25:32 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152515_0006_m_000000_25' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152515_0006_m_000000
20/04/13 15:25:32 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152515_0006_m_000000_25: Committed
20/04/13 15:25:32 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 25). 3198 bytes result sent to driver
20/04/13 15:25:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32
20/04/13 15:25:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 35
20/04/13 15:25:34 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 32)
20/04/13 15:25:34 INFO executor.Executor: Running task 3.0 in stage 8.0 (TID 35)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1456.1 MB)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 15 ms
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 15.0 KB, free 1456.1 MB)
20/04/13 15:25:34 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13
20/04/13 15:25:34 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.1 MB)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 25 ms
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 536.2 KB, free 1455.6 MB)
20/04/13 15:25:34 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 32). 1554 bytes result sent to driver
20/04/13 15:25:34 INFO executor.Executor: Finished task 3.0 in stage 8.0 (TID 35). 1554 bytes result sent to driver
20/04/13 15:25:34 INFO storage.BlockManager: Removing RDD 21
20/04/13 15:25:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 39
20/04/13 15:25:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 42
20/04/13 15:25:34 INFO executor.Executor: Running task 1.0 in stage 9.0 (TID 39)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16
20/04/13 15:25:34 INFO executor.Executor: Running task 4.0 in stage 9.0 (TID 42)
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1456.4 MB)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 17 ms
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 27.4 KB, free 1456.4 MB)
20/04/13 15:25:34 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15
20/04/13 15:25:34 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.4 MB)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 23 ms
20/04/13 15:25:35 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 536.2 KB, free 1455.8 MB)
20/04/13 15:25:35 INFO codegen.CodeGenerator: Code generated in 116.237946 ms
20/04/13 15:25:46 INFO python.PythonUDFRunner: Times: total = 1196, boot = -18184, init = 18330, finish = 1050
20/04/13 15:25:46 INFO executor.Executor: Finished task 4.0 in stage 9.0 (TID 42). 2421 bytes result sent to driver
20/04/13 15:25:47 INFO python.PythonUDFRunner: Times: total = 2811, boot = -12184, init = 12340, finish = 2655
20/04/13 15:25:47 INFO executor.Executor: Finished task 1.0 in stage 9.0 (TID 39). 2421 bytes result sent to driver
20/04/13 15:25:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 45
20/04/13 15:25:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 48
20/04/13 15:25:48 INFO executor.Executor: Running task 1.0 in stage 10.0 (TID 45)
20/04/13 15:25:48 INFO executor.Executor: Running task 4.0 in stage 10.0 (TID 48)
20/04/13 15:25:48 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
20/04/13 15:25:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17
20/04/13 15:25:48 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 1976.0 B, free 1455.8 MB)
20/04/13 15:25:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 8 ms
20/04/13 15:25:48 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 3.3 KB, free 1455.8 MB)
20/04/13 15:25:48 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
20/04/13 15:25:48 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:25:48 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
20/04/13 15:25:49 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:25:49 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:25:49 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:25:49 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 15:25:49 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 14 ms
20/04/13 15:25:49 INFO memory.MemoryStore: Block rdd_51_1 stored as values in memory (estimated size 147.4 KB, free 1455.7 MB)
20/04/13 15:25:49 INFO executor.Executor: Finished task 1.0 in stage 10.0 (TID 45). 1176 bytes result sent to driver
20/04/13 15:25:49 INFO memory.MemoryStore: Block rdd_51_4 stored as values in memory (estimated size 147.7 KB, free 1455.5 MB)
20/04/13 15:25:49 INFO executor.Executor: Finished task 4.0 in stage 10.0 (TID 48). 1176 bytes result sent to driver
20/04/13 15:25:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 51
20/04/13 15:25:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 54
20/04/13 15:25:49 INFO executor.Executor: Running task 1.0 in stage 12.0 (TID 51)
20/04/13 15:25:49 INFO executor.Executor: Running task 4.0 in stage 12.0 (TID 54)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1455.5 MB)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 9 ms
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 4.2 KB, free 1455.5 MB)
20/04/13 15:25:49 INFO storage.BlockManager: Found block rdd_51_1 locally
20/04/13 15:25:49 INFO storage.BlockManager: Found block rdd_51_4 locally
20/04/13 15:25:49 INFO executor.Executor: Finished task 1.0 in stage 12.0 (TID 51). 40772 bytes result sent to driver
20/04/13 15:25:49 INFO executor.Executor: Finished task 4.0 in stage 12.0 (TID 54). 40923 bytes result sent to driver
20/04/13 15:25:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 57
20/04/13 15:25:49 INFO executor.Executor: Running task 1.0 in stage 13.0 (TID 57)
20/04/13 15:25:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 60
20/04/13 15:25:49 INFO executor.Executor: Running task 4.0 in stage 13.0 (TID 60)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 124.7 KB, free 1455.4 MB)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 10 ms
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 347.5 KB, free 1455.1 MB)
20/04/13 15:25:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20
20/04/13 15:25:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1455.0 MB)
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 29 ms
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 536.2 KB, free 1454.5 MB)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 77.2 KB, free 1390.4 MB)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 20 ms
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 987.3 KB, free 1389.5 MB)
20/04/13 15:25:49 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:49 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:25:49 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:49 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:25:49 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:25:50 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:26:04 INFO python.PythonUDFRunner: Times: total = 930, boot = -13603, init = 13722, finish = 811
20/04/13 15:26:04 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9472166
20/04/13 15:26:04 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152549_0013_m_000004_60' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152549_0013_m_000004
20/04/13 15:26:04 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152549_0013_m_000004_60: Committed
20/04/13 15:26:04 INFO executor.Executor: Finished task 4.0 in stage 13.0 (TID 60). 3198 bytes result sent to driver
20/04/13 15:26:05 INFO python.PythonUDFRunner: Times: total = 3242, boot = -11991, init = 12036, finish = 3197
20/04/13 15:26:05 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8604817
20/04/13 15:26:05 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152549_0013_m_000001_57' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152549_0013_m_000001
20/04/13 15:26:05 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152549_0013_m_000001_57: Committed
20/04/13 15:26:05 INFO executor.Executor: Finished task 1.0 in stage 13.0 (TID 57). 3198 bytes result sent to driver
20/04/13 15:26:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 62
20/04/13 15:26:07 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 62)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 23
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1453.5 MB)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 23 took 7 ms
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 8.9 KB, free 1453.5 MB)
20/04/13 15:26:07 INFO codegen.CodeGenerator: Code generated in 19.307579 ms
20/04/13 15:26:07 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1453.4 MB)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 9 ms
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 536.2 KB, free 1452.9 MB)
20/04/13 15:26:07 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 62). 1332 bytes result sent to driver
20/04/13 15:26:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 65
20/04/13 15:26:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68
20/04/13 15:26:07 INFO executor.Executor: Running task 5.0 in stage 15.0 (TID 68)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25
20/04/13 15:26:07 INFO executor.Executor: Running task 2.0 in stage 15.0 (TID 65)
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1452.9 MB)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 11 ms
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.0 KB, free 1452.9 MB)
20/04/13 15:26:07 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1452.9 MB)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 10 ms
20/04/13 15:26:07 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 536.2 KB, free 1452.3 MB)
20/04/13 15:26:07 INFO executor.Executor: Finished task 5.0 in stage 15.0 (TID 68). 1554 bytes result sent to driver
20/04/13 15:26:07 INFO executor.Executor: Finished task 2.0 in stage 15.0 (TID 65). 1554 bytes result sent to driver
20/04/13 15:26:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 71
20/04/13 15:26:07 INFO executor.Executor: Running task 2.0 in stage 16.0 (TID 71)
20/04/13 15:26:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 74
20/04/13 15:26:07 INFO executor.Executor: Running task 5.0 in stage 16.0 (TID 74)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1452.3 MB)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 12 ms
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 27.4 KB, free 1452.3 MB)
20/04/13 15:26:07 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:26:07 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1452.3 MB)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 26 took 16 ms
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 536.2 KB, free 1451.7 MB)
20/04/13 15:26:08 INFO codegen.CodeGenerator: Code generated in 90.305076 ms
20/04/13 15:26:13 INFO python.PythonUDFRunner: Times: total = 599, boot = -14933, init = 15034, finish = 498
20/04/13 15:26:14 INFO executor.Executor: Finished task 5.0 in stage 16.0 (TID 74). 2464 bytes result sent to driver
20/04/13 15:26:20 INFO python.PythonUDFRunner: Times: total = 1021, boot = -17259, init = 17380, finish = 900
20/04/13 15:26:21 INFO executor.Executor: Finished task 2.0 in stage 16.0 (TID 71). 2421 bytes result sent to driver
20/04/13 15:26:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 75
20/04/13 15:26:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 78
20/04/13 15:26:24 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 75)
20/04/13 15:26:24 INFO executor.Executor: Running task 3.0 in stage 17.0 (TID 78)
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache
20/04/13 15:26:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 28
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 1977.0 B, free 1451.7 MB)
20/04/13 15:26:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 28 took 8 ms
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 3.3 KB, free 1451.7 MB)
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:26:24 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:26:24 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:26:24 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 15:26:24 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 15:26:24 INFO memory.MemoryStore: Block rdd_81_3 stored as values in memory (estimated size 2.6 MB, free 1449.1 MB)
20/04/13 15:26:24 INFO executor.Executor: Finished task 3.0 in stage 17.0 (TID 78). 1176 bytes result sent to driver
20/04/13 15:26:24 INFO memory.MemoryStore: Block rdd_81_0 stored as values in memory (estimated size 2.6 MB, free 1446.5 MB)
20/04/13 15:26:24 INFO executor.Executor: Finished task 0.0 in stage 17.0 (TID 75). 1176 bytes result sent to driver
20/04/13 15:26:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 82
20/04/13 15:26:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 85
20/04/13 15:26:24 INFO executor.Executor: Running task 0.0 in stage 19.0 (TID 82)
20/04/13 15:26:24 INFO executor.Executor: Running task 3.0 in stage 19.0 (TID 85)
20/04/13 15:26:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 29
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1446.5 MB)
20/04/13 15:26:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 29 took 8 ms
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 4.2 KB, free 1446.5 MB)
20/04/13 15:26:24 INFO storage.BlockManager: Found block rdd_81_0 locally
20/04/13 15:26:24 INFO storage.BlockManager: Found block rdd_81_3 locally
20/04/13 15:26:25 INFO executor.Executor: Finished task 3.0 in stage 19.0 (TID 85). 676521 bytes result sent to driver
20/04/13 15:26:25 INFO executor.Executor: Finished task 0.0 in stage 19.0 (TID 82). 677273 bytes result sent to driver
20/04/13 15:26:25 INFO storage.BlockManager: Removing RDD 51
20/04/13 15:26:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 87
20/04/13 15:26:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 90
20/04/13 15:26:26 INFO executor.Executor: Running task 0.0 in stage 20.0 (TID 87)
20/04/13 15:26:26 INFO executor.Executor: Running task 3.0 in stage 20.0 (TID 90)
20/04/13 15:26:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 32
20/04/13 15:26:26 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 1062.5 KB, free 1450.2 MB)
20/04/13 15:26:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 32 took 33 ms
20/04/13 15:26:27 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 1883.3 KB, free 1448.3 MB)
20/04/13 15:26:27 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:26:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 31
20/04/13 15:26:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:27 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:27 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:27 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:26:27 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:26:27 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:26:27 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:26:27 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:26:27 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:26:27 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:26:27 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:26:27 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:26:27 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:26:27 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:26:27 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:26:27 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1448.3 MB)
20/04/13 15:26:27 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 164062
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:26:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 31 took 39 ms
20/04/13 15:26:27 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 536.2 KB, free 1447.8 MB)
20/04/13 15:26:27 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:26:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:27 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:27 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 30
20/04/13 15:26:27 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 1770.3 KB, free 1382.0 MB)
20/04/13 15:26:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 30 took 42 ms
20/04/13 15:26:28 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 18.8 MB, free 1363.2 MB)
20/04/13 15:26:28 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:26:28 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:26:28 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 164062
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:26:42 INFO python.PythonUDFRunner: Times: total = 627, boot = -18158, init = 18212, finish = 573
20/04/13 15:26:42 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12201836
20/04/13 15:26:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152626_0020_m_000003_90' to hdfs://178.62.208.209:9000/data/df_4-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152626_0020_m_000003
20/04/13 15:26:43 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152626_0020_m_000003_90: Committed
20/04/13 15:26:43 INFO executor.Executor: Finished task 3.0 in stage 20.0 (TID 90). 3198 bytes result sent to driver
20/04/13 15:26:44 INFO python.PythonUDFRunner: Times: total = 6829, boot = -18501, init = 18581, finish = 6749
20/04/13 15:26:44 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12509081
20/04/13 15:26:44 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152626_0020_m_000000_87' to hdfs://178.62.208.209:9000/data/df_4-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152626_0020_m_000000
20/04/13 15:26:44 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152626_0020_m_000000_87: Committed
20/04/13 15:26:44 INFO executor.Executor: Finished task 0.0 in stage 20.0 (TID 87). 3198 bytes result sent to driver
20/04/13 15:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 93
20/04/13 15:26:46 INFO executor.Executor: Running task 0.0 in stage 21.0 (TID 93)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 34
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1427.8 MB)
20/04/13 15:26:46 INFO storage.BlockManager: Removing RDD 81
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 34 took 7 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 8.9 KB, free 1433.0 MB)
20/04/13 15:26:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 33
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1433.0 MB)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 33 took 10 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 536.2 KB, free 1433.0 MB)
20/04/13 15:26:46 INFO executor.Executor: Finished task 0.0 in stage 21.0 (TID 93). 1332 bytes result sent to driver
20/04/13 15:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 94
20/04/13 15:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 97
20/04/13 15:26:46 INFO executor.Executor: Running task 0.0 in stage 22.0 (TID 94)
20/04/13 15:26:46 INFO executor.Executor: Running task 3.0 in stage 22.0 (TID 97)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 36
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1435.9 MB)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 36 took 14 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 15.0 KB, free 1435.9 MB)
20/04/13 15:26:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 35
20/04/13 15:26:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1435.8 MB)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 35 took 6 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 536.2 KB, free 1435.3 MB)
20/04/13 15:26:46 INFO executor.Executor: Finished task 0.0 in stage 22.0 (TID 94). 1554 bytes result sent to driver
20/04/13 15:26:46 INFO executor.Executor: Finished task 3.0 in stage 22.0 (TID 97). 1554 bytes result sent to driver
20/04/13 15:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 101
20/04/13 15:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 104
20/04/13 15:26:46 INFO executor.Executor: Running task 1.0 in stage 23.0 (TID 101)
20/04/13 15:26:46 INFO executor.Executor: Running task 4.0 in stage 23.0 (TID 104)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 38
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1435.3 MB)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 38 took 9 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 27.4 KB, free 1435.3 MB)
20/04/13 15:26:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 37
20/04/13 15:26:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1435.2 MB)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 37 took 33 ms
20/04/13 15:26:46 INFO codegen.CodeGenerator: Code generated in 42.309159 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 536.2 KB, free 1434.7 MB)
20/04/13 15:27:00 INFO python.PythonUDFRunner: Times: total = 2872, boot = -12923, init = 13028, finish = 2767
20/04/13 15:27:00 INFO executor.Executor: Finished task 4.0 in stage 23.0 (TID 104). 2421 bytes result sent to driver
20/04/13 15:27:01 INFO python.PythonUDFRunner: Times: total = 793, boot = -19062, init = 19186, finish = 669
20/04/13 15:27:01 INFO executor.Executor: Finished task 1.0 in stage 23.0 (TID 101). 2421 bytes result sent to driver
20/04/13 15:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 108
20/04/13 15:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 111
20/04/13 15:27:03 INFO executor.Executor: Running task 2.0 in stage 24.0 (TID 108)
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache
20/04/13 15:27:03 INFO executor.Executor: Running task 5.0 in stage 24.0 (TID 111)
20/04/13 15:27:03 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 39
20/04/13 15:27:03 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 1977.0 B, free 1434.7 MB)
20/04/13 15:27:03 INFO broadcast.TorrentBroadcast: Reading broadcast variable 39 took 9 ms
20/04/13 15:27:03 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 3.3 KB, free 1434.7 MB)
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:27:03 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:27:03 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 15:27:03 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:27:03 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 15 ms
20/04/13 15:27:04 INFO memory.MemoryStore: Block rdd_111_2 stored as values in memory (estimated size 2.6 MB, free 1432.1 MB)
20/04/13 15:27:04 INFO executor.Executor: Finished task 2.0 in stage 24.0 (TID 108). 1176 bytes result sent to driver
20/04/13 15:27:04 INFO memory.MemoryStore: Block rdd_111_5 stored as values in memory (estimated size 2.6 MB, free 1429.5 MB)
20/04/13 15:27:04 INFO executor.Executor: Finished task 5.0 in stage 24.0 (TID 111). 1176 bytes result sent to driver
20/04/13 15:27:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 114
20/04/13 15:27:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 117
20/04/13 15:27:04 INFO executor.Executor: Running task 2.0 in stage 26.0 (TID 114)
20/04/13 15:27:04 INFO executor.Executor: Running task 5.0 in stage 26.0 (TID 117)
20/04/13 15:27:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 40
20/04/13 15:27:04 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1429.5 MB)
20/04/13 15:27:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 40 took 7 ms
20/04/13 15:27:04 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 4.2 KB, free 1429.5 MB)
20/04/13 15:27:04 INFO storage.BlockManager: Found block rdd_111_5 locally
20/04/13 15:27:04 INFO storage.BlockManager: Found block rdd_111_2 locally
20/04/13 15:27:04 INFO executor.Executor: Finished task 5.0 in stage 26.0 (TID 117). 675274 bytes result sent to driver
20/04/13 15:27:04 INFO executor.Executor: Finished task 2.0 in stage 26.0 (TID 114). 677986 bytes result sent to driver
20/04/13 15:27:06 INFO storage.BlockManager: Removing RDD 111
20/04/13 15:27:06 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 120
20/04/13 15:27:06 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 123
20/04/13 15:27:06 INFO executor.Executor: Running task 2.0 in stage 27.0 (TID 120)
20/04/13 15:27:06 INFO executor.Executor: Running task 5.0 in stage 27.0 (TID 123)
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 43
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 1062.2 KB, free 1456.0 MB)
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Reading broadcast variable 43 took 16 ms
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 1883.3 KB, free 1454.1 MB)
20/04/13 15:27:06 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 42
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1454.1 MB)
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Reading broadcast variable 42 took 64 ms
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 536.2 KB, free 1453.6 MB)
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 41
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 1770.4 KB, free 1419.8 MB)
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Reading broadcast variable 41 took 18 ms
20/04/13 15:27:08 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 18.8 MB, free 1369.0 MB)
20/04/13 15:27:08 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:27:08 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:27:08 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:27:08 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:27:08 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 164062
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:27:08 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 164062
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:27:14 INFO python.PythonUDFRunner: Times: total = 413, boot = -18975, init = 19111, finish = 277
20/04/13 15:27:14 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 4632982
20/04/13 15:27:14 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152706_0027_m_000005_123' to hdfs://178.62.208.209:9000/data/df_4-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152706_0027_m_000005
20/04/13 15:27:14 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152706_0027_m_000005_123: Committed
20/04/13 15:27:14 INFO executor.Executor: Finished task 5.0 in stage 27.0 (TID 123). 3198 bytes result sent to driver
20/04/13 15:27:22 INFO python.PythonUDFRunner: Times: total = 4528, boot = -16874, init = 16990, finish = 4412
20/04/13 15:27:22 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12174981
20/04/13 15:27:22 WARN hdfs.DataStreamer: Caught exception
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:973)
	at org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:624)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:801)
20/04/13 15:27:22 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152706_0027_m_000002_120' to hdfs://178.62.208.209:9000/data/df_4-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152706_0027_m_000002
20/04/13 15:27:22 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152706_0027_m_000002_120: Committed
20/04/13 15:27:22 INFO executor.Executor: Finished task 2.0 in stage 27.0 (TID 120). 3198 bytes result sent to driver
20/04/13 15:27:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 125
20/04/13 15:27:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 128
20/04/13 15:27:26 INFO executor.Executor: Running task 0.0 in stage 29.0 (TID 125)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 47
20/04/13 15:27:26 INFO executor.Executor: Running task 3.0 in stage 29.0 (TID 128)
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1433.0 MB)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 47 took 7 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 15.0 KB, free 1433.0 MB)
20/04/13 15:27:26 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:27:26 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 46
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1433.0 MB)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 46 took 6 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 536.2 KB, free 1432.4 MB)
20/04/13 15:27:26 INFO executor.Executor: Finished task 3.0 in stage 29.0 (TID 128). 1554 bytes result sent to driver
20/04/13 15:27:26 INFO executor.Executor: Finished task 0.0 in stage 29.0 (TID 125). 1554 bytes result sent to driver
20/04/13 15:27:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 132
20/04/13 15:27:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 135
20/04/13 15:27:26 INFO executor.Executor: Running task 4.0 in stage 30.0 (TID 135)
20/04/13 15:27:26 INFO executor.Executor: Running task 1.0 in stage 30.0 (TID 132)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 49
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1432.4 MB)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 49 took 7 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 27.4 KB, free 1432.4 MB)
20/04/13 15:27:26 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 48
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1432.4 MB)
20/04/13 15:27:26 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 48 took 25 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 536.2 KB, free 1431.8 MB)
20/04/13 15:27:26 INFO codegen.CodeGenerator: Code generated in 97.940345 ms
20/04/13 15:27:50 INFO python.PythonUDFRunner: Times: total = 4496, boot = -15656, init = 15751, finish = 4401
20/04/13 15:27:50 INFO python.PythonUDFRunner: Times: total = 832, boot = -19699, init = 19807, finish = 724
20/04/13 15:27:54 INFO executor.Executor: Finished task 4.0 in stage 30.0 (TID 135). 2421 bytes result sent to driver
20/04/13 15:27:55 INFO executor.Executor: Finished task 1.0 in stage 30.0 (TID 132). 2421 bytes result sent to driver
20/04/13 15:27:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 137
20/04/13 15:27:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 140
20/04/13 15:27:55 INFO executor.Executor: Running task 0.0 in stage 31.0 (TID 137)
20/04/13 15:27:55 INFO executor.Executor: Running task 3.0 in stage 31.0 (TID 140)
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache
20/04/13 15:27:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 50
20/04/13 15:27:55 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 1977.0 B, free 1431.8 MB)
20/04/13 15:27:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 50 took 11 ms
20/04/13 15:27:55 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 3.3 KB, free 1431.8 MB)
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:27:55 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:27:55 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 2 ms
20/04/13 15:27:55 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:27:55 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 6 ms
20/04/13 15:27:58 INFO memory.MemoryStore: Block rdd_141_3 stored as values in memory (estimated size 36.0 MB, free 1395.9 MB)
20/04/13 15:27:58 INFO memory.MemoryStore: Block rdd_141_0 stored as values in memory (estimated size 35.9 MB, free 1359.9 MB)
20/04/13 15:27:58 INFO executor.Executor: Finished task 3.0 in stage 31.0 (TID 140). 1219 bytes result sent to driver
20/04/13 15:27:58 INFO executor.Executor: Finished task 0.0 in stage 31.0 (TID 137). 1219 bytes result sent to driver
20/04/13 15:27:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 144
20/04/13 15:27:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 147
20/04/13 15:27:58 INFO executor.Executor: Running task 0.0 in stage 33.0 (TID 144)
20/04/13 15:27:58 INFO executor.Executor: Running task 3.0 in stage 33.0 (TID 147)
20/04/13 15:27:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 51
20/04/13 15:27:58 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1359.9 MB)
20/04/13 15:27:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 51 took 8 ms
20/04/13 15:27:58 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 4.2 KB, free 1359.9 MB)
20/04/13 15:27:58 INFO storage.BlockManager: Found block rdd_141_3 locally
20/04/13 15:27:58 INFO storage.BlockManager: Found block rdd_141_0 locally
20/04/13 15:27:59 INFO memory.MemoryStore: Block taskresult_144 stored as bytes in memory (estimated size 5.8 MB, free 1354.1 MB)
20/04/13 15:27:59 INFO executor.Executor: Finished task 0.0 in stage 33.0 (TID 144). 6067753 bytes result sent via BlockManager)
20/04/13 15:27:59 INFO memory.MemoryStore: Block taskresult_147 stored as bytes in memory (estimated size 5.8 MB, free 1348.3 MB)
20/04/13 15:27:59 INFO executor.Executor: Finished task 3.0 in stage 33.0 (TID 147). 6068046 bytes result sent via BlockManager)
20/04/13 15:28:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 150
20/04/13 15:28:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 153
20/04/13 15:28:04 INFO executor.Executor: Running task 1.0 in stage 34.0 (TID 150)
20/04/13 15:28:04 INFO executor.Executor: Running task 4.0 in stage 34.0 (TID 153)
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 54
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 2.2 MB, free 1362.4 MB)
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 54 took 23 ms
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 3.3 MB, free 1359.1 MB)
20/04/13 15:28:04 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 53
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1359.1 MB)
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 53 took 18 ms
20/04/13 15:28:04 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 536.2 KB, free 1358.5 MB)
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 52
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 3.2 MB, free 1291.3 MB)
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 52 took 45 ms
20/04/13 15:28:06 INFO memory.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 31.7 MB, free 1259.6 MB)
20/04/13 15:28:06 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:06 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:28:06 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:06 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:28:06 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 262144
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:28:06 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 262144
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:28:21 INFO python.PythonUDFRunner: Times: total = 4524, boot = -33463, init = 33585, finish = 4402
20/04/13 15:28:21 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9419357
20/04/13 15:28:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152804_0034_m_000004_153' to hdfs://178.62.208.209:9000/data/df_5-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152804_0034_m_000004
20/04/13 15:28:21 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152804_0034_m_000004_153: Committed
20/04/13 15:28:21 INFO executor.Executor: Finished task 4.0 in stage 34.0 (TID 153). 3198 bytes result sent to driver
20/04/13 15:28:21 INFO python.PythonUDFRunner: Times: total = 4529, boot = -37168, init = 37263, finish = 4434
20/04/13 15:28:21 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8554045
20/04/13 15:28:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152804_0034_m_000001_150' to hdfs://178.62.208.209:9000/data/df_5-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152804_0034_m_000001
20/04/13 15:28:21 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152804_0034_m_000001_150: Committed
20/04/13 15:28:21 INFO executor.Executor: Finished task 1.0 in stage 34.0 (TID 150). 3198 bytes result sent to driver
20/04/13 15:28:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 157
20/04/13 15:28:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 160
20/04/13 15:28:23 INFO executor.Executor: Running task 1.0 in stage 36.0 (TID 157)
20/04/13 15:28:23 INFO executor.Executor: Running task 4.0 in stage 36.0 (TID 160)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 58
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1323.6 MB)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 58 took 6 ms
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 15.0 KB, free 1323.6 MB)
20/04/13 15:28:23 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:28:23 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 57
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1323.5 MB)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 57 took 6 ms
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 536.2 KB, free 1323.0 MB)
20/04/13 15:28:23 INFO executor.Executor: Finished task 1.0 in stage 36.0 (TID 157). 1554 bytes result sent to driver
20/04/13 15:28:23 INFO executor.Executor: Finished task 4.0 in stage 36.0 (TID 160). 1554 bytes result sent to driver
20/04/13 15:28:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 162
20/04/13 15:28:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 165
20/04/13 15:28:23 INFO executor.Executor: Running task 0.0 in stage 37.0 (TID 162)
20/04/13 15:28:23 INFO executor.Executor: Running task 3.0 in stage 37.0 (TID 165)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 60
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1323.0 MB)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 60 took 8 ms
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 27.4 KB, free 1323.0 MB)
20/04/13 15:28:23 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 59
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1322.9 MB)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 59 took 12 ms
20/04/13 15:28:23 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 536.2 KB, free 1322.4 MB)
20/04/13 15:28:23 INFO codegen.CodeGenerator: Code generated in 113.787065 ms
20/04/13 15:28:43 INFO python.PythonUDFRunner: Times: total = 743, boot = -14397, init = 14485, finish = 655
20/04/13 15:28:44 INFO python.PythonUDFRunner: Times: total = 3124, boot = -14396, init = 14505, finish = 3015
20/04/13 15:28:47 INFO executor.Executor: Finished task 3.0 in stage 37.0 (TID 165). 2421 bytes result sent to driver
20/04/13 15:28:49 INFO executor.Executor: Finished task 0.0 in stage 37.0 (TID 162). 2421 bytes result sent to driver
20/04/13 15:28:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 169
20/04/13 15:28:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 172
20/04/13 15:28:49 INFO executor.Executor: Running task 1.0 in stage 38.0 (TID 169)
20/04/13 15:28:49 INFO executor.Executor: Running task 4.0 in stage 38.0 (TID 172)
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache
20/04/13 15:28:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 61
20/04/13 15:28:49 INFO memory.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 1975.0 B, free 1322.4 MB)
20/04/13 15:28:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 61 took 10 ms
20/04/13 15:28:49 INFO memory.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 3.3 KB, free 1322.4 MB)
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:28:49 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:28:49 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 15:28:49 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:28:49 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 14 ms
20/04/13 15:28:54 INFO memory.MemoryStore: Block rdd_171_1 stored as values in memory (estimated size 36.6 MB, free 1196.7 MB)
20/04/13 15:28:54 INFO executor.Executor: Finished task 1.0 in stage 38.0 (TID 169). 1219 bytes result sent to driver
20/04/13 15:28:54 INFO memory.MemoryStore: Block rdd_171_4 stored as values in memory (estimated size 36.1 MB, free 1249.7 MB)
20/04/13 15:28:54 INFO executor.Executor: Finished task 4.0 in stage 38.0 (TID 172). 1219 bytes result sent to driver
20/04/13 15:28:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 174
20/04/13 15:28:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 177
20/04/13 15:28:54 INFO executor.Executor: Running task 1.0 in stage 40.0 (TID 174)
20/04/13 15:28:54 INFO executor.Executor: Running task 4.0 in stage 40.0 (TID 177)
20/04/13 15:28:54 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 62
20/04/13 15:28:54 INFO memory.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1249.7 MB)
20/04/13 15:28:54 INFO broadcast.TorrentBroadcast: Reading broadcast variable 62 took 10 ms
20/04/13 15:28:54 INFO memory.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 4.2 KB, free 1249.7 MB)
20/04/13 15:28:54 INFO storage.BlockManager: Found block rdd_171_1 locally
20/04/13 15:28:54 INFO storage.BlockManager: Found block rdd_171_4 locally
20/04/13 15:28:54 INFO memory.MemoryStore: Block taskresult_177 stored as bytes in memory (estimated size 5.8 MB, free 1245.0 MB)
20/04/13 15:28:54 INFO executor.Executor: Finished task 4.0 in stage 40.0 (TID 177). 6067645 bytes result sent via BlockManager)
20/04/13 15:28:54 INFO memory.MemoryStore: Block taskresult_174 stored as bytes in memory (estimated size 5.8 MB, free 1239.3 MB)
20/04/13 15:28:54 INFO executor.Executor: Finished task 1.0 in stage 40.0 (TID 174). 6068082 bytes result sent via BlockManager)
20/04/13 15:28:56 INFO storage.BlockManager: Removing RDD 141
20/04/13 15:28:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 182
20/04/13 15:28:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 185
20/04/13 15:28:59 INFO executor.Executor: Running task 2.0 in stage 41.0 (TID 182)
20/04/13 15:28:59 INFO executor.Executor: Running task 5.0 in stage 41.0 (TID 185)
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 65
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 2.2 MB, free 1383.2 MB)
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 65 took 16 ms
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_65 stored as values in memory (estimated size 3.3 MB, free 1379.9 MB)
20/04/13 15:28:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 64
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1379.9 MB)
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 64 took 21 ms
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_64 stored as values in memory (estimated size 536.2 KB, free 1379.4 MB)
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 63
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 3.3 MB, free 1312.1 MB)
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 63 took 27 ms
20/04/13 15:29:01 INFO memory.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 31.7 MB, free 1280.4 MB)
20/04/13 15:29:01 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:29:01 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:29:01 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:29:01 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 262144
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:29:01 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:29:01 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:29:01 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 262144
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:29:06 INFO python.PythonUDFRunner: Times: total = 287, boot = -32852, init = 32926, finish = 213
20/04/13 15:29:06 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 3709841
20/04/13 15:29:06 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152859_0041_m_000005_185' to hdfs://178.62.208.209:9000/data/df_5-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152859_0041_m_000005
20/04/13 15:29:06 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152859_0041_m_000005_185: Committed
20/04/13 15:29:06 INFO executor.Executor: Finished task 5.0 in stage 41.0 (TID 185). 3198 bytes result sent to driver
20/04/13 15:29:13 INFO python.PythonUDFRunner: Times: total = 761, boot = -35197, init = 35350, finish = 608
20/04/13 15:29:13 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9313671
20/04/13 15:29:13 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152859_0041_m_000002_182' to hdfs://178.62.208.209:9000/data/df_5-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152859_0041_m_000002
20/04/13 15:29:13 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152859_0041_m_000002_182: Committed
20/04/13 15:29:13 INFO executor.Executor: Finished task 2.0 in stage 41.0 (TID 182). 3241 bytes result sent to driver
20/04/13 15:29:15 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
20/04/13 15:29:15 INFO memory.MemoryStore: MemoryStore cleared
20/04/13 15:29:15 INFO storage.BlockManager: BlockManager stopped
20/04/13 15:29:15 INFO util.ShutdownHookManager: Shutdown hook called
20/04/13 15:29:15 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0007/spark-48e18c7d-0251-48fc-99f4-d9e7fb44204e
End of LogType:stderr

LogType:stdout
Log Upload Time:Mon Apr 13 15:29:16 +0000 2020
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_1586780871303_0007_01_000004 on 178.62.200.211_37461
===========================================================================
LogType:stderr
Log Upload Time:Mon Apr 13 15:29:16 +0000 2020
LogLength:117873
Log Contents:
20/04/13 15:24:40 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 22288@178.62.200.211
20/04/13 15:24:40 INFO util.SignalUtils: Registered signal handler for TERM
20/04/13 15:24:40 INFO util.SignalUtils: Registered signal handler for HUP
20/04/13 15:24:40 INFO util.SignalUtils: Registered signal handler for INT
20/04/13 15:24:42 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 15:24:42 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 15:24:42 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 15:24:42 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 15:24:42 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 15:24:42 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:39481 after 76 ms (0 ms spent in bootstraps)
20/04/13 15:24:42 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 15:24:42 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 15:24:42 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 15:24:42 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 15:24:42 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 15:24:42 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:39481 after 28 ms (0 ms spent in bootstraps)
20/04/13 15:24:43 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0007/blockmgr-76ecd525-9a0f-4a1d-b3b4-d385d4871e46
20/04/13 15:24:43 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
20/04/13 15:24:43 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.210.13:39481
20/04/13 15:24:43 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/13 15:24:43 INFO executor.Executor: Starting executor ID 3 on host 178.62.200.211
20/04/13 15:24:43 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35537.
20/04/13 15:24:43 INFO netty.NettyBlockTransferService: Server created on 178.62.200.211:35537
20/04/13 15:24:43 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/13 15:24:43 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(3, 178.62.200.211, 35537, None)
20/04/13 15:24:43 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(3, 178.62.200.211, 35537, None)
20/04/13 15:24:43 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(3, 178.62.200.211, 35537, None)
20/04/13 15:24:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2
20/04/13 15:24:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5
20/04/13 15:24:49 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 2)
20/04/13 15:24:49 INFO executor.Executor: Running task 4.0 in stage 1.0 (TID 5)
20/04/13 15:24:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3
20/04/13 15:24:49 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:44585 after 10 ms (0 ms spent in bootstraps)
20/04/13 15:24:49 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1458.6 MB)
20/04/13 15:24:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 116 ms
20/04/13 15:24:50 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.0 KB, free 1458.6 MB)
20/04/13 15:24:51 INFO codegen.CodeGenerator: Code generated in 622.6387 ms
20/04/13 15:24:53 INFO codegen.CodeGenerator: Code generated in 45.359279 ms
20/04/13 15:24:53 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:24:53 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:24:53 INFO codegen.CodeGenerator: Code generated in 16.522309 ms
20/04/13 15:24:53 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
20/04/13 15:24:53 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:36375 after 7 ms (0 ms spent in bootstraps)
20/04/13 15:24:53 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.5 MB)
20/04/13 15:24:53 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 45 ms
20/04/13 15:24:53 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 536.2 KB, free 1458.0 MB)
20/04/13 15:24:55 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 2). 1640 bytes result sent to driver
20/04/13 15:24:55 INFO executor.Executor: Finished task 4.0 in stage 1.0 (TID 5). 1597 bytes result sent to driver
20/04/13 15:24:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7
20/04/13 15:24:56 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 7)
20/04/13 15:24:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10
20/04/13 15:24:56 INFO executor.Executor: Running task 3.0 in stage 2.0 (TID 10)
20/04/13 15:24:56 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
20/04/13 15:24:56 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1458.0 MB)
20/04/13 15:24:56 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 15 ms
20/04/13 15:24:56 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 27.4 KB, free 1458.0 MB)
20/04/13 15:24:56 INFO codegen.CodeGenerator: Code generated in 35.358936 ms
20/04/13 15:24:57 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:24:57 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:24:57 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
20/04/13 15:24:57 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.0 MB)
20/04/13 15:24:57 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 38 ms
20/04/13 15:24:57 INFO codegen.CodeGenerator: Code generated in 47.097471 ms
20/04/13 15:24:57 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 536.2 KB, free 1457.4 MB)
20/04/13 15:24:57 INFO codegen.CodeGenerator: Code generated in 62.005524 ms
20/04/13 15:24:57 INFO codegen.CodeGenerator: Code generated in 108.801071 ms
20/04/13 15:25:12 INFO python.PythonUDFRunner: Times: total = 6939, boot = 423, init = 333, finish = 6183
20/04/13 15:25:12 INFO executor.Executor: Finished task 3.0 in stage 2.0 (TID 10). 2464 bytes result sent to driver
20/04/13 15:25:13 INFO python.PythonUDFRunner: Times: total = 5256, boot = 419, init = 342, finish = 4495
20/04/13 15:25:13 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 7). 2421 bytes result sent to driver
20/04/13 15:25:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15
20/04/13 15:25:13 INFO executor.Executor: Running task 2.0 in stage 3.0 (TID 15)
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/13 15:25:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/13 15:25:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18
20/04/13 15:25:13 INFO executor.Executor: Running task 5.0 in stage 3.0 (TID 18)
20/04/13 15:25:13 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1976.0 B, free 1457.4 MB)
20/04/13 15:25:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 23 ms
20/04/13 15:25:13 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 1457.4 MB)
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:25:13 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:25:13 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:25:13 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:36927 after 3 ms (0 ms spent in bootstraps)
20/04/13 15:25:13 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 24 ms
20/04/13 15:25:13 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 22 ms
20/04/13 15:25:14 INFO memory.MemoryStore: Block rdd_21_5 stored as values in memory (estimated size 147.2 KB, free 1457.3 MB)
20/04/13 15:25:14 INFO memory.MemoryStore: Block rdd_21_2 stored as values in memory (estimated size 146.7 KB, free 1457.1 MB)
20/04/13 15:25:14 INFO executor.Executor: Finished task 5.0 in stage 3.0 (TID 18). 1176 bytes result sent to driver
20/04/13 15:25:14 INFO executor.Executor: Finished task 2.0 in stage 3.0 (TID 15). 1176 bytes result sent to driver
20/04/13 15:25:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19
20/04/13 15:25:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22
20/04/13 15:25:14 INFO executor.Executor: Running task 5.0 in stage 5.0 (TID 22)
20/04/13 15:25:14 INFO executor.Executor: Running task 2.0 in stage 5.0 (TID 19)
20/04/13 15:25:14 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/13 15:25:14 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1457.1 MB)
20/04/13 15:25:14 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 20 ms
20/04/13 15:25:14 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.2 KB, free 1457.1 MB)
20/04/13 15:25:14 INFO storage.BlockManager: Found block rdd_21_5 locally
20/04/13 15:25:14 INFO storage.BlockManager: Found block rdd_21_2 locally
20/04/13 15:25:14 INFO executor.Executor: Finished task 5.0 in stage 5.0 (TID 22). 41280 bytes result sent to driver
20/04/13 15:25:14 INFO executor.Executor: Finished task 2.0 in stage 5.0 (TID 19). 40807 bytes result sent to driver
20/04/13 15:25:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 27
20/04/13 15:25:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 30
20/04/13 15:25:15 INFO executor.Executor: Running task 2.0 in stage 6.0 (TID 27)
20/04/13 15:25:15 INFO executor.Executor: Running task 5.0 in stage 6.0 (TID 30)
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 124.8 KB, free 1457.0 MB)
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 25 ms
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 347.5 KB, free 1456.7 MB)
20/04/13 15:25:15 INFO codegen.CodeGenerator: Code generated in 20.352732 ms
20/04/13 15:25:15 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:25:15 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:25:15 INFO codegen.CodeGenerator: Code generated in 33.51025 ms
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9
20/04/13 15:25:15 INFO codegen.CodeGenerator: Code generated in 42.455533 ms
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.6 MB)
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 60 ms
20/04/13 15:25:15 INFO codegen.CodeGenerator: Code generated in 60.874322 ms
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 536.2 KB, free 1456.1 MB)
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 77.2 KB, free 1392.0 MB)
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 64 ms
20/04/13 15:25:16 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 987.3 KB, free 1391.1 MB)
20/04/13 15:25:16 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:16 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:16 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:16 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:25:16 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:25:16 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:25:16 INFO compress.CodecPool: Got brand-new compressor [.snappy]
20/04/13 15:25:16 INFO compress.CodecPool: Got brand-new compressor [.snappy]
20/04/13 15:25:25 INFO python.PythonUDFRunner: Times: total = 755, boot = -13542, init = 13818, finish = 479
20/04/13 15:25:25 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 3218647
20/04/13 15:25:25 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152515_0006_m_000005_30' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152515_0006_m_000005
20/04/13 15:25:25 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152515_0006_m_000005_30: Committed
20/04/13 15:25:25 INFO executor.Executor: Finished task 5.0 in stage 6.0 (TID 30). 3241 bytes result sent to driver
20/04/13 15:25:33 INFO python.PythonUDFRunner: Times: total = 6307, boot = -11829, init = 12118, finish = 6018
20/04/13 15:25:33 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8454266
20/04/13 15:25:33 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152515_0006_m_000002_27' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152515_0006_m_000002
20/04/13 15:25:33 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152515_0006_m_000002_27: Committed
20/04/13 15:25:33 INFO executor.Executor: Finished task 2.0 in stage 6.0 (TID 27). 3198 bytes result sent to driver
20/04/13 15:25:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 31
20/04/13 15:25:34 INFO executor.Executor: Running task 0.0 in stage 7.0 (TID 31)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1455.1 MB)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 8 ms
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 8.9 KB, free 1455.1 MB)
20/04/13 15:25:34 INFO codegen.CodeGenerator: Code generated in 13.281235 ms
20/04/13 15:25:34 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1455.0 MB)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 8 ms
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 536.2 KB, free 1454.5 MB)
20/04/13 15:25:34 INFO executor.Executor: Finished task 0.0 in stage 7.0 (TID 31). 1332 bytes result sent to driver
20/04/13 15:25:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 34
20/04/13 15:25:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 37
20/04/13 15:25:34 INFO executor.Executor: Running task 2.0 in stage 8.0 (TID 34)
20/04/13 15:25:34 INFO executor.Executor: Running task 5.0 in stage 8.0 (TID 37)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1455.5 MB)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 14 ms
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 15.0 KB, free 1455.5 MB)
20/04/13 15:25:34 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13
20/04/13 15:25:34 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1455.5 MB)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 12 ms
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 536.2 KB, free 1455.0 MB)
20/04/13 15:25:34 INFO executor.Executor: Finished task 5.0 in stage 8.0 (TID 37). 1554 bytes result sent to driver
20/04/13 15:25:34 INFO executor.Executor: Finished task 2.0 in stage 8.0 (TID 34). 1554 bytes result sent to driver
20/04/13 15:25:34 INFO storage.BlockManager: Removing RDD 21
20/04/13 15:25:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 38
20/04/13 15:25:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 41
20/04/13 15:25:34 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 38)
20/04/13 15:25:34 INFO executor.Executor: Running task 3.0 in stage 9.0 (TID 41)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1456.4 MB)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 18 ms
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 27.4 KB, free 1456.4 MB)
20/04/13 15:25:34 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15
20/04/13 15:25:34 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:25:35 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.4 MB)
20/04/13 15:25:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 35 ms
20/04/13 15:25:35 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 536.2 KB, free 1455.8 MB)
20/04/13 15:25:35 INFO codegen.CodeGenerator: Code generated in 107.429163 ms
20/04/13 15:25:48 INFO python.PythonUDFRunner: Times: total = 3250, boot = -13080, init = 13229, finish = 3101
20/04/13 15:25:48 INFO executor.Executor: Finished task 3.0 in stage 9.0 (TID 41). 2421 bytes result sent to driver
20/04/13 15:25:48 INFO python.PythonUDFRunner: Times: total = 3173, boot = -18632, init = 18744, finish = 3061
20/04/13 15:25:48 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 38). 2421 bytes result sent to driver
20/04/13 15:25:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 44
20/04/13 15:25:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 47
20/04/13 15:25:48 INFO executor.Executor: Running task 3.0 in stage 10.0 (TID 47)
20/04/13 15:25:48 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
20/04/13 15:25:48 INFO executor.Executor: Running task 0.0 in stage 10.0 (TID 44)
20/04/13 15:25:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17
20/04/13 15:25:48 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 1976.0 B, free 1455.8 MB)
20/04/13 15:25:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 13 ms
20/04/13 15:25:48 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 3.3 KB, free 1455.8 MB)
20/04/13 15:25:49 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
20/04/13 15:25:49 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:25:49 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
20/04/13 15:25:49 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:25:49 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:25:49 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:25:49 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 0 ms
20/04/13 15:25:49 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 2 ms
20/04/13 15:25:49 INFO memory.MemoryStore: Block rdd_51_3 stored as values in memory (estimated size 146.5 KB, free 1455.7 MB)
20/04/13 15:25:49 INFO memory.MemoryStore: Block rdd_51_0 stored as values in memory (estimated size 147.7 KB, free 1455.5 MB)
20/04/13 15:25:49 INFO executor.Executor: Finished task 3.0 in stage 10.0 (TID 47). 1176 bytes result sent to driver
20/04/13 15:25:49 INFO executor.Executor: Finished task 0.0 in stage 10.0 (TID 44). 1176 bytes result sent to driver
20/04/13 15:25:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 52
20/04/13 15:25:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 55
20/04/13 15:25:49 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 52)
20/04/13 15:25:49 INFO executor.Executor: Running task 3.0 in stage 12.0 (TID 55)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1455.5 MB)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 13 ms
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 4.2 KB, free 1455.5 MB)
20/04/13 15:25:49 INFO storage.BlockManager: Found block rdd_51_3 locally
20/04/13 15:25:49 INFO executor.Executor: Finished task 3.0 in stage 12.0 (TID 55). 41008 bytes result sent to driver
20/04/13 15:25:49 INFO storage.BlockManager: Found block rdd_51_0 locally
20/04/13 15:25:49 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 52). 40880 bytes result sent to driver
20/04/13 15:25:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 56
20/04/13 15:25:49 INFO executor.Executor: Running task 0.0 in stage 13.0 (TID 56)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21
20/04/13 15:25:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 59
20/04/13 15:25:49 INFO executor.Executor: Running task 3.0 in stage 13.0 (TID 59)
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 124.7 KB, free 1455.4 MB)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 14 ms
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 347.5 KB, free 1455.1 MB)
20/04/13 15:25:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:49 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:25:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:25:49 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1455.0 MB)
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 29 ms
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 536.2 KB, free 1454.5 MB)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 77.2 KB, free 1390.4 MB)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 29 ms
20/04/13 15:25:50 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 987.3 KB, free 1389.5 MB)
20/04/13 15:25:50 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:50 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:50 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:25:50 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:25:50 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:25:50 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:25:50 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:25:50 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:25:50 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:25:50 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:25:50 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:25:50 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:25:50 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:26:06 INFO python.PythonUDFRunner: Times: total = 961, boot = -11622, init = 11693, finish = 890
20/04/13 15:26:06 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9364404
20/04/13 15:26:06 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152549_0013_m_000003_59' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152549_0013_m_000003
20/04/13 15:26:06 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152549_0013_m_000003_59: Committed
20/04/13 15:26:06 INFO executor.Executor: Finished task 3.0 in stage 13.0 (TID 59). 3241 bytes result sent to driver
20/04/13 15:26:07 INFO python.PythonUDFRunner: Times: total = 3487, boot = -11531, init = 11625, finish = 3393
20/04/13 15:26:07 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8862836
20/04/13 15:26:07 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152549_0013_m_000000_56' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152549_0013_m_000000
20/04/13 15:26:07 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152549_0013_m_000000_56: Committed
20/04/13 15:26:07 INFO executor.Executor: Finished task 0.0 in stage 13.0 (TID 56). 3198 bytes result sent to driver
20/04/13 15:26:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 64
20/04/13 15:26:07 INFO executor.Executor: Running task 1.0 in stage 15.0 (TID 64)
20/04/13 15:26:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 67
20/04/13 15:26:07 INFO executor.Executor: Running task 4.0 in stage 15.0 (TID 67)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1453.5 MB)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 13 ms
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.0 KB, free 1453.5 MB)
20/04/13 15:26:07 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24
20/04/13 15:26:07 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1453.4 MB)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 15 ms
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 536.2 KB, free 1452.9 MB)
20/04/13 15:26:07 INFO executor.Executor: Finished task 1.0 in stage 15.0 (TID 64). 1554 bytes result sent to driver
20/04/13 15:26:07 INFO executor.Executor: Finished task 4.0 in stage 15.0 (TID 67). 1597 bytes result sent to driver
20/04/13 15:26:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 70
20/04/13 15:26:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 73
20/04/13 15:26:07 INFO executor.Executor: Running task 1.0 in stage 16.0 (TID 70)
20/04/13 15:26:07 INFO executor.Executor: Running task 4.0 in stage 16.0 (TID 73)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1452.9 MB)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 15 ms
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 27.4 KB, free 1452.9 MB)
20/04/13 15:26:07 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26
20/04/13 15:26:08 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1452.8 MB)
20/04/13 15:26:08 INFO broadcast.TorrentBroadcast: Reading broadcast variable 26 took 21 ms
20/04/13 15:26:08 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:26:08 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 536.2 KB, free 1452.3 MB)
20/04/13 15:26:08 INFO codegen.CodeGenerator: Code generated in 107.296834 ms
20/04/13 15:26:23 INFO python.PythonUDFRunner: Times: total = 3856, boot = -14736, init = 14799, finish = 3793
20/04/13 15:26:23 INFO executor.Executor: Finished task 4.0 in stage 16.0 (TID 73). 2421 bytes result sent to driver
20/04/13 15:26:23 INFO python.PythonUDFRunner: Times: total = 3700, boot = -17232, init = 17317, finish = 3615
20/04/13 15:26:24 INFO executor.Executor: Finished task 1.0 in stage 16.0 (TID 70). 2421 bytes result sent to driver
20/04/13 15:26:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 76
20/04/13 15:26:24 INFO executor.Executor: Running task 1.0 in stage 17.0 (TID 76)
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache
20/04/13 15:26:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 28
20/04/13 15:26:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 79
20/04/13 15:26:24 INFO executor.Executor: Running task 4.0 in stage 17.0 (TID 79)
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 1977.0 B, free 1452.3 MB)
20/04/13 15:26:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 28 took 13 ms
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 3.3 KB, free 1452.3 MB)
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:26:24 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:26:24 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 2 ms
20/04/13 15:26:24 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:26:24 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 4 ms
20/04/13 15:26:24 INFO memory.MemoryStore: Block rdd_81_1 stored as values in memory (estimated size 2.6 MB, free 1449.7 MB)
20/04/13 15:26:24 INFO memory.MemoryStore: Block rdd_81_4 stored as values in memory (estimated size 2.6 MB, free 1447.1 MB)
20/04/13 15:26:24 INFO executor.Executor: Finished task 1.0 in stage 17.0 (TID 76). 1176 bytes result sent to driver
20/04/13 15:26:24 INFO executor.Executor: Finished task 4.0 in stage 17.0 (TID 79). 1176 bytes result sent to driver
20/04/13 15:26:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 83
20/04/13 15:26:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 86
20/04/13 15:26:24 INFO executor.Executor: Running task 4.0 in stage 19.0 (TID 86)
20/04/13 15:26:24 INFO executor.Executor: Running task 1.0 in stage 19.0 (TID 83)
20/04/13 15:26:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 29
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1447.1 MB)
20/04/13 15:26:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 29 took 9 ms
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 4.2 KB, free 1447.1 MB)
20/04/13 15:26:24 INFO storage.BlockManager: Found block rdd_81_4 locally
20/04/13 15:26:24 INFO storage.BlockManager: Found block rdd_81_1 locally
20/04/13 15:26:24 INFO executor.Executor: Finished task 1.0 in stage 19.0 (TID 83). 677902 bytes result sent to driver
20/04/13 15:26:25 INFO executor.Executor: Finished task 4.0 in stage 19.0 (TID 86). 677387 bytes result sent to driver
20/04/13 15:26:25 INFO storage.BlockManager: Removing RDD 51
20/04/13 15:26:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 89
20/04/13 15:26:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 92
20/04/13 15:26:26 INFO executor.Executor: Running task 2.0 in stage 20.0 (TID 89)
20/04/13 15:26:26 INFO executor.Executor: Running task 5.0 in stage 20.0 (TID 92)
20/04/13 15:26:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 32
20/04/13 15:26:26 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 1062.5 KB, free 1450.2 MB)
20/04/13 15:26:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 32 took 14 ms
20/04/13 15:26:26 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 1883.3 KB, free 1448.3 MB)
20/04/13 15:26:27 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:26:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 31
20/04/13 15:26:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:27 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:27 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:27 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1448.3 MB)
20/04/13 15:26:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 31 took 20 ms
20/04/13 15:26:27 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 536.2 KB, free 1447.8 MB)
20/04/13 15:26:27 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:26:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:27 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:27 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 30
20/04/13 15:26:27 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 1770.3 KB, free 1414.1 MB)
20/04/13 15:26:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 30 took 34 ms
20/04/13 15:26:28 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 18.8 MB, free 1363.2 MB)
20/04/13 15:26:28 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:26:28 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:26:28 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:26:28 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:26:28 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 164062
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:26:28 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 164062
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:26:35 INFO python.PythonUDFRunner: Times: total = 379, boot = -15468, init = 15563, finish = 284
20/04/13 15:26:35 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 4558858
20/04/13 15:26:36 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152626_0020_m_000005_92' to hdfs://178.62.208.209:9000/data/df_4-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152626_0020_m_000005
20/04/13 15:26:36 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152626_0020_m_000005_92: Committed
20/04/13 15:26:36 INFO executor.Executor: Finished task 5.0 in stage 20.0 (TID 92). 3198 bytes result sent to driver
20/04/13 15:26:45 INFO python.PythonUDFRunner: Times: total = 4969, boot = -15258, init = 15336, finish = 4891
20/04/13 15:26:45 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 11951906
20/04/13 15:26:46 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152626_0020_m_000002_89' to hdfs://178.62.208.209:9000/data/df_4-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152626_0020_m_000002
20/04/13 15:26:46 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152626_0020_m_000002_89: Committed
20/04/13 15:26:46 INFO executor.Executor: Finished task 2.0 in stage 20.0 (TID 89). 3198 bytes result sent to driver
20/04/13 15:26:46 INFO storage.BlockManager: Removing RDD 81
20/04/13 15:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 96
20/04/13 15:26:46 INFO executor.Executor: Running task 2.0 in stage 22.0 (TID 96)
20/04/13 15:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 99
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 36
20/04/13 15:26:46 INFO executor.Executor: Running task 5.0 in stage 22.0 (TID 99)
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1436.4 MB)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 36 took 8 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 15.0 KB, free 1436.4 MB)
20/04/13 15:26:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 35
20/04/13 15:26:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1436.4 MB)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 35 took 8 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 536.2 KB, free 1435.9 MB)
20/04/13 15:26:46 INFO executor.Executor: Finished task 5.0 in stage 22.0 (TID 99). 1554 bytes result sent to driver
20/04/13 15:26:46 INFO executor.Executor: Finished task 2.0 in stage 22.0 (TID 96). 1597 bytes result sent to driver
20/04/13 15:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 100
20/04/13 15:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 103
20/04/13 15:26:46 INFO executor.Executor: Running task 0.0 in stage 23.0 (TID 100)
20/04/13 15:26:46 INFO executor.Executor: Running task 3.0 in stage 23.0 (TID 103)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 38
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1435.9 MB)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 38 took 8 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 27.4 KB, free 1435.8 MB)
20/04/13 15:26:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 37
20/04/13 15:26:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1435.8 MB)
20/04/13 15:26:46 INFO codegen.CodeGenerator: Code generated in 37.570746 ms
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 37 took 45 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 536.2 KB, free 1435.3 MB)
20/04/13 15:27:02 INFO python.PythonUDFRunner: Times: total = 824, boot = -19281, init = 19379, finish = 726
20/04/13 15:27:03 INFO executor.Executor: Finished task 3.0 in stage 23.0 (TID 103). 2421 bytes result sent to driver
20/04/13 15:27:03 INFO python.PythonUDFRunner: Times: total = 3307, boot = -14779, init = 14853, finish = 3233
20/04/13 15:27:03 INFO executor.Executor: Finished task 0.0 in stage 23.0 (TID 100). 2421 bytes result sent to driver
20/04/13 15:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 106
20/04/13 15:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 109
20/04/13 15:27:03 INFO executor.Executor: Running task 3.0 in stage 24.0 (TID 109)
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache
20/04/13 15:27:03 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 39
20/04/13 15:27:03 INFO executor.Executor: Running task 0.0 in stage 24.0 (TID 106)
20/04/13 15:27:03 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 1977.0 B, free 1435.3 MB)
20/04/13 15:27:03 INFO broadcast.TorrentBroadcast: Reading broadcast variable 39 took 7 ms
20/04/13 15:27:03 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 3.3 KB, free 1435.3 MB)
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:27:03 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:27:03 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 0 ms
20/04/13 15:27:03 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:27:03 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 11 ms
20/04/13 15:27:04 INFO memory.MemoryStore: Block rdd_111_3 stored as values in memory (estimated size 2.6 MB, free 1432.6 MB)
20/04/13 15:27:04 INFO executor.Executor: Finished task 3.0 in stage 24.0 (TID 109). 1176 bytes result sent to driver
20/04/13 15:27:04 INFO memory.MemoryStore: Block rdd_111_0 stored as values in memory (estimated size 2.6 MB, free 1430.0 MB)
20/04/13 15:27:04 INFO executor.Executor: Finished task 0.0 in stage 24.0 (TID 106). 1176 bytes result sent to driver
20/04/13 15:27:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 113
20/04/13 15:27:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 116
20/04/13 15:27:04 INFO executor.Executor: Running task 0.0 in stage 26.0 (TID 113)
20/04/13 15:27:04 INFO executor.Executor: Running task 3.0 in stage 26.0 (TID 116)
20/04/13 15:27:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 40
20/04/13 15:27:04 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1430.0 MB)
20/04/13 15:27:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 40 took 9 ms
20/04/13 15:27:04 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 4.2 KB, free 1430.0 MB)
20/04/13 15:27:04 INFO storage.BlockManager: Found block rdd_111_3 locally
20/04/13 15:27:04 INFO storage.BlockManager: Found block rdd_111_0 locally
20/04/13 15:27:04 INFO executor.Executor: Finished task 3.0 in stage 26.0 (TID 116). 676521 bytes result sent to driver
20/04/13 15:27:04 INFO executor.Executor: Finished task 0.0 in stage 26.0 (TID 113). 677273 bytes result sent to driver
20/04/13 15:27:06 INFO storage.BlockManager: Removing RDD 111
20/04/13 15:27:06 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 118
20/04/13 15:27:06 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 121
20/04/13 15:27:06 INFO executor.Executor: Running task 3.0 in stage 27.0 (TID 121)
20/04/13 15:27:06 INFO executor.Executor: Running task 0.0 in stage 27.0 (TID 118)
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 43
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 1062.2 KB, free 1456.0 MB)
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Reading broadcast variable 43 took 16 ms
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 1883.3 KB, free 1454.1 MB)
20/04/13 15:27:06 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 42
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:27:06 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:27:06 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:27:06 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:27:06 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:27:06 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:27:06 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:27:06 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:27:06 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:27:06 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:27:06 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:27:06 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:27:06 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1454.1 MB)
20/04/13 15:27:06 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 164062
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Reading broadcast variable 42 took 28 ms
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 536.2 KB, free 1453.6 MB)
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 41
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 1770.4 KB, free 1387.8 MB)
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Reading broadcast variable 41 took 45 ms
20/04/13 15:27:08 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 18.8 MB, free 1369.0 MB)
20/04/13 15:27:08 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:27:08 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:27:08 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:27:08 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 164062
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:27:24 INFO python.PythonUDFRunner: Times: total = 621, boot = -18909, init = 18970, finish = 560
20/04/13 15:27:24 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12397389
20/04/13 15:27:24 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152706_0027_m_000003_121' to hdfs://178.62.208.209:9000/data/df_4-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152706_0027_m_000003
20/04/13 15:27:24 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152706_0027_m_000003_121: Committed
20/04/13 15:27:24 INFO executor.Executor: Finished task 3.0 in stage 27.0 (TID 121). 3198 bytes result sent to driver
20/04/13 15:27:26 INFO python.PythonUDFRunner: Times: total = 4898, boot = -16415, init = 16462, finish = 4851
20/04/13 15:27:26 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12763788
20/04/13 15:27:26 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152706_0027_m_000000_118' to hdfs://178.62.208.209:9000/data/df_4-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152706_0027_m_000000
20/04/13 15:27:26 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152706_0027_m_000000_118: Committed
20/04/13 15:27:26 INFO executor.Executor: Finished task 0.0 in stage 27.0 (TID 118). 3198 bytes result sent to driver
20/04/13 15:27:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 124
20/04/13 15:27:26 INFO executor.Executor: Running task 0.0 in stage 28.0 (TID 124)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 45
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1433.0 MB)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 45 took 7 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 8.9 KB, free 1433.0 MB)
20/04/13 15:27:26 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 44
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1433.0 MB)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 44 took 6 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 536.2 KB, free 1432.4 MB)
20/04/13 15:27:26 INFO executor.Executor: Finished task 0.0 in stage 28.0 (TID 124). 1332 bytes result sent to driver
20/04/13 15:27:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 127
20/04/13 15:27:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 130
20/04/13 15:27:26 INFO executor.Executor: Running task 5.0 in stage 29.0 (TID 130)
20/04/13 15:27:26 INFO executor.Executor: Running task 2.0 in stage 29.0 (TID 127)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 47
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1432.4 MB)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 47 took 9 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 15.0 KB, free 1432.4 MB)
20/04/13 15:27:26 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:27:26 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 46
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1432.4 MB)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 46 took 6 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 536.2 KB, free 1431.9 MB)
20/04/13 15:27:26 INFO executor.Executor: Finished task 5.0 in stage 29.0 (TID 130). 1554 bytes result sent to driver
20/04/13 15:27:26 INFO executor.Executor: Finished task 2.0 in stage 29.0 (TID 127). 1554 bytes result sent to driver
20/04/13 15:27:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 133
20/04/13 15:27:26 INFO executor.Executor: Running task 2.0 in stage 30.0 (TID 133)
20/04/13 15:27:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 136
20/04/13 15:27:26 INFO executor.Executor: Running task 5.0 in stage 30.0 (TID 136)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 49
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1431.9 MB)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 49 took 9 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 27.4 KB, free 1431.8 MB)
20/04/13 15:27:26 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 48
20/04/13 15:27:26 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1431.8 MB)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 48 took 29 ms
20/04/13 15:27:26 INFO codegen.CodeGenerator: Code generated in 66.236018 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 536.2 KB, free 1431.3 MB)
20/04/13 15:27:36 INFO python.PythonUDFRunner: Times: total = 426, boot = -19543, init = 19686, finish = 283
20/04/13 15:27:39 INFO executor.Executor: Finished task 5.0 in stage 30.0 (TID 136). 2421 bytes result sent to driver
20/04/13 15:27:49 INFO python.PythonUDFRunner: Times: total = 3766, boot = -15270, init = 15355, finish = 3681
20/04/13 15:27:54 INFO executor.Executor: Finished task 2.0 in stage 30.0 (TID 133). 2421 bytes result sent to driver
20/04/13 15:27:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 138
20/04/13 15:27:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 141
20/04/13 15:27:55 INFO executor.Executor: Running task 4.0 in stage 31.0 (TID 141)
20/04/13 15:27:55 INFO executor.Executor: Running task 1.0 in stage 31.0 (TID 138)
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache
20/04/13 15:27:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 50
20/04/13 15:27:55 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 1977.0 B, free 1431.3 MB)
20/04/13 15:27:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 50 took 8 ms
20/04/13 15:27:55 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 3.3 KB, free 1431.3 MB)
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:27:55 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:27:55 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:27:55 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 15:27:55 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 3 ms
20/04/13 15:27:58 INFO memory.MemoryStore: Block rdd_141_4 stored as values in memory (estimated size 36.2 MB, free 1358.8 MB)
20/04/13 15:27:58 INFO executor.Executor: Finished task 4.0 in stage 31.0 (TID 141). 1219 bytes result sent to driver
20/04/13 15:27:58 INFO memory.MemoryStore: Block rdd_141_1 stored as values in memory (estimated size 36.0 MB, free 1359.1 MB)
20/04/13 15:27:58 INFO executor.Executor: Finished task 1.0 in stage 31.0 (TID 138). 1219 bytes result sent to driver
20/04/13 15:27:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 145
20/04/13 15:27:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 148
20/04/13 15:27:58 INFO executor.Executor: Running task 4.0 in stage 33.0 (TID 148)
20/04/13 15:27:58 INFO executor.Executor: Running task 1.0 in stage 33.0 (TID 145)
20/04/13 15:27:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 51
20/04/13 15:27:58 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1359.1 MB)
20/04/13 15:27:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 51 took 9 ms
20/04/13 15:27:58 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 4.2 KB, free 1359.1 MB)
20/04/13 15:27:58 INFO storage.BlockManager: Found block rdd_141_4 locally
20/04/13 15:27:58 INFO storage.BlockManager: Found block rdd_141_1 locally
20/04/13 15:27:59 INFO memory.MemoryStore: Block taskresult_145 stored as bytes in memory (estimated size 5.8 MB, free 1353.3 MB)
20/04/13 15:27:59 INFO executor.Executor: Finished task 1.0 in stage 33.0 (TID 145). 6068082 bytes result sent via BlockManager)
20/04/13 15:27:59 INFO memory.MemoryStore: Block taskresult_148 stored as bytes in memory (estimated size 5.8 MB, free 1347.5 MB)
20/04/13 15:27:59 INFO executor.Executor: Finished task 4.0 in stage 33.0 (TID 148). 6067645 bytes result sent via BlockManager)
20/04/13 15:28:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 149
20/04/13 15:28:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 152
20/04/13 15:28:04 INFO executor.Executor: Running task 3.0 in stage 34.0 (TID 152)
20/04/13 15:28:04 INFO executor.Executor: Running task 0.0 in stage 34.0 (TID 149)
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 54
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 2.2 MB, free 1362.1 MB)
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 54 took 30 ms
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 3.3 MB, free 1358.9 MB)
20/04/13 15:28:04 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 53
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:04 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:04 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:28:04 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:28:04 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:28:04 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:28:04 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:28:04 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:28:04 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:28:04 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:28:04 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:28:04 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:28:04 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 262144
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1358.8 MB)
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 53 took 23 ms
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 536.2 KB, free 1358.3 MB)
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 52
20/04/13 15:28:05 INFO memory.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 3.2 MB, free 1291.0 MB)
20/04/13 15:28:05 INFO broadcast.TorrentBroadcast: Reading broadcast variable 52 took 286 ms
20/04/13 15:28:06 INFO memory.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 31.7 MB, free 1259.4 MB)
20/04/13 15:28:06 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:06 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:28:06 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:28:06 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 262144
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:28:22 INFO python.PythonUDFRunner: Times: total = 897, boot = -37570, init = 37641, finish = 826
20/04/13 15:28:22 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9692401
20/04/13 15:28:22 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152804_0034_m_000003_152' to hdfs://178.62.208.209:9000/data/df_5-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152804_0034_m_000003
20/04/13 15:28:22 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152804_0034_m_000003_152: Committed
20/04/13 15:28:22 INFO executor.Executor: Finished task 3.0 in stage 34.0 (TID 152). 3198 bytes result sent to driver
20/04/13 15:28:22 INFO python.PythonUDFRunner: Times: total = 5429, boot = -34222, init = 34577, finish = 5074
20/04/13 15:28:22 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8527085
20/04/13 15:28:23 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152804_0034_m_000000_149' to hdfs://178.62.208.209:9000/data/df_5-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152804_0034_m_000000
20/04/13 15:28:23 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152804_0034_m_000000_149: Committed
20/04/13 15:28:23 INFO executor.Executor: Finished task 0.0 in stage 34.0 (TID 149). 3198 bytes result sent to driver
20/04/13 15:28:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 158
20/04/13 15:28:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 161
20/04/13 15:28:23 INFO executor.Executor: Running task 2.0 in stage 36.0 (TID 158)
20/04/13 15:28:23 INFO executor.Executor: Running task 5.0 in stage 36.0 (TID 161)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 58
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1323.3 MB)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 58 took 7 ms
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 15.0 KB, free 1323.3 MB)
20/04/13 15:28:23 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 57
20/04/13 15:28:23 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1323.3 MB)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 57 took 14 ms
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 536.2 KB, free 1322.8 MB)
20/04/13 15:28:23 INFO executor.Executor: Finished task 5.0 in stage 36.0 (TID 161). 1554 bytes result sent to driver
20/04/13 15:28:23 INFO executor.Executor: Finished task 2.0 in stage 36.0 (TID 158). 1554 bytes result sent to driver
20/04/13 15:28:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 164
20/04/13 15:28:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 167
20/04/13 15:28:23 INFO executor.Executor: Running task 2.0 in stage 37.0 (TID 164)
20/04/13 15:28:23 INFO executor.Executor: Running task 5.0 in stage 37.0 (TID 167)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 60
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1322.8 MB)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 60 took 14 ms
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 27.4 KB, free 1322.7 MB)
20/04/13 15:28:23 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:28:23 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 59
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1322.7 MB)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 59 took 35 ms
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 536.2 KB, free 1322.2 MB)
20/04/13 15:28:23 INFO codegen.CodeGenerator: Code generated in 52.812638 ms
20/04/13 15:28:31 INFO python.PythonUDFRunner: Times: total = 430, boot = -18052, init = 18146, finish = 336
20/04/13 15:28:35 INFO executor.Executor: Finished task 5.0 in stage 37.0 (TID 167). 2421 bytes result sent to driver
20/04/13 15:28:46 INFO python.PythonUDFRunner: Times: total = 3698, boot = -13521, init = 13602, finish = 3617
20/04/13 15:28:49 INFO executor.Executor: Finished task 2.0 in stage 37.0 (TID 164). 2421 bytes result sent to driver
20/04/13 15:28:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 170
20/04/13 15:28:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 173
20/04/13 15:28:49 INFO executor.Executor: Running task 5.0 in stage 38.0 (TID 173)
20/04/13 15:28:49 INFO executor.Executor: Running task 2.0 in stage 38.0 (TID 170)
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache
20/04/13 15:28:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 61
20/04/13 15:28:49 INFO memory.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 1975.0 B, free 1322.2 MB)
20/04/13 15:28:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 61 took 14 ms
20/04/13 15:28:49 INFO memory.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 3.3 KB, free 1322.2 MB)
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:28:49 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:28:49 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 15:28:49 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:28:49 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 15:28:52 INFO memory.MemoryStore: Block rdd_171_5 stored as values in memory (estimated size 35.9 MB, free 1250.0 MB)
20/04/13 15:28:52 INFO executor.Executor: Finished task 5.0 in stage 38.0 (TID 173). 1219 bytes result sent to driver
20/04/13 15:28:53 INFO memory.MemoryStore: Block rdd_171_2 stored as values in memory (estimated size 35.9 MB, free 1250.4 MB)
20/04/13 15:28:53 INFO executor.Executor: Finished task 2.0 in stage 38.0 (TID 170). 1219 bytes result sent to driver
20/04/13 15:28:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 175
20/04/13 15:28:54 INFO executor.Executor: Running task 2.0 in stage 40.0 (TID 175)
20/04/13 15:28:54 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 62
20/04/13 15:28:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 178
20/04/13 15:28:54 INFO executor.Executor: Running task 5.0 in stage 40.0 (TID 178)
20/04/13 15:28:54 INFO memory.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1250.4 MB)
20/04/13 15:28:54 INFO broadcast.TorrentBroadcast: Reading broadcast variable 62 took 8 ms
20/04/13 15:28:54 INFO memory.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 4.2 KB, free 1250.4 MB)
20/04/13 15:28:54 INFO storage.BlockManager: Found block rdd_171_2 locally
20/04/13 15:28:54 INFO storage.BlockManager: Found block rdd_171_5 locally
20/04/13 15:28:54 INFO memory.MemoryStore: Block taskresult_178 stored as bytes in memory (estimated size 5.8 MB, free 1245.8 MB)
20/04/13 15:28:54 INFO executor.Executor: Finished task 5.0 in stage 40.0 (TID 178). 6068055 bytes result sent via BlockManager)
20/04/13 15:28:55 INFO memory.MemoryStore: Block taskresult_175 stored as bytes in memory (estimated size 5.8 MB, free 1245.4 MB)
20/04/13 15:28:55 INFO executor.Executor: Finished task 2.0 in stage 40.0 (TID 175). 6067294 bytes result sent via BlockManager)
20/04/13 15:28:56 INFO storage.BlockManager: Removing RDD 141
20/04/13 15:28:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 181
20/04/13 15:28:59 INFO executor.Executor: Running task 1.0 in stage 41.0 (TID 181)
20/04/13 15:28:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 184
20/04/13 15:28:59 INFO executor.Executor: Running task 4.0 in stage 41.0 (TID 184)
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 65
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 2.2 MB, free 1384.1 MB)
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 65 took 14 ms
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_65 stored as values in memory (estimated size 3.3 MB, free 1380.8 MB)
20/04/13 15:28:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 64
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1380.8 MB)
20/04/13 15:28:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 64 took 29 ms
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_64 stored as values in memory (estimated size 536.2 KB, free 1380.2 MB)
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 63
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 3.3 MB, free 1313.0 MB)
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 63 took 58 ms
20/04/13 15:29:02 INFO memory.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 31.7 MB, free 1281.3 MB)
20/04/13 15:29:02 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:29:02 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:29:02 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:29:02 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:29:02 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:29:02 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 262144
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:29:02 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 262144
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:29:14 INFO python.PythonUDFRunner: Times: total = 602, boot = -35516, init = 35582, finish = 536
20/04/13 15:29:14 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9476607
20/04/13 15:29:15 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152859_0041_m_000004_184' to hdfs://178.62.208.209:9000/data/df_5-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152859_0041_m_000004
20/04/13 15:29:15 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152859_0041_m_000004_184: Committed
20/04/13 15:29:15 INFO executor.Executor: Finished task 4.0 in stage 41.0 (TID 184). 3198 bytes result sent to driver
20/04/13 15:29:15 INFO python.PythonUDFRunner: Times: total = 4777, boot = -32276, init = 32327, finish = 4726
20/04/13 15:29:15 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8620385
20/04/13 15:29:15 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152859_0041_m_000001_181' to hdfs://178.62.208.209:9000/data/df_5-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152859_0041_m_000001
20/04/13 15:29:15 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152859_0041_m_000001_181: Committed
20/04/13 15:29:15 INFO executor.Executor: Finished task 1.0 in stage 41.0 (TID 181). 3198 bytes result sent to driver
20/04/13 15:29:15 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
20/04/13 15:29:15 INFO memory.MemoryStore: MemoryStore cleared
20/04/13 15:29:15 INFO storage.BlockManager: BlockManager stopped
20/04/13 15:29:15 INFO util.ShutdownHookManager: Shutdown hook called
20/04/13 15:29:15 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0007/spark-2ff9f27d-a820-40b0-bb71-176a8db49478
End of LogType:stderr

LogType:stdout
Log Upload Time:Mon Apr 13 15:29:16 +0000 2020
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_1586780871303_0007_01_000001 on 178.62.210.13_35211
==========================================================================
LogType:pyspark.log
Log Upload Time:Mon Apr 13 15:29:17 +0000 2020
LogLength:6473
Log Contents:
[PYTHON] 2020-04-13 15:24:44,270.270 INFO preprocess - wrapper: preprocess_df keyword arguments:
[PYTHON] 2020-04-13 15:24:44,274.274 INFO preprocess - wrapper: n_shingles: 3
[PYTHON] 2020-04-13 15:24:44,274.274 INFO preprocess - wrapper: use_binary_vectors: True
[PYTHON] 2020-04-13 15:24:44,274.274 INFO preprocess - wrapper: use_dense_vectors: False
[PYTHON] 2020-04-13 15:24:44,274.274 INFO preprocess - preprocess_df: preprocessing started
[PYTHON] 2020-04-13 15:24:44,274.274 INFO preprocess - preprocess_df: in_file = '/data/uniprot-proteome_UP000005640.tab'
[PYTHON] 2020-04-13 15:24:44,274.274 INFO preprocess - preprocess_df: n_shingles = ''
[PYTHON] 2020-04-13 15:25:33,941.941 INFO preprocess - preprocess_df: out_file=/data/df_3-shingles_sparse-binary-vectors.parquet
[PYTHON] 2020-04-13 15:25:33,942.942 INFO preprocess - wrapper: preprocess_df finished in 49.67s
[PYTHON] 2020-04-13 15:25:33,942.942 INFO preprocess - <module>: completed 1 in 49.67s (49.67s/it)
[PYTHON] 2020-04-13 15:25:33,942.942 INFO preprocess - <module>: estimated remaining time: 248.36s
[PYTHON] 2020-04-13 15:25:33,942.942 INFO preprocess - wrapper: preprocess_df keyword arguments:
[PYTHON] 2020-04-13 15:25:33,942.942 INFO preprocess - wrapper: n_shingles: 3
[PYTHON] 2020-04-13 15:25:33,942.942 INFO preprocess - wrapper: use_binary_vectors: False
[PYTHON] 2020-04-13 15:25:33,942.942 INFO preprocess - wrapper: use_dense_vectors: False
[PYTHON] 2020-04-13 15:25:33,942.942 INFO preprocess - preprocess_df: preprocessing started
[PYTHON] 2020-04-13 15:25:33,942.942 INFO preprocess - preprocess_df: in_file = '/data/uniprot-proteome_UP000005640.tab'
[PYTHON] 2020-04-13 15:25:33,942.942 INFO preprocess - preprocess_df: n_shingles = ''
[PYTHON] 2020-04-13 15:26:07,246.246 INFO preprocess - preprocess_df: out_file=/data/df_3-shingles_sparse-count-vectors.parquet
[PYTHON] 2020-04-13 15:26:07,246.246 INFO preprocess - wrapper: preprocess_df finished in 33.30s
[PYTHON] 2020-04-13 15:26:07,246.246 INFO preprocess - <module>: completed 2 in 82.98s (41.49s/it)
[PYTHON] 2020-04-13 15:26:07,247.247 INFO preprocess - <module>: estimated remaining time: 165.95s
[PYTHON] 2020-04-13 15:26:07,247.247 INFO preprocess - wrapper: preprocess_df keyword arguments:
[PYTHON] 2020-04-13 15:26:07,247.247 INFO preprocess - wrapper: n_shingles: 4
[PYTHON] 2020-04-13 15:26:07,247.247 INFO preprocess - wrapper: use_binary_vectors: True
[PYTHON] 2020-04-13 15:26:07,247.247 INFO preprocess - wrapper: use_dense_vectors: False
[PYTHON] 2020-04-13 15:26:07,247.247 INFO preprocess - preprocess_df: preprocessing started
[PYTHON] 2020-04-13 15:26:07,247.247 INFO preprocess - preprocess_df: in_file = '/data/uniprot-proteome_UP000005640.tab'
[PYTHON] 2020-04-13 15:26:07,247.247 INFO preprocess - preprocess_df: n_shingles = ''
[PYTHON] 2020-04-13 15:26:46,107.107 INFO preprocess - preprocess_df: out_file=/data/df_4-shingles_sparse-binary-vectors.parquet
[PYTHON] 2020-04-13 15:26:46,107.107 INFO preprocess - wrapper: preprocess_df finished in 38.86s
[PYTHON] 2020-04-13 15:26:46,107.107 INFO preprocess - <module>: completed 3 in 121.84s (40.61s/it)
[PYTHON] 2020-04-13 15:26:46,107.107 INFO preprocess - <module>: estimated remaining time: 121.84s
[PYTHON] 2020-04-13 15:26:46,107.107 INFO preprocess - wrapper: preprocess_df keyword arguments:
[PYTHON] 2020-04-13 15:26:46,107.107 INFO preprocess - wrapper: n_shingles: 4
[PYTHON] 2020-04-13 15:26:46,107.107 INFO preprocess - wrapper: use_binary_vectors: False
[PYTHON] 2020-04-13 15:26:46,107.107 INFO preprocess - wrapper: use_dense_vectors: False
[PYTHON] 2020-04-13 15:26:46,108.108 INFO preprocess - preprocess_df: preprocessing started
[PYTHON] 2020-04-13 15:26:46,108.108 INFO preprocess - preprocess_df: in_file = '/data/uniprot-proteome_UP000005640.tab'
[PYTHON] 2020-04-13 15:26:46,108.108 INFO preprocess - preprocess_df: n_shingles = ''
[PYTHON] 2020-04-13 15:27:26,208.208 INFO preprocess - preprocess_df: out_file=/data/df_4-shingles_sparse-count-vectors.parquet
[PYTHON] 2020-04-13 15:27:26,208.208 INFO preprocess - wrapper: preprocess_df finished in 40.10s
[PYTHON] 2020-04-13 15:27:26,208.208 INFO preprocess - <module>: completed 4 in 161.94s (40.48s/it)
[PYTHON] 2020-04-13 15:27:26,208.208 INFO preprocess - <module>: estimated remaining time: 80.97s
[PYTHON] 2020-04-13 15:27:26,208.208 INFO preprocess - wrapper: preprocess_df keyword arguments:
[PYTHON] 2020-04-13 15:27:26,209.209 INFO preprocess - wrapper: n_shingles: 5
[PYTHON] 2020-04-13 15:27:26,209.209 INFO preprocess - wrapper: use_binary_vectors: True
[PYTHON] 2020-04-13 15:27:26,209.209 INFO preprocess - wrapper: use_dense_vectors: False
[PYTHON] 2020-04-13 15:27:26,209.209 INFO preprocess - preprocess_df: preprocessing started
[PYTHON] 2020-04-13 15:27:26,209.209 INFO preprocess - preprocess_df: in_file = '/data/uniprot-proteome_UP000005640.tab'
[PYTHON] 2020-04-13 15:27:26,209.209 INFO preprocess - preprocess_df: n_shingles = ''
[PYTHON] 2020-04-13 15:28:23,107.107 INFO preprocess - preprocess_df: out_file=/data/df_5-shingles_sparse-binary-vectors.parquet
[PYTHON] 2020-04-13 15:28:23,108.108 INFO preprocess - wrapper: preprocess_df finished in 56.90s
[PYTHON] 2020-04-13 15:28:23,108.108 INFO preprocess - <module>: completed 5 in 218.84s (43.77s/it)
[PYTHON] 2020-04-13 15:28:23,108.108 INFO preprocess - <module>: estimated remaining time: 43.77s
[PYTHON] 2020-04-13 15:28:23,108.108 INFO preprocess - wrapper: preprocess_df keyword arguments:
[PYTHON] 2020-04-13 15:28:23,108.108 INFO preprocess - wrapper: n_shingles: 5
[PYTHON] 2020-04-13 15:28:23,108.108 INFO preprocess - wrapper: use_binary_vectors: False
[PYTHON] 2020-04-13 15:28:23,108.108 INFO preprocess - wrapper: use_dense_vectors: False
[PYTHON] 2020-04-13 15:28:23,108.108 INFO preprocess - preprocess_df: preprocessing started
[PYTHON] 2020-04-13 15:28:23,109.109 INFO preprocess - preprocess_df: in_file = '/data/uniprot-proteome_UP000005640.tab'
[PYTHON] 2020-04-13 15:28:23,109.109 INFO preprocess - preprocess_df: n_shingles = ''
[PYTHON] 2020-04-13 15:29:15,285.285 INFO preprocess - preprocess_df: out_file=/data/df_5-shingles_sparse-count-vectors.parquet
[PYTHON] 2020-04-13 15:29:15,286.286 INFO preprocess - wrapper: preprocess_df finished in 52.18s
[PYTHON] 2020-04-13 15:29:15,286.286 INFO preprocess - <module>: completed 6 in 271.02s (45.17s/it)
[PYTHON] 2020-04-13 15:29:15,286.286 INFO preprocess - <module>: preprocessing finished in 271.02s (4.52 minutes)
End of LogType:pyspark.log

LogType:stderr
Log Upload Time:Mon Apr 13 15:29:17 +0000 2020
LogLength:311846
Log Contents:
20/04/13 15:24:34 INFO util.SignalUtils: Registered signal handler for TERM
20/04/13 15:24:34 INFO util.SignalUtils: Registered signal handler for HUP
20/04/13 15:24:34 INFO util.SignalUtils: Registered signal handler for INT
20/04/13 15:24:34 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 15:24:34 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 15:24:34 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 15:24:34 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 15:24:34 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 15:24:35 INFO yarn.ApplicationMaster: Preparing Local resources
20/04/13 15:24:36 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1586780871303_0007_000001
20/04/13 15:24:36 INFO yarn.ApplicationMaster: Starting the user application in a separate Thread
20/04/13 15:24:36 INFO yarn.ApplicationMaster: Waiting for spark context initialization...
20/04/13 15:24:36 INFO spark.SparkContext: Running Spark version 2.4.5
20/04/13 15:24:36 INFO spark.SparkContext: Submitted application: ClusteringExperiment
20/04/13 15:24:36 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 15:24:36 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 15:24:36 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 15:24:36 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 15:24:36 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 15:24:37 INFO util.Utils: Successfully started service 'sparkDriver' on port 39481.
20/04/13 15:24:37 INFO spark.SparkEnv: Registering MapOutputTracker
20/04/13 15:24:37 INFO spark.SparkEnv: Registering BlockManagerMaster
20/04/13 15:24:37 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/04/13 15:24:37 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/04/13 15:24:37 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0007/blockmgr-846dc080-149c-48bc-bde8-139dab7f4f59
20/04/13 15:24:37 INFO memory.MemoryStore: MemoryStore started with capacity 912.3 MB
20/04/13 15:24:37 INFO spark.SparkEnv: Registering OutputCommitCoordinator
20/04/13 15:24:37 INFO util.log: Logging initialized @3559ms
20/04/13 15:24:37 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
20/04/13 15:24:37 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
20/04/13 15:24:37 INFO server.Server: Started @3674ms
20/04/13 15:24:37 INFO server.AbstractConnector: Started ServerConnector@1c5dc510{HTTP/1.1,[http/1.1]}{0.0.0.0:33879}
20/04/13 15:24:37 INFO util.Utils: Successfully started service 'SparkUI' on port 33879.
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ebcfaa{/jobs,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46d2df4a{/jobs/json,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5773cc74{/jobs/job,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a74c211{/jobs/job/json,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3a0c789a{/stages,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@73d513c3{/stages/json,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@436584f3{/stages/stage,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@dcc891c{/stages/stage/json,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6261c64a{/stages/pool,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@588350{/stages/pool/json,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30acec7f{/storage,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59c1071f{/storage/json,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@35fed9fb{/storage/rdd,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71f88e17{/storage/rdd/json,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26604b43{/environment,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7748596f{/environment/json,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@343a677c{/executors,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@187dfda{/executors/json,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2519b6fa{/executors/threadDump,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10af0017{/executors/threadDump/json,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@774994f9{/static,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53f1e871{/,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e589cf4{/api,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26c3b686{/jobs/job/kill,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@36e75649{/stages/stage/kill,null,AVAILABLE,@Spark}
20/04/13 15:24:37 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://178.62.210.13:33879
20/04/13 15:24:37 INFO cluster.YarnClusterScheduler: Created YarnClusterScheduler
20/04/13 15:24:37 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1586780871303_0007 and attemptId Some(appattempt_1586780871303_0007_000001)
20/04/13 15:24:37 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44585.
20/04/13 15:24:37 INFO netty.NettyBlockTransferService: Server created on 178.62.210.13:44585
20/04/13 15:24:37 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/13 15:24:37 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 178.62.210.13, 44585, None)
20/04/13 15:24:37 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.210.13:44585 with 912.3 MB RAM, BlockManagerId(driver, 178.62.210.13, 44585, None)
20/04/13 15:24:37 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 178.62.210.13, 44585, None)
20/04/13 15:24:37 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 178.62.210.13, 44585, None)
20/04/13 15:24:38 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
20/04/13 15:24:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@94f55cd{/metrics/json,null,AVAILABLE,@Spark}
20/04/13 15:24:38 INFO client.RMProxy: Connecting to ResourceManager at /178.62.208.209:8030
20/04/13 15:24:38 INFO yarn.YarnRMClient: Registering the ApplicationMaster
20/04/13 15:24:38 INFO yarn.ApplicationMaster: 
===============================================================================
YARN executor launch context:
  env:
    CLASSPATH -> {{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>/usr/src/hadoop-2.8.5/etc/hadoop:/usr/src/hadoop-2.8.5/share/hadoop/common/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/common/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/*:/usr/src/hadoop-2.8.5/contrib/capacity-scheduler/*.jar<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__
    SPARK_DIST_CLASSPATH -> /usr/src/hadoop-2.8.5/etc/hadoop:/usr/src/hadoop-2.8.5/share/hadoop/common/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/common/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/hdfs/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/yarn/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/lib/*:/usr/src/hadoop-2.8.5/share/hadoop/mapreduce/*:/usr/src/hadoop-2.8.5/contrib/capacity-scheduler/*.jar
    SPARK_YARN_STAGING_DIR -> hdfs://178.62.208.209:9000/user/root/.sparkStaging/application_1586780871303_0007
    SPARK_USER -> root
    PYTHONPATH -> /usr/src/spark-2.4.5-bin-without-hadoop/python:/usr/src/spark-2.4.5-bin-without-hadoop/python/build:/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/pyspark.zip:/usr/src/spark-2.4.5-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip

  command:
    {{JAVA_HOME}}/bin/java \ 
      -server \ 
      -Xmx3072m \ 
      -Djava.io.tmpdir={{PWD}}/tmp \ 
      '-Dspark.driver.port=39481' \ 
      '-Dspark.ui.port=0' \ 
      -Dspark.yarn.app.container.log.dir=<LOG_DIR> \ 
      -XX:OnOutOfMemoryError='kill %p' \ 
      org.apache.spark.executor.CoarseGrainedExecutorBackend \ 
      --driver-url \ 
      spark://CoarseGrainedScheduler@178.62.210.13:39481 \ 
      --executor-id \ 
      <executorId> \ 
      --hostname \ 
      <hostname> \ 
      --cores \ 
      2 \ 
      --app-id \ 
      application_1586780871303_0007 \ 
      --user-class-path \ 
      file:$PWD/__app__.jar \ 
      1><LOG_DIR>/stdout \ 
      2><LOG_DIR>/stderr

  resources:
    pyspark.zip -> resource { scheme: "hdfs" host: "178.62.208.209" port: 9000 file: "/user/root/.sparkStaging/application_1586780871303_0007/pyspark.zip" } size: 591945 timestamp: 1586791471425 type: FILE visibility: PRIVATE
    py4j-0.10.7-src.zip -> resource { scheme: "hdfs" host: "178.62.208.209" port: 9000 file: "/user/root/.sparkStaging/application_1586780871303_0007/py4j-0.10.7-src.zip" } size: 42437 timestamp: 1586791471450 type: FILE visibility: PRIVATE
    __spark_libs__ -> resource { scheme: "hdfs" host: "178.62.208.209" port: 9000 file: "/user/root/.sparkStaging/application_1586780871303_0007/__spark_libs__2486182116843167623.zip" } size: 168822862 timestamp: 1586791471274 type: ARCHIVE visibility: PRIVATE
    __spark_conf__ -> resource { scheme: "hdfs" host: "178.62.208.209" port: 9000 file: "/user/root/.sparkStaging/application_1586780871303_0007/__spark_conf__.zip" } size: 233311 timestamp: 1586791471582 type: ARCHIVE visibility: PRIVATE

===============================================================================
20/04/13 15:24:38 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark://YarnAM@178.62.210.13:39481)
20/04/13 15:24:38 INFO yarn.YarnAllocator: Will request 3 executor container(s), each with 2 core(s) and 3456 MB memory (including 384 MB of overhead)
20/04/13 15:24:38 INFO yarn.YarnAllocator: Submitted 3 unlocalized container requests.
20/04/13 15:24:38 INFO yarn.ApplicationMaster: Started progress reporter thread with (heartbeat : 3000, initial allocation : 200) intervals
20/04/13 15:24:38 INFO impl.AMRMClientImpl: Received new token for : 178.62.200.211:37461
20/04/13 15:24:38 INFO yarn.YarnAllocator: Launching container container_1586780871303_0007_01_000002 on host 178.62.200.211 for executor with ID 1
20/04/13 15:24:38 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/13 15:24:38 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/13 15:24:39 INFO impl.AMRMClientImpl: Received new token for : 178.62.210.13:35211
20/04/13 15:24:39 INFO yarn.YarnAllocator: Launching container container_1586780871303_0007_01_000003 on host 178.62.210.13 for executor with ID 2
20/04/13 15:24:39 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
20/04/13 15:24:39 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/13 15:24:39 INFO yarn.YarnAllocator: Launching container container_1586780871303_0007_01_000004 on host 178.62.200.211 for executor with ID 3
20/04/13 15:24:39 INFO yarn.YarnAllocator: Received 2 containers from YARN, launching executors on 1 of them.
20/04/13 15:24:39 INFO impl.ContainerManagementProtocolProxy: yarn.client.max-cached-nodemanagers-proxies : 0
20/04/13 15:24:41 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.210.13:38976) with ID 2
20/04/13 15:24:41 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.210.13:36375 with 1458.6 MB RAM, BlockManagerId(2, 178.62.210.13, 36375, None)
20/04/13 15:24:42 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 0 of them.
20/04/13 15:24:43 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.200.211:57158) with ID 1
20/04/13 15:24:43 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (178.62.200.211:57160) with ID 3
20/04/13 15:24:43 INFO cluster.YarnClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/04/13 15:24:43 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
20/04/13 15:24:43 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.200.211:36927 with 1458.6 MB RAM, BlockManagerId(1, 178.62.200.211, 36927, None)
20/04/13 15:24:43 INFO storage.BlockManagerMasterEndpoint: Registering block manager 178.62.200.211:35537 with 1458.6 MB RAM, BlockManagerId(3, 178.62.200.211, 35537, None)
20/04/13 15:24:43 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0007/container_1586780871303_0007_01_000001/spark-warehouse').
20/04/13 15:24:43 INFO internal.SharedState: Warehouse path is 'file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0007/container_1586780871303_0007_01_000001/spark-warehouse'.
20/04/13 15:24:43 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.
20/04/13 15:24:43 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15ac894f{/SQL,null,AVAILABLE,@Spark}
20/04/13 15:24:43 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.
20/04/13 15:24:43 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56595bad{/SQL/json,null,AVAILABLE,@Spark}
20/04/13 15:24:43 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.
20/04/13 15:24:43 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24083b27{/SQL/execution,null,AVAILABLE,@Spark}
20/04/13 15:24:43 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.
20/04/13 15:24:43 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68a7df0{/SQL/execution/json,null,AVAILABLE,@Spark}
20/04/13 15:24:43 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.
20/04/13 15:24:43 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28401a09{/static/sql,null,AVAILABLE,@Spark}
20/04/13 15:24:44 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/04/13 15:24:44 INFO datasources.InMemoryFileIndex: It took 105 ms to list leaf files for 1 paths.
20/04/13 15:24:44 INFO datasources.InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.
20/04/13 15:24:46 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:24:46 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
20/04/13 15:24:46 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 15:24:46 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:24:46 INFO codegen.CodeGenerator: Code generated in 283.011459 ms
20/04/13 15:24:46 INFO codegen.CodeGenerator: Code generated in 23.477882 ms
20/04/13 15:24:47 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 337.3 KB, free 912.0 MB)
20/04/13 15:24:47 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 31.5 KB, free 911.9 MB)
20/04/13 15:24:47 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 912.3 MB)
20/04/13 15:24:47 INFO spark.SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
20/04/13 15:24:47 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:24:47 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 15:24:47 INFO scheduler.DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/04/13 15:24:47 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 15:24:47 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:24:47 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:24:47 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:24:47 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.9 KB, free 911.9 MB)
20/04/13 15:24:47 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 911.9 MB)
20/04/13 15:24:47 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.210.13:44585 (size: 4.6 KB, free: 912.3 MB)
20/04/13 15:24:47 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163
20/04/13 15:24:47 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/04/13 15:24:47 INFO cluster.YarnClusterScheduler: Adding task set 0.0 with 1 tasks
20/04/13 15:24:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 178.62.210.13, executor 2, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:24:47 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 178.62.210.13:36375 (size: 4.6 KB, free: 1458.6 MB)
20/04/13 15:24:48 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1458.6 MB)
20/04/13 15:24:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1958 ms on 178.62.210.13 (executor 2) (1/1)
20/04/13 15:24:49 INFO cluster.YarnClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/04/13 15:24:49 INFO scheduler.DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 2.043 s
20/04/13 15:24:49 INFO scheduler.DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 2.100001 s
20/04/13 15:24:49 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:24:49 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:24:49 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 15:24:49 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:24:49 INFO codegen.CodeGenerator: Code generated in 9.859284 ms
20/04/13 15:24:49 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 337.3 KB, free 911.6 MB)
20/04/13 15:24:49 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 31.5 KB, free 911.6 MB)
20/04/13 15:24:49 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 912.2 MB)
20/04/13 15:24:49 INFO spark.SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
20/04/13 15:24:49 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:24:49 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 15:24:49 INFO scheduler.DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 15:24:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 5
20/04/13 15:24:49 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 11
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 13
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 23
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 31
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 25
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 21
20/04/13 15:24:49 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 18
20/04/13 15:24:49 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:24:49 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.0 KB, free 911.6 MB)
20/04/13 15:24:49 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KB, free 911.5 MB)
20/04/13 15:24:49 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 178.62.210.13:44585 (size: 8.0 KB, free: 912.2 MB)
20/04/13 15:24:49 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163
20/04/13 15:24:49 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:24:49 INFO cluster.YarnClusterScheduler: Adding task set 1.0 with 6 tasks
20/04/13 15:24:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:24:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, 178.62.200.211, executor 3, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 15:24:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 15:24:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 15:24:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, 178.62.200.211, executor 3, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 15:24:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 15:24:49 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 178.62.210.13:36375 in memory (size: 4.6 KB, free: 1458.6 MB)
20/04/13 15:24:49 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 178.62.210.13:44585 in memory (size: 4.6 KB, free: 912.2 MB)
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 10
20/04/13 15:24:49 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 912.3 MB)
20/04/13 15:24:49 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1458.6 MB)
20/04/13 15:24:49 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 178.62.210.13:36375 (size: 8.0 KB, free: 1458.6 MB)
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 28
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 29
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 9
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 1
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 14
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 8
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 3
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 12
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 15
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 20
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 6
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 2
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 16
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 30
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 4
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 17
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 7
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 26
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 27
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 19
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 22
20/04/13 15:24:49 INFO spark.ContextCleaner: Cleaned accumulator 24
20/04/13 15:24:49 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 178.62.200.211:35537 (size: 8.0 KB, free: 1458.6 MB)
20/04/13 15:24:49 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 178.62.200.211:36927 (size: 8.0 KB, free: 1458.6 MB)
20/04/13 15:24:50 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1458.6 MB)
20/04/13 15:24:51 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 1585 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:24:51 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 1693 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:24:53 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1458.6 MB)
20/04/13 15:24:53 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1458.6 MB)
20/04/13 15:24:55 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 5942 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 15:24:55 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 5995 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 15:24:55 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 6294 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:24:55 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 6296 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 15:24:55 INFO cluster.YarnClusterScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/04/13 15:24:55 INFO scheduler.DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 6.316 s
20/04/13 15:24:55 INFO scheduler.DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 6.336806 s
20/04/13 15:24:56 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:24:56 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:24:56 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Sequence: string>
20/04/13 15:24:56 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:24:56 INFO codegen.CodeGenerator: Code generated in 17.497202 ms
20/04/13 15:24:56 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 337.3 KB, free 911.6 MB)
20/04/13 15:24:56 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 31.5 KB, free 911.6 MB)
20/04/13 15:24:56 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 912.2 MB)
20/04/13 15:24:56 INFO spark.SparkContext: Created broadcast 4 from rdd at CountVectorizer.scala:187
20/04/13 15:24:56 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:24:56 INFO spark.SparkContext: Starting job: count at CountVectorizer.scala:230
20/04/13 15:24:56 INFO scheduler.DAGScheduler: Registering RDD 19 (flatMap at CountVectorizer.scala:205) as input to shuffle 0
20/04/13 15:24:56 INFO scheduler.DAGScheduler: Got job 2 (count at CountVectorizer.scala:230) with 6 output partitions
20/04/13 15:24:56 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (count at CountVectorizer.scala:230)
20/04/13 15:24:56 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/04/13 15:24:56 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/04/13 15:24:56 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[19] at flatMap at CountVectorizer.scala:205), which has no missing parents
20/04/13 15:24:56 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 27.4 KB, free 911.5 MB)
20/04/13 15:24:56 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.4 KB, free 911.5 MB)
20/04/13 15:24:56 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.210.13:44585 (size: 13.4 KB, free: 912.2 MB)
20/04/13 15:24:56 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1163
20/04/13 15:24:56 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[19] at flatMap at CountVectorizer.scala:205) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:24:56 INFO cluster.YarnClusterScheduler: Adding task set 2.0 with 6 tasks
20/04/13 15:24:56 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 7, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 8258 bytes)
20/04/13 15:24:56 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 8, 178.62.200.211, executor 1, partition 1, NODE_LOCAL, 8258 bytes)
20/04/13 15:24:56 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 9, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 8258 bytes)
20/04/13 15:24:56 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 2.0 (TID 10, 178.62.200.211, executor 3, partition 3, NODE_LOCAL, 8258 bytes)
20/04/13 15:24:56 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 2.0 (TID 11, 178.62.200.211, executor 1, partition 4, NODE_LOCAL, 8258 bytes)
20/04/13 15:24:56 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 2.0 (TID 12, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 8258 bytes)
20/04/13 15:24:56 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.210.13:36375 (size: 13.4 KB, free: 1458.5 MB)
20/04/13 15:24:56 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.200.211:35537 (size: 13.4 KB, free: 1458.5 MB)
20/04/13 15:24:56 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 178.62.200.211:36927 (size: 13.4 KB, free: 1458.5 MB)
20/04/13 15:24:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1458.5 MB)
20/04/13 15:24:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1458.5 MB)
20/04/13 15:24:57 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1458.5 MB)
20/04/13 15:25:01 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 2.0 (TID 12) in 5009 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:25:01 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 58941
20/04/13 15:25:05 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 9) in 8839 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:25:12 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 2.0 (TID 10) in 16583 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 15:25:13 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 2.0 (TID 11) in 17028 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:25:13 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 7) in 17345 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 15:25:13 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 8) in 17386 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 15:25:13 INFO cluster.YarnClusterScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/04/13 15:25:13 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (flatMap at CountVectorizer.scala:205) finished in 17.443 s
20/04/13 15:25:13 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/04/13 15:25:13 INFO scheduler.DAGScheduler: running: Set()
20/04/13 15:25:13 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 3)
20/04/13 15:25:13 INFO scheduler.DAGScheduler: failed: Set()
20/04/13 15:25:13 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[21] at map at CountVectorizer.scala:223), which has no missing parents
20/04/13 15:25:13 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 911.5 MB)
20/04/13 15:25:13 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1976.0 B, free 911.5 MB)
20/04/13 15:25:13 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.210.13:44585 (size: 1976.0 B, free: 912.2 MB)
20/04/13 15:25:13 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1163
20/04/13 15:25:13 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 3 (MapPartitionsRDD[21] at map at CountVectorizer.scala:223) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:25:13 INFO cluster.YarnClusterScheduler: Adding task set 3.0 with 6 tasks
20/04/13 15:25:13 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 13, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 7651 bytes)
20/04/13 15:25:13 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 14, 178.62.210.13, executor 2, partition 1, NODE_LOCAL, 7651 bytes)
20/04/13 15:25:13 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 15, 178.62.200.211, executor 3, partition 2, NODE_LOCAL, 7651 bytes)
20/04/13 15:25:13 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 3.0 (TID 16, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 7651 bytes)
20/04/13 15:25:13 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 3.0 (TID 17, 178.62.210.13, executor 2, partition 4, NODE_LOCAL, 7651 bytes)
20/04/13 15:25:13 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 3.0 (TID 18, 178.62.200.211, executor 3, partition 5, NODE_LOCAL, 7651 bytes)
20/04/13 15:25:13 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.200.211:35537 (size: 1976.0 B, free: 1458.5 MB)
20/04/13 15:25:13 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.210.13:36375 (size: 1976.0 B, free: 1458.5 MB)
20/04/13 15:25:13 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 178.62.200.211:36927 (size: 1976.0 B, free: 1458.5 MB)
20/04/13 15:25:13 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.210.13:38976
20/04/13 15:25:13 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.200.211:57160
20/04/13 15:25:13 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 178.62.200.211:57158
20/04/13 15:25:14 INFO storage.BlockManagerInfo: Added rdd_21_1 in memory on 178.62.210.13:36375 (size: 147.4 KB, free: 1458.4 MB)
20/04/13 15:25:14 INFO storage.BlockManagerInfo: Added rdd_21_4 in memory on 178.62.210.13:36375 (size: 147.4 KB, free: 1458.2 MB)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 3.0 (TID 17) in 285 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 14) in 290 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:25:14 INFO storage.BlockManagerInfo: Added rdd_21_5 in memory on 178.62.200.211:35537 (size: 147.2 KB, free: 1458.4 MB)
20/04/13 15:25:14 INFO storage.BlockManagerInfo: Added rdd_21_2 in memory on 178.62.200.211:35537 (size: 146.7 KB, free: 1458.2 MB)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 3.0 (TID 18) in 493 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 15) in 496 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 15:25:14 INFO storage.BlockManagerInfo: Added rdd_21_0 in memory on 178.62.200.211:36927 (size: 147.7 KB, free: 1458.4 MB)
20/04/13 15:25:14 INFO storage.BlockManagerInfo: Added rdd_21_3 in memory on 178.62.200.211:36927 (size: 147.4 KB, free: 1458.2 MB)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 13) in 527 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 3.0 (TID 16) in 527 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 15:25:14 INFO cluster.YarnClusterScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/04/13 15:25:14 INFO scheduler.DAGScheduler: ResultStage 3 (count at CountVectorizer.scala:230) finished in 0.536 s
20/04/13 15:25:14 INFO scheduler.DAGScheduler: Job 2 finished: count at CountVectorizer.scala:230, took 18.004275 s
20/04/13 15:25:14 INFO spark.SparkContext: Starting job: top at CountVectorizer.scala:233
20/04/13 15:25:14 INFO scheduler.DAGScheduler: Got job 3 (top at CountVectorizer.scala:233) with 6 output partitions
20/04/13 15:25:14 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (top at CountVectorizer.scala:233)
20/04/13 15:25:14 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
20/04/13 15:25:14 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:25:14 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at top at CountVectorizer.scala:233), which has no missing parents
20/04/13 15:25:14 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.2 KB, free 911.5 MB)
20/04/13 15:25:14 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 911.5 MB)
20/04/13 15:25:14 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.210.13:44585 (size: 2.4 KB, free: 912.2 MB)
20/04/13 15:25:14 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1163
20/04/13 15:25:14 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at top at CountVectorizer.scala:233) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:25:14 INFO cluster.YarnClusterScheduler: Adding task set 5.0 with 6 tasks
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 19, 178.62.200.211, executor 3, partition 2, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 20, 178.62.200.211, executor 1, partition 0, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 21, 178.62.210.13, executor 2, partition 1, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 5.0 (TID 22, 178.62.200.211, executor 3, partition 5, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 5.0 (TID 23, 178.62.200.211, executor 1, partition 3, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 5.0 (TID 24, 178.62.210.13, executor 2, partition 4, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:25:14 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.210.13:36375 (size: 2.4 KB, free: 1458.2 MB)
20/04/13 15:25:14 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.200.211:36927 (size: 2.4 KB, free: 1458.2 MB)
20/04/13 15:25:14 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 178.62.200.211:35537 (size: 2.4 KB, free: 1458.2 MB)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 5.0 (TID 24) in 100 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 21) in 102 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 20) in 118 ms on 178.62.200.211 (executor 1) (3/6)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 5.0 (TID 23) in 124 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 5.0 (TID 22) in 139 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 15:25:14 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 19) in 145 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:25:14 INFO cluster.YarnClusterScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/04/13 15:25:14 INFO scheduler.DAGScheduler: ResultStage 5 (top at CountVectorizer.scala:233) finished in 0.155 s
20/04/13 15:25:14 INFO scheduler.DAGScheduler: Job 3 finished: top at CountVectorizer.scala:233, took 0.160941 s
20/04/13 15:25:14 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 1186.5 KB, free 910.3 MB)
20/04/13 15:25:14 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 77.2 KB, free 910.3 MB)
20/04/13 15:25:14 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.210.13:44585 (size: 77.2 KB, free: 912.1 MB)
20/04/13 15:25:14 INFO spark.SparkContext: Created broadcast 8 from broadcast at CountVectorizer.scala:298
20/04/13 15:25:14 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:25:14 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:25:14 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Entry: string, Entry name: string, Sequence: string ... 1 more fields>
20/04/13 15:25:14 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:25:15 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO codegen.CodeGenerator: Code generated in 30.098358 ms
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 337.3 KB, free 909.9 MB)
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 31.5 KB, free 909.9 MB)
20/04/13 15:25:15 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 912.1 MB)
20/04/13 15:25:15 INFO spark.SparkContext: Created broadcast 9 from parquet at NativeMethodAccessorImpl.java:0
20/04/13 15:25:15 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:25:15 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
20/04/13 15:25:15 INFO scheduler.DAGScheduler: Got job 4 (parquet at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 15:25:15 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0)
20/04/13 15:25:15 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:25:15 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:25:15 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 347.5 KB, free 909.6 MB)
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 124.8 KB, free 909.5 MB)
20/04/13 15:25:15 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.210.13:44585 (size: 124.8 KB, free: 912.0 MB)
20/04/13 15:25:15 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1163
20/04/13 15:25:15 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:25:15 INFO cluster.YarnClusterScheduler: Adding task set 6.0 with 6 tasks
20/04/13 15:25:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 25, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:15 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 26, 178.62.210.13, executor 2, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:15 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 27, 178.62.200.211, executor 3, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:15 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 28, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:15 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 29, 178.62.210.13, executor 2, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:15 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 30, 178.62.200.211, executor 3, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:15 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.200.211:36927 (size: 124.8 KB, free: 1458.1 MB)
20/04/13 15:25:15 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.210.13:36375 (size: 124.8 KB, free: 1458.1 MB)
20/04/13 15:25:15 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 178.62.200.211:35537 (size: 124.8 KB, free: 1458.1 MB)
20/04/13 15:25:15 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1458.1 MB)
20/04/13 15:25:15 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1458.1 MB)
20/04/13 15:25:15 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1458.1 MB)
20/04/13 15:25:15 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.200.211:36927 (size: 77.2 KB, free: 1458.0 MB)
20/04/13 15:25:15 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.210.13:36375 (size: 77.2 KB, free: 1458.0 MB)
20/04/13 15:25:15 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 178.62.200.211:35537 (size: 77.2 KB, free: 1458.0 MB)
20/04/13 15:25:25 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 6.0 (TID 30) in 10322 ms on 178.62.200.211 (executor 3) (1/6)
20/04/13 15:25:27 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 6.0 (TID 29) in 12248 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:25:27 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 26) in 12660 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 15:25:31 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 6.0 (TID 28) in 16356 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:25:32 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 25) in 16974 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:25:33 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 27) in 18498 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:25:33 INFO cluster.YarnClusterScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool 
20/04/13 15:25:33 INFO scheduler.DAGScheduler: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0) finished in 18.577 s
20/04/13 15:25:33 INFO scheduler.DAGScheduler: Job 4 finished: parquet at NativeMethodAccessorImpl.java:0, took 18.580810 s
20/04/13 15:25:33 INFO datasources.FileFormatWriter: Write Job d00f65d4-4ff4-4556-a49b-c55da86bdb27 committed.
20/04/13 15:25:33 INFO datasources.FileFormatWriter: Finished processing stats for write job d00f65d4-4ff4-4556-a49b-c55da86bdb27.
20/04/13 15:25:33 INFO datasources.InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/04/13 15:25:33 INFO datasources.InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/04/13 15:25:34 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:25:34 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#60, None)) > 0)
20/04/13 15:25:34 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 15:25:34 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 337.3 KB, free 909.1 MB)
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 31.5 KB, free 909.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 912.0 MB)
20/04/13 15:25:34 INFO spark.SparkContext: Created broadcast 11 from csv at NativeMethodAccessorImpl.java:0
20/04/13 15:25:34 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:25:34 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Got job 5 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[33] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 8.9 KB, free 909.1 MB)
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 4.6 KB, free 909.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 178.62.210.13:44585 (size: 4.6 KB, free: 912.0 MB)
20/04/13 15:25:34 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1163
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[33] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/04/13 15:25:34 INFO cluster.YarnClusterScheduler: Adding task set 7.0 with 1 tasks
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 31, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 178.62.200.211:35537 (size: 4.6 KB, free: 1458.0 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1458.0 MB)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 31) in 68 ms on 178.62.200.211 (executor 3) (1/1)
20/04/13 15:25:34 INFO cluster.YarnClusterScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool 
20/04/13 15:25:34 INFO scheduler.DAGScheduler: ResultStage 7 (csv at NativeMethodAccessorImpl.java:0) finished in 0.078 s
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Job 5 finished: csv at NativeMethodAccessorImpl.java:0, took 0.080363 s
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 44
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 95
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 43
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 48
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 70
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 86
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 51
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 50
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 53
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 118
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 52
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 84
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 98
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 178.62.210.13:36375 in memory (size: 1976.0 B, free: 1458.0 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 178.62.200.211:35537 in memory (size: 1976.0 B, free: 1458.0 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 178.62.200.211:36927 in memory (size: 1976.0 B, free: 1458.0 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 178.62.210.13:44585 in memory (size: 1976.0 B, free: 912.0 MB)
20/04/13 15:25:34 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:25:34 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:25:34 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 15:25:34 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 168
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 178.62.210.13:44585 in memory (size: 2.4 KB, free: 912.0 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 178.62.200.211:36927 in memory (size: 2.4 KB, free: 1458.0 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 178.62.200.211:35537 in memory (size: 2.4 KB, free: 1458.0 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 178.62.210.13:36375 in memory (size: 2.4 KB, free: 1458.0 MB)
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 337.3 KB, free 908.8 MB)
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 203
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 45
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 190
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 114
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 159
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 60
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 87
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 42
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 165
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 144
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 158
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 46
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 178
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 183
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 161
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 186
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 108
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 115
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 174
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 192
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 91
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 197
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 153
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 194
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 67
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 77
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 172
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 57
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 49
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 55
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 135
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 69
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 90
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 171
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 47
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 193
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 123
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 117
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 162
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 178.62.200.211:35537 in memory (size: 124.8 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 178.62.200.211:36927 in memory (size: 124.8 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 178.62.210.13:36375 in memory (size: 124.8 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 31.5 KB, free 909.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 911.9 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 178.62.210.13:44585 in memory (size: 124.8 KB, free: 912.0 MB)
20/04/13 15:25:34 INFO spark.SparkContext: Created broadcast 13 from csv at NativeMethodAccessorImpl.java:0
20/04/13 15:25:34 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:25:34 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Got job 6 (csv at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[39] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 15.0 KB, free 909.2 MB)
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 104
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 39
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 151
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 132
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 133
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 182
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 63
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 92
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 121
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 180
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.0 KB, free 909.2 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 178.62.210.13:44585 (size: 8.0 KB, free: 912.0 MB)
20/04/13 15:25:34 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1163
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 8 (MapPartitionsRDD[39] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:25:34 INFO cluster.YarnClusterScheduler: Adding task set 8.0 with 6 tasks
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1458.2 MB)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 32, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 33, 178.62.210.13, executor 2, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 8.0 (TID 34, 178.62.200.211, executor 3, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 8.0 (TID 35, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 8.0 (TID 36, 178.62.210.13, executor 2, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 8.0 (TID 37, 178.62.200.211, executor 3, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 912.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1458.2 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 178.62.210.13:36375 (size: 8.0 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 178.62.200.211:35537 (size: 8.0 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 136
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 75
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 99
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 81
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 178.62.200.211:36927 (size: 8.0 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.210.13:44585 in memory (size: 13.4 KB, free: 912.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.200.211:36927 in memory (size: 13.4 KB, free: 1458.2 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.200.211:35537 in memory (size: 13.4 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 178.62.210.13:36375 in memory (size: 13.4 KB, free: 1458.2 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 196
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 163
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 61
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 188
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 101
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 71
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 82
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 191
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 164
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 107
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 122
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 73
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 65
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 155
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 189
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 149
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 128
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 177
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 102
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 76
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 62
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 58
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 97
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 205
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 59
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 143
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 157
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 79
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 112
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 176
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 40
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 78
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 140
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 187
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 106
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 145
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 200
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 198
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 142
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 148
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 179
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 116
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 137
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 178.62.210.13:36375 in memory (size: 8.0 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 178.62.200.211:35537 in memory (size: 8.0 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 178.62.200.211:36927 in memory (size: 8.0 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 178.62.210.13:44585 in memory (size: 8.0 KB, free: 912.1 MB)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 33) in 267 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 8.0 (TID 37) in 266 ms on 178.62.200.211 (executor 3) (2/6)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 8.0 (TID 36) in 292 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 170
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 72
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 154
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 156
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 83
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 113
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 912.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1458.2 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1458.2 MB)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 32) in 359 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 110
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 207
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned shuffle 0
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 178.62.210.13:44585 in memory (size: 4.6 KB, free: 912.1 MB)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 8.0 (TID 35) in 376 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 8.0 (TID 34) in 389 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:25:34 INFO cluster.YarnClusterScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool 
20/04/13 15:25:34 INFO scheduler.DAGScheduler: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0) finished in 0.398 s
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Job 6 finished: csv at NativeMethodAccessorImpl.java:0, took 0.401216 s
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 178.62.200.211:35537 in memory (size: 4.6 KB, free: 1458.1 MB)
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 85
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 124
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 96
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 167
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 204
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 129
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 54
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 105
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 100
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 134
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 120
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 201
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 64
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 93
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 195
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 173
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 41
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 111
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 131
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 80
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 181
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 202
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 912.2 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1458.2 MB)
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 141
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 166
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 37
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 56
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 208
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 68
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 160
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 152
20/04/13 15:25:34 INFO storage.BlockManager: Removing RDD 21
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned RDD 21
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 185
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 126
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 88
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 139
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 109
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 38
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 206
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 103
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 127
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 146
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 94
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 119
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 175
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 138
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 66
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 150
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 74
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 89
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 147
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 130
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 169
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 184
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 125
20/04/13 15:25:34 INFO spark.ContextCleaner: Cleaned accumulator 199
20/04/13 15:25:34 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:25:34 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:25:34 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Sequence: string>
20/04/13 15:25:34 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 337.3 KB, free 910.0 MB)
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 31.5 KB, free 910.0 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 912.1 MB)
20/04/13 15:25:34 INFO spark.SparkContext: Created broadcast 15 from rdd at CountVectorizer.scala:187
20/04/13 15:25:34 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:25:34 INFO spark.SparkContext: Starting job: count at CountVectorizer.scala:230
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Registering RDD 49 (flatMap at CountVectorizer.scala:205) as input to shuffle 1
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Got job 7 (count at CountVectorizer.scala:230) with 6 output partitions
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (count at CountVectorizer.scala:230)
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[49] at flatMap at CountVectorizer.scala:205), which has no missing parents
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 27.4 KB, free 909.9 MB)
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.4 KB, free 909.9 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 178.62.210.13:44585 (size: 13.4 KB, free: 912.1 MB)
20/04/13 15:25:34 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1163
20/04/13 15:25:34 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[49] at flatMap at CountVectorizer.scala:205) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:25:34 INFO cluster.YarnClusterScheduler: Adding task set 9.0 with 6 tasks
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 38, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 8258 bytes)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 39, 178.62.200.211, executor 1, partition 1, NODE_LOCAL, 8258 bytes)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 9.0 (TID 40, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 8258 bytes)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 9.0 (TID 41, 178.62.200.211, executor 3, partition 3, NODE_LOCAL, 8258 bytes)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 9.0 (TID 42, 178.62.200.211, executor 1, partition 4, NODE_LOCAL, 8258 bytes)
20/04/13 15:25:34 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 9.0 (TID 43, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 8258 bytes)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 178.62.210.13:36375 (size: 13.4 KB, free: 1458.4 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 178.62.200.211:36927 (size: 13.4 KB, free: 1458.4 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 178.62.200.211:35537 (size: 13.4 KB, free: 1458.4 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1458.4 MB)
20/04/13 15:25:34 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1458.4 MB)
20/04/13 15:25:35 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1458.4 MB)
20/04/13 15:25:37 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 9.0 (TID 43) in 2882 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:25:41 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 9.0 (TID 40) in 7036 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:25:46 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 9.0 (TID 42) in 11755 ms on 178.62.200.211 (executor 1) (3/6)
20/04/13 15:25:47 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 39) in 12486 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:25:48 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 9.0 (TID 41) in 13443 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 15:25:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 38) in 14055 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:25:48 INFO cluster.YarnClusterScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool 
20/04/13 15:25:48 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (flatMap at CountVectorizer.scala:205) finished in 14.069 s
20/04/13 15:25:48 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/04/13 15:25:48 INFO scheduler.DAGScheduler: running: Set()
20/04/13 15:25:48 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)
20/04/13 15:25:48 INFO scheduler.DAGScheduler: failed: Set()
20/04/13 15:25:48 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[51] at map at CountVectorizer.scala:223), which has no missing parents
20/04/13 15:25:48 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 3.3 KB, free 909.9 MB)
20/04/13 15:25:48 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 1976.0 B, free 909.9 MB)
20/04/13 15:25:48 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 178.62.210.13:44585 (size: 1976.0 B, free: 912.1 MB)
20/04/13 15:25:48 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1163
20/04/13 15:25:48 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 10 (MapPartitionsRDD[51] at map at CountVectorizer.scala:223) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:25:48 INFO cluster.YarnClusterScheduler: Adding task set 10.0 with 6 tasks
20/04/13 15:25:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 44, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 7651 bytes)
20/04/13 15:25:48 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 45, 178.62.200.211, executor 1, partition 1, NODE_LOCAL, 7651 bytes)
20/04/13 15:25:48 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 10.0 (TID 46, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 7651 bytes)
20/04/13 15:25:48 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 10.0 (TID 47, 178.62.200.211, executor 3, partition 3, NODE_LOCAL, 7651 bytes)
20/04/13 15:25:48 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 10.0 (TID 48, 178.62.200.211, executor 1, partition 4, NODE_LOCAL, 7651 bytes)
20/04/13 15:25:48 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 10.0 (TID 49, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 7651 bytes)
20/04/13 15:25:48 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 178.62.200.211:36927 (size: 1976.0 B, free: 1458.4 MB)
20/04/13 15:25:48 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 178.62.210.13:36375 (size: 1976.0 B, free: 1458.4 MB)
20/04/13 15:25:48 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 178.62.200.211:35537 (size: 1976.0 B, free: 1458.4 MB)
20/04/13 15:25:48 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 178.62.200.211:57158
20/04/13 15:25:49 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 178.62.210.13:38976
20/04/13 15:25:49 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 178.62.200.211:57160
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added rdd_51_5 in memory on 178.62.210.13:36375 (size: 147.2 KB, free: 1458.3 MB)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 10.0 (TID 49) in 122 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added rdd_51_1 in memory on 178.62.200.211:36927 (size: 147.4 KB, free: 1458.3 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added rdd_51_2 in memory on 178.62.210.13:36375 (size: 146.8 KB, free: 1458.1 MB)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 10.0 (TID 46) in 134 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 45) in 152 ms on 178.62.200.211 (executor 1) (3/6)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added rdd_51_4 in memory on 178.62.200.211:36927 (size: 147.7 KB, free: 1458.1 MB)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 10.0 (TID 48) in 182 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added rdd_51_3 in memory on 178.62.200.211:35537 (size: 146.5 KB, free: 1458.3 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added rdd_51_0 in memory on 178.62.200.211:35537 (size: 147.7 KB, free: 1458.1 MB)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 10.0 (TID 47) in 214 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 44) in 217 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:25:49 INFO cluster.YarnClusterScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool 
20/04/13 15:25:49 INFO scheduler.DAGScheduler: ResultStage 10 (count at CountVectorizer.scala:230) finished in 0.232 s
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Job 7 finished: count at CountVectorizer.scala:230, took 14.309608 s
20/04/13 15:25:49 INFO spark.SparkContext: Starting job: top at CountVectorizer.scala:233
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Got job 8 (top at CountVectorizer.scala:233) with 6 output partitions
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (top at CountVectorizer.scala:233)
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[52] at top at CountVectorizer.scala:233), which has no missing parents
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 4.2 KB, free 909.9 MB)
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.4 KB, free 909.9 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 178.62.210.13:44585 (size: 2.4 KB, free: 912.1 MB)
20/04/13 15:25:49 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1163
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 12 (MapPartitionsRDD[52] at top at CountVectorizer.scala:233) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:25:49 INFO cluster.YarnClusterScheduler: Adding task set 12.0 with 6 tasks
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 12.0 (TID 50, 178.62.210.13, executor 2, partition 2, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 12.0 (TID 51, 178.62.200.211, executor 1, partition 1, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 52, 178.62.200.211, executor 3, partition 0, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 12.0 (TID 53, 178.62.210.13, executor 2, partition 5, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 12.0 (TID 54, 178.62.200.211, executor 1, partition 4, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 12.0 (TID 55, 178.62.200.211, executor 3, partition 3, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 178.62.200.211:36927 (size: 2.4 KB, free: 1458.1 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 178.62.200.211:35537 (size: 2.4 KB, free: 1458.1 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 178.62.210.13:36375 (size: 2.4 KB, free: 1458.1 MB)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 12.0 (TID 55) in 40 ms on 178.62.200.211 (executor 3) (1/6)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 12.0 (TID 50) in 45 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 12.0 (TID 53) in 47 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 12.0 (TID 51) in 50 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 52) in 69 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 12.0 (TID 54) in 71 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 15:25:49 INFO cluster.YarnClusterScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool 
20/04/13 15:25:49 INFO scheduler.DAGScheduler: ResultStage 12 (top at CountVectorizer.scala:233) finished in 0.088 s
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Job 8 finished: top at CountVectorizer.scala:233, took 0.092608 s
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 1186.5 KB, free 908.8 MB)
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 77.2 KB, free 908.7 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 178.62.210.13:44585 (size: 77.2 KB, free: 912.0 MB)
20/04/13 15:25:49 INFO spark.SparkContext: Created broadcast 19 from broadcast at CountVectorizer.scala:298
20/04/13 15:25:49 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:25:49 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:25:49 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Entry: string, Entry name: string, Sequence: string ... 1 more fields>
20/04/13 15:25:49 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:25:49 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 337.3 KB, free 908.3 MB)
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 31.5 KB, free 908.3 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 912.0 MB)
20/04/13 15:25:49 INFO spark.SparkContext: Created broadcast 20 from parquet at NativeMethodAccessorImpl.java:0
20/04/13 15:25:49 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:25:49 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Got job 9 (parquet at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (parquet at NativeMethodAccessorImpl.java:0)
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[57] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 347.5 KB, free 908.0 MB)
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 124.7 KB, free 907.9 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 178.62.210.13:44585 (size: 124.7 KB, free: 911.9 MB)
20/04/13 15:25:49 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1163
20/04/13 15:25:49 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 13 (MapPartitionsRDD[57] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:25:49 INFO cluster.YarnClusterScheduler: Adding task set 13.0 with 6 tasks
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 56, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 57, 178.62.200.211, executor 1, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 13.0 (TID 58, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 13.0 (TID 59, 178.62.200.211, executor 3, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 13.0 (TID 60, 178.62.200.211, executor 1, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 13.0 (TID 61, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 178.62.200.211:36927 (size: 124.7 KB, free: 1458.0 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 178.62.210.13:36375 (size: 124.7 KB, free: 1458.0 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 178.62.200.211:35537 (size: 124.7 KB, free: 1458.0 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1458.0 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1458.0 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1458.0 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 178.62.210.13:36375 (size: 77.2 KB, free: 1457.9 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 178.62.200.211:36927 (size: 77.2 KB, free: 1457.9 MB)
20/04/13 15:25:49 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 178.62.200.211:35537 (size: 77.2 KB, free: 1457.9 MB)
20/04/13 15:25:54 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 13.0 (TID 61) in 4764 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:25:59 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 13.0 (TID 58) in 10014 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:26:04 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 13.0 (TID 60) in 14959 ms on 178.62.200.211 (executor 1) (3/6)
20/04/13 15:26:05 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 57) in 15742 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:26:06 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 13.0 (TID 59) in 17108 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 56) in 17521 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:26:07 INFO cluster.YarnClusterScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool 
20/04/13 15:26:07 INFO scheduler.DAGScheduler: ResultStage 13 (parquet at NativeMethodAccessorImpl.java:0) finished in 17.567 s
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Job 9 finished: parquet at NativeMethodAccessorImpl.java:0, took 17.570822 s
20/04/13 15:26:07 INFO datasources.FileFormatWriter: Write Job 4544034a-6d51-4cf4-abd1-1490093c855b committed.
20/04/13 15:26:07 INFO datasources.FileFormatWriter: Finished processing stats for write job 4544034a-6d51-4cf4-abd1-1490093c855b.
20/04/13 15:26:07 INFO datasources.InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/04/13 15:26:07 INFO datasources.InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/04/13 15:26:07 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:26:07 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#120, None)) > 0)
20/04/13 15:26:07 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 15:26:07 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 337.3 KB, free 907.5 MB)
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 31.5 KB, free 907.5 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 911.8 MB)
20/04/13 15:26:07 INFO spark.SparkContext: Created broadcast 22 from csv at NativeMethodAccessorImpl.java:0
20/04/13 15:26:07 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:26:07 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Got job 10 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[63] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 8.9 KB, free 907.5 MB)
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 4.6 KB, free 907.5 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 178.62.210.13:44585 (size: 4.6 KB, free: 911.8 MB)
20/04/13 15:26:07 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1163
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[63] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/04/13 15:26:07 INFO cluster.YarnClusterScheduler: Adding task set 14.0 with 1 tasks
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 62, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 178.62.200.211:36927 (size: 4.6 KB, free: 1457.9 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1457.9 MB)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 62) in 80 ms on 178.62.200.211 (executor 1) (1/1)
20/04/13 15:26:07 INFO cluster.YarnClusterScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool 
20/04/13 15:26:07 INFO scheduler.DAGScheduler: ResultStage 14 (csv at NativeMethodAccessorImpl.java:0) finished in 0.085 s
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Job 10 finished: csv at NativeMethodAccessorImpl.java:0, took 0.089612 s
20/04/13 15:26:07 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:26:07 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:26:07 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 15:26:07 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 337.3 KB, free 907.2 MB)
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 31.5 KB, free 907.1 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 911.8 MB)
20/04/13 15:26:07 INFO spark.SparkContext: Created broadcast 24 from csv at NativeMethodAccessorImpl.java:0
20/04/13 15:26:07 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:26:07 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Got job 11 (csv at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Final stage: ResultStage 15 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[69] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.0 KB, free 907.1 MB)
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.0 KB, free 907.1 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 178.62.210.13:44585 (size: 8.0 KB, free: 911.8 MB)
20/04/13 15:26:07 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1163
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 15 (MapPartitionsRDD[69] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:26:07 INFO cluster.YarnClusterScheduler: Adding task set 15.0 with 6 tasks
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 63, 178.62.210.13, executor 2, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 64, 178.62.200.211, executor 3, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 15.0 (TID 65, 178.62.200.211, executor 1, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 15.0 (TID 66, 178.62.210.13, executor 2, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 15.0 (TID 67, 178.62.200.211, executor 3, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 15.0 (TID 68, 178.62.200.211, executor 1, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 178.62.210.13:36375 (size: 8.0 KB, free: 1457.9 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 178.62.200.211:35537 (size: 8.0 KB, free: 1457.9 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 178.62.200.211:36927 (size: 8.0 KB, free: 1457.8 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1457.9 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1457.9 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1457.8 MB)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 15.0 (TID 68) in 152 ms on 178.62.200.211 (executor 1) (1/6)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 63) in 188 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 15.0 (TID 66) in 203 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 15.0 (TID 65) in 205 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 64) in 208 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 15.0 (TID 67) in 215 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:26:07 INFO cluster.YarnClusterScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool 
20/04/13 15:26:07 INFO scheduler.DAGScheduler: ResultStage 15 (csv at NativeMethodAccessorImpl.java:0) finished in 0.221 s
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Job 11 finished: csv at NativeMethodAccessorImpl.java:0, took 0.224450 s
20/04/13 15:26:07 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:26:07 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:26:07 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Sequence: string>
20/04/13 15:26:07 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 337.3 KB, free 906.8 MB)
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 31.5 KB, free 906.7 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 911.8 MB)
20/04/13 15:26:07 INFO spark.SparkContext: Created broadcast 26 from rdd at CountVectorizer.scala:187
20/04/13 15:26:07 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:26:07 INFO spark.SparkContext: Starting job: count at CountVectorizer.scala:230
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Registering RDD 79 (flatMap at CountVectorizer.scala:205) as input to shuffle 2
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Got job 12 (count at CountVectorizer.scala:230) with 6 output partitions
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (count at CountVectorizer.scala:230)
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 16)
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[79] at flatMap at CountVectorizer.scala:205), which has no missing parents
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 27.4 KB, free 906.7 MB)
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 13.4 KB, free 906.7 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 178.62.210.13:44585 (size: 13.4 KB, free: 911.8 MB)
20/04/13 15:26:07 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1163
20/04/13 15:26:07 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[79] at flatMap at CountVectorizer.scala:205) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:26:07 INFO cluster.YarnClusterScheduler: Adding task set 16.0 with 6 tasks
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 69, 178.62.210.13, executor 2, partition 0, NODE_LOCAL, 8258 bytes)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 16.0 (TID 70, 178.62.200.211, executor 3, partition 1, NODE_LOCAL, 8258 bytes)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 16.0 (TID 71, 178.62.200.211, executor 1, partition 2, NODE_LOCAL, 8258 bytes)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 16.0 (TID 72, 178.62.210.13, executor 2, partition 3, NODE_LOCAL, 8258 bytes)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 16.0 (TID 73, 178.62.200.211, executor 3, partition 4, NODE_LOCAL, 8258 bytes)
20/04/13 15:26:07 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 16.0 (TID 74, 178.62.200.211, executor 1, partition 5, NODE_LOCAL, 8258 bytes)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 178.62.210.13:36375 (size: 13.4 KB, free: 1457.8 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 178.62.200.211:36927 (size: 13.4 KB, free: 1457.8 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 178.62.200.211:35537 (size: 13.4 KB, free: 1457.8 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1457.8 MB)
20/04/13 15:26:07 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1457.8 MB)
20/04/13 15:26:08 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1457.8 MB)
20/04/13 15:26:14 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 16.0 (TID 74) in 6362 ms on 178.62.200.211 (executor 1) (1/6)
20/04/13 15:26:17 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 16.0 (TID 72) in 10025 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:26:18 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 69) in 10577 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 15:26:21 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 16.0 (TID 71) in 13256 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:26:23 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 16.0 (TID 73) in 15650 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 16.0 (TID 70) in 16155 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:26:24 INFO cluster.YarnClusterScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool 
20/04/13 15:26:24 INFO scheduler.DAGScheduler: ShuffleMapStage 16 (flatMap at CountVectorizer.scala:205) finished in 16.170 s
20/04/13 15:26:24 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/04/13 15:26:24 INFO scheduler.DAGScheduler: running: Set()
20/04/13 15:26:24 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 17)
20/04/13 15:26:24 INFO scheduler.DAGScheduler: failed: Set()
20/04/13 15:26:24 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[81] at map at CountVectorizer.scala:223), which has no missing parents
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 3.3 KB, free 906.7 MB)
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 1977.0 B, free 906.7 MB)
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 178.62.210.13:44585 (size: 1977.0 B, free: 911.8 MB)
20/04/13 15:26:24 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1163
20/04/13 15:26:24 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 17 (MapPartitionsRDD[81] at map at CountVectorizer.scala:223) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:26:24 INFO cluster.YarnClusterScheduler: Adding task set 17.0 with 6 tasks
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 75, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 7651 bytes)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 76, 178.62.200.211, executor 3, partition 1, NODE_LOCAL, 7651 bytes)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 17.0 (TID 77, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 7651 bytes)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 17.0 (TID 78, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 7651 bytes)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 17.0 (TID 79, 178.62.200.211, executor 3, partition 4, NODE_LOCAL, 7651 bytes)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 17.0 (TID 80, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 7651 bytes)
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 178.62.200.211:36927 (size: 1977.0 B, free: 1457.8 MB)
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 178.62.200.211:35537 (size: 1977.0 B, free: 1457.8 MB)
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 178.62.210.13:36375 (size: 1977.0 B, free: 1457.8 MB)
20/04/13 15:26:24 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 178.62.200.211:57158
20/04/13 15:26:24 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 178.62.200.211:57160
20/04/13 15:26:24 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 178.62.210.13:38976
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added rdd_81_2 in memory on 178.62.210.13:36375 (size: 2.6 MB, free: 1455.2 MB)
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added rdd_81_5 in memory on 178.62.210.13:36375 (size: 2.6 MB, free: 1452.6 MB)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 17.0 (TID 77) in 412 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 17.0 (TID 80) in 417 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added rdd_81_1 in memory on 178.62.200.211:35537 (size: 2.6 MB, free: 1455.2 MB)
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added rdd_81_4 in memory on 178.62.200.211:35537 (size: 2.6 MB, free: 1452.6 MB)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 76) in 732 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 17.0 (TID 79) in 733 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added rdd_81_3 in memory on 178.62.200.211:36927 (size: 2.6 MB, free: 1455.1 MB)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 17.0 (TID 78) in 744 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added rdd_81_0 in memory on 178.62.200.211:36927 (size: 2.6 MB, free: 1452.5 MB)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 75) in 770 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 15:26:24 INFO cluster.YarnClusterScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool 
20/04/13 15:26:24 INFO scheduler.DAGScheduler: ResultStage 17 (count at CountVectorizer.scala:230) finished in 0.774 s
20/04/13 15:26:24 INFO scheduler.DAGScheduler: Job 12 finished: count at CountVectorizer.scala:230, took 16.950555 s
20/04/13 15:26:24 INFO spark.SparkContext: Starting job: top at CountVectorizer.scala:233
20/04/13 15:26:24 INFO scheduler.DAGScheduler: Got job 13 (top at CountVectorizer.scala:233) with 6 output partitions
20/04/13 15:26:24 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (top at CountVectorizer.scala:233)
20/04/13 15:26:24 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
20/04/13 15:26:24 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:26:24 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[82] at top at CountVectorizer.scala:233), which has no missing parents
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 4.2 KB, free 906.7 MB)
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.4 KB, free 906.7 MB)
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 178.62.210.13:44585 (size: 2.4 KB, free: 911.8 MB)
20/04/13 15:26:24 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1163
20/04/13 15:26:24 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 19 (MapPartitionsRDD[82] at top at CountVectorizer.scala:233) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:26:24 INFO cluster.YarnClusterScheduler: Adding task set 19.0 with 6 tasks
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 19.0 (TID 81, 178.62.210.13, executor 2, partition 2, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 82, 178.62.200.211, executor 1, partition 0, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 19.0 (TID 83, 178.62.200.211, executor 3, partition 1, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 19.0 (TID 84, 178.62.210.13, executor 2, partition 5, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 19.0 (TID 85, 178.62.200.211, executor 1, partition 3, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:26:24 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 19.0 (TID 86, 178.62.200.211, executor 3, partition 4, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 178.62.210.13:36375 (size: 2.4 KB, free: 1452.6 MB)
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 178.62.200.211:36927 (size: 2.4 KB, free: 1452.5 MB)
20/04/13 15:26:24 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 178.62.200.211:35537 (size: 2.4 KB, free: 1452.6 MB)
20/04/13 15:26:25 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 19.0 (TID 83) in 136 ms on 178.62.200.211 (executor 3) (1/6)
20/04/13 15:26:25 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 19.0 (TID 81) in 153 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:26:25 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 19.0 (TID 84) in 237 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 401
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 318
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 435
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 392
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 210
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 233
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 387
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 355
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 434
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 295
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 402
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 332
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned shuffle 1
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 350
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 269
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 341
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 367
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 369
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 378
20/04/13 15:26:25 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 19.0 (TID 86) in 250 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1452.6 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1452.6 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 911.8 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1452.6 MB)
20/04/13 15:26:25 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 82) in 290 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:26:25 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 19.0 (TID 85) in 299 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 15:26:25 INFO cluster.YarnClusterScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool 
20/04/13 15:26:25 INFO scheduler.DAGScheduler: ResultStage 19 (top at CountVectorizer.scala:233) finished in 0.307 s
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 230
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 272
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 251
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 436
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 450
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 279
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 223
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 347
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 260
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 381
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 239
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 384
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 281
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 293
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 220
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 911.8 MB)
20/04/13 15:26:25 INFO scheduler.DAGScheduler: Job 13 finished: top at CountVectorizer.scala:233, took 0.315447 s
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1452.6 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1452.6 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1452.6 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 391
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 296
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 322
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 377
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 240
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 425
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 440
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 324
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 410
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 455
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 447
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 333
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 277
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 426
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 308
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 245
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 257
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 396
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 306
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 469
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 325
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 266
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 326
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 465
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 471
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 442
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 278
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 291
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 178.62.200.211:36927 in memory (size: 4.6 KB, free: 1452.6 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 178.62.210.13:44585 in memory (size: 4.6 KB, free: 911.8 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 364
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 412
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 303
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 273
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 428
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 310
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 407
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 285
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 297
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 292
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 354
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 321
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 432
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 178.62.200.211:36927 in memory (size: 13.4 KB, free: 1452.6 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 178.62.200.211:35537 in memory (size: 13.4 KB, free: 1452.6 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 178.62.210.13:36375 in memory (size: 13.4 KB, free: 1452.6 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 178.62.210.13:44585 in memory (size: 13.4 KB, free: 911.8 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 268
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 301
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 358
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 463
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 225
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 315
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 226
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 373
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 331
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 390
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 280
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 911.9 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1452.6 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 379
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 466
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 261
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 457
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 252
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 338
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 317
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 368
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 178.62.200.211:35537 in memory (size: 124.7 KB, free: 1452.8 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 178.62.200.211:36927 in memory (size: 124.7 KB, free: 1452.8 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 178.62.210.13:36375 in memory (size: 124.7 KB, free: 1452.8 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 178.62.210.13:44585 in memory (size: 124.7 KB, free: 912.0 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 423
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 276
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 372
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 209
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 254
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 328
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 339
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 236
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 351
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 404
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 433
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 255
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 374
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 380
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 382
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 370
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 458
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 316
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 337
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 178.62.200.211:35537 in memory (size: 1977.0 B, free: 1452.8 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 178.62.200.211:36927 in memory (size: 1977.0 B, free: 1452.8 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 178.62.210.13:44585 in memory (size: 1977.0 B, free: 912.0 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 178.62.210.13:36375 in memory (size: 1977.0 B, free: 1452.8 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 246
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 178.62.210.13:44585 in memory (size: 77.2 KB, free: 912.1 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 178.62.200.211:36927 in memory (size: 77.2 KB, free: 1452.8 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 178.62.200.211:35537 in memory (size: 77.2 KB, free: 1452.8 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 178.62.210.13:36375 in memory (size: 77.2 KB, free: 1452.8 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 413
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 178.62.200.211:36927 in memory (size: 2.4 KB, free: 1452.8 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 178.62.200.211:35537 in memory (size: 2.4 KB, free: 1452.9 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 178.62.210.13:44585 in memory (size: 2.4 KB, free: 912.1 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 178.62.210.13:36375 in memory (size: 2.4 KB, free: 1452.8 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 345
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 405
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 221
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 319
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 399
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 307
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 248
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 409
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 411
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 437
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 467
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 444
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 235
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 376
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 453
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 217
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 259
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 443
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 309
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 389
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 448
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 241
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 323
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 445
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 459
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 340
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 289
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 320
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 365
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 242
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 385
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 359
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 270
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 244
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 327
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 290
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 330
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 265
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 231
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 311
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 430
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 400
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 219
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 247
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 298
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 224
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 227
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 215
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 431
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 451
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 211
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 305
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 383
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 366
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 299
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 229
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 456
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 352
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 314
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 258
20/04/13 15:26:25 INFO storage.BlockManager: Removing RDD 51
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned RDD 51
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 329
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 363
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 449
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 468
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 178.62.210.13:44585 in memory (size: 1976.0 B, free: 912.1 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 178.62.200.211:35537 in memory (size: 1976.0 B, free: 1453.1 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 178.62.200.211:36927 in memory (size: 1976.0 B, free: 1453.1 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 178.62.210.13:36375 in memory (size: 1976.0 B, free: 1453.1 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 371
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 271
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 446
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 288
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 334
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 348
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 415
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 462
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 393
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 250
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 375
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 362
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 287
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 349
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 262
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 234
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 302
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 212
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 256
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 452
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 427
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 360
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 912.1 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 286
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 460
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 403
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 408
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 300
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 312
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 438
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 249
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 912.1 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 336
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 343
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 406
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 238
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 398
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 461
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 283
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 335
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 213
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 395
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 357
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 214
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 237
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 304
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 178.62.200.211:35537 in memory (size: 13.4 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 178.62.200.211:36927 in memory (size: 13.4 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 178.62.210.13:36375 in memory (size: 13.4 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 178.62.210.13:44585 in memory (size: 13.4 KB, free: 912.1 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 353
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 439
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 454
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 294
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 216
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 263
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 356
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 342
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 222
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 253
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 424
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 274
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 394
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 284
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 386
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 397
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 313
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 267
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 282
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 178.62.210.13:44585 in memory (size: 8.0 KB, free: 912.2 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 178.62.200.211:35537 in memory (size: 8.0 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 178.62.200.211:36927 in memory (size: 8.0 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 178.62.210.13:36375 in memory (size: 8.0 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 361
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 422
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 218
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 232
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 470
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 441
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 275
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 178.62.210.13:36375 in memory (size: 8.0 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 178.62.200.211:35537 in memory (size: 8.0 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 178.62.200.211:36927 in memory (size: 8.0 KB, free: 1453.2 MB)
20/04/13 15:26:25 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 178.62.210.13:44585 in memory (size: 8.0 KB, free: 912.2 MB)
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 464
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 388
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 264
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 346
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 414
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 243
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 429
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 228
20/04/13 15:26:25 INFO spark.ContextCleaner: Cleaned accumulator 344
20/04/13 15:26:26 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 22.6 MB, free 887.7 MB)
20/04/13 15:26:26 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 1770.3 KB, free 886.0 MB)
20/04/13 15:26:26 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 178.62.210.13:44585 (size: 1770.3 KB, free: 910.4 MB)
20/04/13 15:26:26 INFO spark.SparkContext: Created broadcast 30 from broadcast at CountVectorizer.scala:298
20/04/13 15:26:26 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:26:26 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:26:26 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Entry: string, Entry name: string, Sequence: string ... 1 more fields>
20/04/13 15:26:26 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:26:26 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:26 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:26 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:26 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:26 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:26 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:26 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:26 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 337.3 KB, free 885.7 MB)
20/04/13 15:26:26 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 31.5 KB, free 885.7 MB)
20/04/13 15:26:26 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 910.4 MB)
20/04/13 15:26:26 INFO spark.SparkContext: Created broadcast 31 from parquet at NativeMethodAccessorImpl.java:0
20/04/13 15:26:26 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:26:26 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
20/04/13 15:26:26 INFO scheduler.DAGScheduler: Got job 14 (parquet at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 15:26:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0)
20/04/13 15:26:26 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:26:26 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:26:26 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[87] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:26:26 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 1883.3 KB, free 883.8 MB)
20/04/13 15:26:26 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 1062.5 KB, free 882.8 MB)
20/04/13 15:26:26 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 178.62.210.13:44585 (size: 1062.5 KB, free: 909.4 MB)
20/04/13 15:26:26 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1163
20/04/13 15:26:26 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 20 (MapPartitionsRDD[87] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:26:26 INFO cluster.YarnClusterScheduler: Adding task set 20.0 with 6 tasks
20/04/13 15:26:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 87, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:26 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 20.0 (TID 88, 178.62.210.13, executor 2, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:26 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 20.0 (TID 89, 178.62.200.211, executor 3, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:26 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 20.0 (TID 90, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:26 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 20.0 (TID 91, 178.62.210.13, executor 2, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:26 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 20.0 (TID 92, 178.62.200.211, executor 3, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:26 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 178.62.210.13:36375 (size: 1062.5 KB, free: 1452.2 MB)
20/04/13 15:26:26 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 178.62.200.211:35537 (size: 1062.5 KB, free: 1452.2 MB)
20/04/13 15:26:27 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 178.62.200.211:36927 (size: 1062.5 KB, free: 1452.2 MB)
20/04/13 15:26:27 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1452.2 MB)
20/04/13 15:26:27 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 178.62.210.13:36375 (size: 1770.3 KB, free: 1450.4 MB)
20/04/13 15:26:27 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1452.1 MB)
20/04/13 15:26:27 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1452.2 MB)
20/04/13 15:26:27 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 178.62.200.211:35537 (size: 1770.3 KB, free: 1450.4 MB)
20/04/13 15:26:27 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 178.62.200.211:36927 (size: 1770.3 KB, free: 1450.4 MB)
20/04/13 15:26:36 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 20.0 (TID 92) in 9077 ms on 178.62.200.211 (executor 3) (1/6)
20/04/13 15:26:39 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 20.0 (TID 91) in 12538 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:26:40 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 20.0 (TID 88) in 13377 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 15:26:43 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 20.0 (TID 90) in 16087 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:26:44 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 87) in 17883 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 20.0 (TID 89) in 19092 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:26:46 INFO cluster.YarnClusterScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool 
20/04/13 15:26:46 INFO scheduler.DAGScheduler: ResultStage 20 (parquet at NativeMethodAccessorImpl.java:0) finished in 19.191 s
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Job 14 finished: parquet at NativeMethodAccessorImpl.java:0, took 19.193562 s
20/04/13 15:26:46 INFO datasources.FileFormatWriter: Write Job dd32cfdb-e913-489e-8895-390253139fff committed.
20/04/13 15:26:46 INFO datasources.FileFormatWriter: Finished processing stats for write job dd32cfdb-e913-489e-8895-390253139fff.
20/04/13 15:26:46 INFO datasources.InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/04/13 15:26:46 INFO datasources.InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/04/13 15:26:46 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:26:46 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#180, None)) > 0)
20/04/13 15:26:46 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 15:26:46 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 337.3 KB, free 882.5 MB)
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 31.5 KB, free 882.4 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 909.3 MB)
20/04/13 15:26:46 INFO spark.SparkContext: Created broadcast 33 from csv at NativeMethodAccessorImpl.java:0
20/04/13 15:26:46 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:26:46 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Got job 15 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 21 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[93] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 8.9 KB, free 882.4 MB)
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.6 KB, free 882.4 MB)
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 523
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 513
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 492
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 178.62.210.13:44585 (size: 4.6 KB, free: 909.3 MB)
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 510
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 479
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 417
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 504
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 505
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 496
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 509
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 508
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1450.5 MB)
20/04/13 15:26:46 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1163
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1450.5 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1450.4 MB)
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[93] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/04/13 15:26:46 INFO cluster.YarnClusterScheduler: Adding task set 21.0 with 1 tasks
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 93, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 909.4 MB)
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 499
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 420
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 418
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 485
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 515
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 483
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 419
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 498
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 503
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 421
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 493
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 178.62.200.211:36927 (size: 4.6 KB, free: 1455.7 MB)
20/04/13 15:26:46 INFO storage.BlockManager: Removing RDD 81
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned RDD 81
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 491
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 482
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 490
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 474
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 500
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 526
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 530
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 524
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 473
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 480
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 416
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned shuffle 2
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 489
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 520
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 494
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 517
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 527
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 516
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1455.7 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 178.62.210.13:44585 in memory (size: 2.4 KB, free: 909.4 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 178.62.210.13:36375 in memory (size: 2.4 KB, free: 1455.7 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 178.62.200.211:36927 in memory (size: 2.4 KB, free: 1455.7 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 178.62.200.211:35537 in memory (size: 2.4 KB, free: 1455.7 MB)
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 476
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 909.4 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1455.7 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1455.7 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1455.7 MB)
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 507
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 518
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 501
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 522
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 487
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 512
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 475
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 481
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 472
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 525
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 506
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 519
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 478
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 495
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 484
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 488
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 178.62.210.13:44585 in memory (size: 1062.5 KB, free: 910.4 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 178.62.210.13:36375 in memory (size: 1062.5 KB, free: 1456.8 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 178.62.200.211:35537 in memory (size: 1062.5 KB, free: 1456.8 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 178.62.200.211:36927 in memory (size: 1062.5 KB, free: 1456.7 MB)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 93) in 49 ms on 178.62.200.211 (executor 1) (1/1)
20/04/13 15:26:46 INFO cluster.YarnClusterScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool 
20/04/13 15:26:46 INFO scheduler.DAGScheduler: ResultStage 21 (csv at NativeMethodAccessorImpl.java:0) finished in 0.142 s
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Job 15 finished: csv at NativeMethodAccessorImpl.java:0, took 0.145468 s
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 528
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 521
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 514
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 486
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 502
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 511
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 477
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 529
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 531
20/04/13 15:26:46 INFO spark.ContextCleaner: Cleaned accumulator 497
20/04/13 15:26:46 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:26:46 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:26:46 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 15:26:46 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 337.3 KB, free 885.7 MB)
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 31.5 KB, free 885.7 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 910.4 MB)
20/04/13 15:26:46 INFO spark.SparkContext: Created broadcast 35 from csv at NativeMethodAccessorImpl.java:0
20/04/13 15:26:46 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:26:46 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Got job 16 (csv at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[99] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 15.0 KB, free 885.6 MB)
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 8.0 KB, free 885.6 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 178.62.210.13:44585 (size: 8.0 KB, free: 910.4 MB)
20/04/13 15:26:46 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1163
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 22 (MapPartitionsRDD[99] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:26:46 INFO cluster.YarnClusterScheduler: Adding task set 22.0 with 6 tasks
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 94, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 22.0 (TID 95, 178.62.210.13, executor 2, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 22.0 (TID 96, 178.62.200.211, executor 3, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 22.0 (TID 97, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 22.0 (TID 98, 178.62.210.13, executor 2, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 22.0 (TID 99, 178.62.200.211, executor 3, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 178.62.210.13:36375 (size: 8.0 KB, free: 1456.8 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 178.62.200.211:35537 (size: 8.0 KB, free: 1456.8 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 178.62.200.211:36927 (size: 8.0 KB, free: 1456.7 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1456.7 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1456.7 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1456.7 MB)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 22.0 (TID 99) in 114 ms on 178.62.200.211 (executor 3) (1/6)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 22.0 (TID 98) in 163 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 22.0 (TID 95) in 164 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 94) in 191 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 22.0 (TID 97) in 211 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 22.0 (TID 96) in 213 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:26:46 INFO cluster.YarnClusterScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool 
20/04/13 15:26:46 INFO scheduler.DAGScheduler: ResultStage 22 (csv at NativeMethodAccessorImpl.java:0) finished in 0.220 s
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Job 16 finished: csv at NativeMethodAccessorImpl.java:0, took 0.222133 s
20/04/13 15:26:46 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:26:46 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:26:46 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Sequence: string>
20/04/13 15:26:46 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 337.3 KB, free 885.3 MB)
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 31.5 KB, free 885.3 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 910.4 MB)
20/04/13 15:26:46 INFO spark.SparkContext: Created broadcast 37 from rdd at CountVectorizer.scala:187
20/04/13 15:26:46 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:26:46 INFO spark.SparkContext: Starting job: count at CountVectorizer.scala:230
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Registering RDD 109 (flatMap at CountVectorizer.scala:205) as input to shuffle 3
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Got job 17 (count at CountVectorizer.scala:230) with 6 output partitions
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (count at CountVectorizer.scala:230)
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 23)
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[109] at flatMap at CountVectorizer.scala:205), which has no missing parents
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 27.4 KB, free 885.2 MB)
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 13.4 KB, free 885.2 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 178.62.210.13:44585 (size: 13.4 KB, free: 910.3 MB)
20/04/13 15:26:46 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1163
20/04/13 15:26:46 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[109] at flatMap at CountVectorizer.scala:205) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:26:46 INFO cluster.YarnClusterScheduler: Adding task set 23.0 with 6 tasks
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 100, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 8258 bytes)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 23.0 (TID 101, 178.62.200.211, executor 1, partition 1, NODE_LOCAL, 8258 bytes)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 23.0 (TID 102, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 8258 bytes)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 23.0 (TID 103, 178.62.200.211, executor 3, partition 3, NODE_LOCAL, 8258 bytes)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 23.0 (TID 104, 178.62.200.211, executor 1, partition 4, NODE_LOCAL, 8258 bytes)
20/04/13 15:26:46 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 23.0 (TID 105, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 8258 bytes)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 178.62.200.211:35537 (size: 13.4 KB, free: 1456.7 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 178.62.200.211:36927 (size: 13.4 KB, free: 1456.7 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 178.62.210.13:36375 (size: 13.4 KB, free: 1456.7 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1456.7 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1456.6 MB)
20/04/13 15:26:46 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1456.7 MB)
20/04/13 15:26:51 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 23.0 (TID 105) in 4985 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:26:57 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 23.0 (TID 102) in 10485 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:27:00 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 23.0 (TID 104) in 13975 ms on 178.62.200.211 (executor 1) (3/6)
20/04/13 15:27:01 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 23.0 (TID 101) in 14941 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:27:03 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 23.0 (TID 103) in 16197 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 15:27:03 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 100) in 17056 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:27:03 INFO cluster.YarnClusterScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool 
20/04/13 15:27:03 INFO scheduler.DAGScheduler: ShuffleMapStage 23 (flatMap at CountVectorizer.scala:205) finished in 17.077 s
20/04/13 15:27:03 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/04/13 15:27:03 INFO scheduler.DAGScheduler: running: Set()
20/04/13 15:27:03 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 24)
20/04/13 15:27:03 INFO scheduler.DAGScheduler: failed: Set()
20/04/13 15:27:03 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[111] at map at CountVectorizer.scala:223), which has no missing parents
20/04/13 15:27:03 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 3.3 KB, free 885.2 MB)
20/04/13 15:27:03 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 1977.0 B, free 885.2 MB)
20/04/13 15:27:03 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 178.62.210.13:44585 (size: 1977.0 B, free: 910.3 MB)
20/04/13 15:27:03 INFO spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1163
20/04/13 15:27:03 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 24 (MapPartitionsRDD[111] at map at CountVectorizer.scala:223) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:27:03 INFO cluster.YarnClusterScheduler: Adding task set 24.0 with 6 tasks
20/04/13 15:27:03 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 106, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 7651 bytes)
20/04/13 15:27:03 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 24.0 (TID 107, 178.62.210.13, executor 2, partition 1, NODE_LOCAL, 7651 bytes)
20/04/13 15:27:03 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 24.0 (TID 108, 178.62.200.211, executor 1, partition 2, NODE_LOCAL, 7651 bytes)
20/04/13 15:27:03 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 24.0 (TID 109, 178.62.200.211, executor 3, partition 3, NODE_LOCAL, 7651 bytes)
20/04/13 15:27:03 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 24.0 (TID 110, 178.62.210.13, executor 2, partition 4, NODE_LOCAL, 7651 bytes)
20/04/13 15:27:03 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 24.0 (TID 111, 178.62.200.211, executor 1, partition 5, NODE_LOCAL, 7651 bytes)
20/04/13 15:27:03 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 178.62.210.13:36375 (size: 1977.0 B, free: 1456.7 MB)
20/04/13 15:27:03 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 178.62.200.211:35537 (size: 1977.0 B, free: 1456.7 MB)
20/04/13 15:27:03 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 178.62.200.211:36927 (size: 1977.0 B, free: 1456.6 MB)
20/04/13 15:27:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 178.62.200.211:57160
20/04/13 15:27:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 178.62.200.211:57158
20/04/13 15:27:03 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 178.62.210.13:38976
20/04/13 15:27:04 INFO storage.BlockManagerInfo: Added rdd_111_4 in memory on 178.62.210.13:36375 (size: 2.6 MB, free: 1454.0 MB)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 24.0 (TID 110) in 371 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:27:04 INFO storage.BlockManagerInfo: Added rdd_111_1 in memory on 178.62.210.13:36375 (size: 2.6 MB, free: 1451.4 MB)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 24.0 (TID 107) in 384 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:27:04 INFO storage.BlockManagerInfo: Added rdd_111_3 in memory on 178.62.200.211:35537 (size: 2.6 MB, free: 1454.0 MB)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 24.0 (TID 109) in 494 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 15:27:04 INFO storage.BlockManagerInfo: Added rdd_111_0 in memory on 178.62.200.211:35537 (size: 2.6 MB, free: 1451.4 MB)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 106) in 514 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 15:27:04 INFO storage.BlockManagerInfo: Added rdd_111_2 in memory on 178.62.200.211:36927 (size: 2.6 MB, free: 1454.0 MB)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 24.0 (TID 108) in 528 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:27:04 INFO storage.BlockManagerInfo: Added rdd_111_5 in memory on 178.62.200.211:36927 (size: 2.6 MB, free: 1451.4 MB)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 24.0 (TID 111) in 541 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 15:27:04 INFO cluster.YarnClusterScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool 
20/04/13 15:27:04 INFO scheduler.DAGScheduler: ResultStage 24 (count at CountVectorizer.scala:230) finished in 0.547 s
20/04/13 15:27:04 INFO scheduler.DAGScheduler: Job 17 finished: count at CountVectorizer.scala:230, took 17.630076 s
20/04/13 15:27:04 INFO spark.SparkContext: Starting job: top at CountVectorizer.scala:233
20/04/13 15:27:04 INFO scheduler.DAGScheduler: Got job 18 (top at CountVectorizer.scala:233) with 6 output partitions
20/04/13 15:27:04 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (top at CountVectorizer.scala:233)
20/04/13 15:27:04 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
20/04/13 15:27:04 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:27:04 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[112] at top at CountVectorizer.scala:233), which has no missing parents
20/04/13 15:27:04 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 4.2 KB, free 885.2 MB)
20/04/13 15:27:04 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 2.4 KB, free 885.2 MB)
20/04/13 15:27:04 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 178.62.210.13:44585 (size: 2.4 KB, free: 910.3 MB)
20/04/13 15:27:04 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1163
20/04/13 15:27:04 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 26 (MapPartitionsRDD[112] at top at CountVectorizer.scala:233) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:27:04 INFO cluster.YarnClusterScheduler: Adding task set 26.0 with 6 tasks
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 26.0 (TID 112, 178.62.210.13, executor 2, partition 1, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 113, 178.62.200.211, executor 3, partition 0, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 26.0 (TID 114, 178.62.200.211, executor 1, partition 2, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 26.0 (TID 115, 178.62.210.13, executor 2, partition 4, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 26.0 (TID 116, 178.62.200.211, executor 3, partition 3, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 26.0 (TID 117, 178.62.200.211, executor 1, partition 5, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:27:04 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 178.62.210.13:36375 (size: 2.4 KB, free: 1451.4 MB)
20/04/13 15:27:04 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 178.62.200.211:35537 (size: 2.4 KB, free: 1451.4 MB)
20/04/13 15:27:04 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 178.62.200.211:36927 (size: 2.4 KB, free: 1451.4 MB)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 26.0 (TID 115) in 94 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 26.0 (TID 112) in 107 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 26.0 (TID 116) in 146 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 113) in 164 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 26.0 (TID 117) in 165 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:27:04 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 26.0 (TID 114) in 167 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 15:27:04 INFO cluster.YarnClusterScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool 
20/04/13 15:27:04 INFO scheduler.DAGScheduler: ResultStage 26 (top at CountVectorizer.scala:233) finished in 0.202 s
20/04/13 15:27:04 INFO scheduler.DAGScheduler: Job 18 finished: top at CountVectorizer.scala:233, took 0.213448 s
20/04/13 15:27:05 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 22.6 MB, free 862.6 MB)
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 1770.4 KB, free 860.9 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 178.62.210.13:44585 (size: 1770.4 KB, free: 908.6 MB)
20/04/13 15:27:06 INFO spark.SparkContext: Created broadcast 41 from broadcast at CountVectorizer.scala:298
20/04/13 15:27:06 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:27:06 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:27:06 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Entry: string, Entry name: string, Sequence: string ... 1 more fields>
20/04/13 15:27:06 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:27:06 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 337.3 KB, free 860.6 MB)
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 557
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 657
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 667
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 597
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 645
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 625
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 626
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 570
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 672
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 630
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 580
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 666
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 553
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 581
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 178.62.210.13:44585 in memory (size: 13.4 KB, free: 908.6 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 178.62.200.211:36927 in memory (size: 13.4 KB, free: 1451.4 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 178.62.200.211:35537 in memory (size: 13.4 KB, free: 1451.4 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 178.62.210.13:36375 in memory (size: 13.4 KB, free: 1451.5 MB)
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 614
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 571
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 575
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 595
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 547
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 655
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 548
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 563
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 572
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 635
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 607
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 640
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 624
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1451.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 908.7 MB)
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 31.5 KB, free 860.6 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 908.6 MB)
20/04/13 15:27:06 INFO spark.SparkContext: Created broadcast 42 from parquet at NativeMethodAccessorImpl.java:0
20/04/13 15:27:06 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 637
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 600
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 604
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 634
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 599
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 605
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 574
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 561
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 619
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 612
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 551
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 533
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 668
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 609
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 610
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 652
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 632
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 578
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 670
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 541
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 576
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 564
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 660
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 582
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 602
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 654
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 569
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 178.62.210.13:44585 in memory (size: 1770.3 KB, free: 910.4 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 178.62.200.211:35537 in memory (size: 1770.3 KB, free: 1453.2 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 178.62.200.211:36927 in memory (size: 1770.3 KB, free: 1453.2 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 178.62.210.13:36375 in memory (size: 1770.3 KB, free: 1453.2 MB)
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 615
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 549
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 611
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 659
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 606
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 535
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 562
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 649
20/04/13 15:27:06 INFO storage.BlockManager: Removing RDD 111
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned RDD 111
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 664
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 537
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 573
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 617
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 178.62.200.211:35537 in memory (size: 8.0 KB, free: 1458.4 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 178.62.210.13:44585 in memory (size: 8.0 KB, free: 910.4 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 178.62.200.211:36927 in memory (size: 8.0 KB, free: 1458.4 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 178.62.210.13:36375 in memory (size: 8.0 KB, free: 1458.4 MB)
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 543
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 627
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 596
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 567
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 542
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 613
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 662
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 544
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 538
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1458.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 910.4 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1458.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1458.5 MB)
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 673
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 579
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 587
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 591
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 593
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned shuffle 3
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 643
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1458.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1458.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1458.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 910.4 MB)
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 594
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 589
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 650
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 623
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 550
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 584
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 178.62.200.211:36927 in memory (size: 1977.0 B, free: 1458.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 178.62.200.211:35537 in memory (size: 1977.0 B, free: 1458.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 178.62.210.13:44585 in memory (size: 1977.0 B, free: 910.4 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 178.62.210.13:36375 in memory (size: 1977.0 B, free: 1458.5 MB)
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 546
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 585
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 618
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 639
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 631
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 669
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 554
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 665
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 656
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 566
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 568
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 651
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 638
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 558
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 559
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 661
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 539
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 577
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 616
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 633
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 663
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 592
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 653
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 565
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 540
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 583
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 178.62.200.211:35537 in memory (size: 2.4 KB, free: 1458.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 178.62.200.211:36927 in memory (size: 2.4 KB, free: 1458.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 178.62.210.13:36375 in memory (size: 2.4 KB, free: 1458.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 178.62.210.13:44585 in memory (size: 2.4 KB, free: 910.4 MB)
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 586
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 608
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 658
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 536
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 636
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 641
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 648
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 628
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 642
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 647
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 532
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 552
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 545
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 603
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 629
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 601
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 560
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 671
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 556
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 555
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 598
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 646
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 620
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 534
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 621
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 178.62.200.211:36927 in memory (size: 4.6 KB, free: 1458.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 178.62.210.13:44585 in memory (size: 4.6 KB, free: 910.4 MB)
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 644
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 622
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 590
20/04/13 15:27:06 INFO spark.ContextCleaner: Cleaned accumulator 588
20/04/13 15:27:06 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
20/04/13 15:27:06 INFO scheduler.DAGScheduler: Got job 19 (parquet at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 15:27:06 INFO scheduler.DAGScheduler: Final stage: ResultStage 27 (parquet at NativeMethodAccessorImpl.java:0)
20/04/13 15:27:06 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:27:06 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:27:06 INFO scheduler.DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[117] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 1883.3 KB, free 884.2 MB)
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 1062.2 KB, free 883.2 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 178.62.210.13:44585 (size: 1062.2 KB, free: 909.4 MB)
20/04/13 15:27:06 INFO spark.SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1163
20/04/13 15:27:06 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 27 (MapPartitionsRDD[117] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:27:06 INFO cluster.YarnClusterScheduler: Adding task set 27.0 with 6 tasks
20/04/13 15:27:06 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 118, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:27:06 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 27.0 (TID 119, 178.62.210.13, executor 2, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 15:27:06 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 27.0 (TID 120, 178.62.200.211, executor 1, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 15:27:06 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 27.0 (TID 121, 178.62.200.211, executor 3, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 15:27:06 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 27.0 (TID 122, 178.62.210.13, executor 2, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 15:27:06 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 27.0 (TID 123, 178.62.200.211, executor 1, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 178.62.210.13:36375 (size: 1062.2 KB, free: 1457.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 178.62.200.211:36927 (size: 1062.2 KB, free: 1457.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 178.62.200.211:35537 (size: 1062.2 KB, free: 1457.5 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1457.4 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 178.62.210.13:36375 (size: 1770.4 KB, free: 1455.7 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1457.4 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1457.4 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 178.62.200.211:35537 (size: 1770.4 KB, free: 1455.7 MB)
20/04/13 15:27:06 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 178.62.200.211:36927 (size: 1770.4 KB, free: 1455.7 MB)
20/04/13 15:27:14 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 27.0 (TID 123) in 7916 ms on 178.62.200.211 (executor 1) (1/6)
20/04/13 15:27:19 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 27.0 (TID 122) in 12718 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:27:19 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 27.0 (TID 119) in 13500 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 15:27:22 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 27.0 (TID 120) in 15833 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:27:24 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 27.0 (TID 121) in 17952 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 118) in 19671 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:27:26 INFO cluster.YarnClusterScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool 
20/04/13 15:27:26 INFO scheduler.DAGScheduler: ResultStage 27 (parquet at NativeMethodAccessorImpl.java:0) finished in 19.761 s
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Job 19 finished: parquet at NativeMethodAccessorImpl.java:0, took 19.763581 s
20/04/13 15:27:26 INFO datasources.FileFormatWriter: Write Job da348508-b0e8-4f48-9bfd-db3de03b7eea committed.
20/04/13 15:27:26 INFO datasources.FileFormatWriter: Finished processing stats for write job da348508-b0e8-4f48-9bfd-db3de03b7eea.
20/04/13 15:27:26 INFO datasources.InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.
20/04/13 15:27:26 INFO datasources.InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/04/13 15:27:26 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:27:26 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#240, None)) > 0)
20/04/13 15:27:26 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 15:27:26 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 337.3 KB, free 882.8 MB)
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 31.5 KB, free 882.8 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 909.4 MB)
20/04/13 15:27:26 INFO spark.SparkContext: Created broadcast 44 from csv at NativeMethodAccessorImpl.java:0
20/04/13 15:27:26 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:27:26 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Got job 20 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[123] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 8.9 KB, free 882.8 MB)
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 4.6 KB, free 882.8 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 178.62.210.13:44585 (size: 4.6 KB, free: 909.4 MB)
20/04/13 15:27:26 INFO spark.SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1163
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[123] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/04/13 15:27:26 INFO cluster.YarnClusterScheduler: Adding task set 28.0 with 1 tasks
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 124, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 178.62.200.211:35537 (size: 4.6 KB, free: 1455.7 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1455.7 MB)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 124) in 37 ms on 178.62.200.211 (executor 3) (1/1)
20/04/13 15:27:26 INFO cluster.YarnClusterScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool 
20/04/13 15:27:26 INFO scheduler.DAGScheduler: ResultStage 28 (csv at NativeMethodAccessorImpl.java:0) finished in 0.044 s
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Job 20 finished: csv at NativeMethodAccessorImpl.java:0, took 0.047431 s
20/04/13 15:27:26 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:27:26 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:27:26 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 15:27:26 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 337.3 KB, free 882.4 MB)
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 31.5 KB, free 882.4 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 909.3 MB)
20/04/13 15:27:26 INFO spark.SparkContext: Created broadcast 46 from csv at NativeMethodAccessorImpl.java:0
20/04/13 15:27:26 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:27:26 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Got job 21 (csv at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 29 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[129] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 15.0 KB, free 882.4 MB)
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 8.0 KB, free 882.4 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 178.62.210.13:44585 (size: 8.0 KB, free: 909.3 MB)
20/04/13 15:27:26 INFO spark.SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1163
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 29 (MapPartitionsRDD[129] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:27:26 INFO cluster.YarnClusterScheduler: Adding task set 29.0 with 6 tasks
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 125, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 29.0 (TID 126, 178.62.210.13, executor 2, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 29.0 (TID 127, 178.62.200.211, executor 3, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 29.0 (TID 128, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 29.0 (TID 129, 178.62.210.13, executor 2, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 29.0 (TID 130, 178.62.200.211, executor 3, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 178.62.210.13:36375 (size: 8.0 KB, free: 1455.7 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 178.62.200.211:36927 (size: 8.0 KB, free: 1455.7 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 178.62.200.211:35537 (size: 8.0 KB, free: 1455.7 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1455.7 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1455.7 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1455.6 MB)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 29.0 (TID 130) in 131 ms on 178.62.200.211 (executor 3) (1/6)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 29.0 (TID 126) in 144 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 29.0 (TID 129) in 163 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 29.0 (TID 128) in 163 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 125) in 175 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 29.0 (TID 127) in 184 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:27:26 INFO cluster.YarnClusterScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool 
20/04/13 15:27:26 INFO scheduler.DAGScheduler: ResultStage 29 (csv at NativeMethodAccessorImpl.java:0) finished in 0.189 s
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Job 21 finished: csv at NativeMethodAccessorImpl.java:0, took 0.190516 s
20/04/13 15:27:26 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:27:26 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:27:26 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Sequence: string>
20/04/13 15:27:26 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 337.3 KB, free 882.1 MB)
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 31.5 KB, free 882.0 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 909.3 MB)
20/04/13 15:27:26 INFO spark.SparkContext: Created broadcast 48 from rdd at CountVectorizer.scala:187
20/04/13 15:27:26 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:27:26 INFO spark.SparkContext: Starting job: count at CountVectorizer.scala:230
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Registering RDD 139 (flatMap at CountVectorizer.scala:205) as input to shuffle 4
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Got job 22 (count at CountVectorizer.scala:230) with 6 output partitions
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (count at CountVectorizer.scala:230)
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 30)
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[139] at flatMap at CountVectorizer.scala:205), which has no missing parents
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 27.4 KB, free 882.0 MB)
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 13.4 KB, free 882.0 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 178.62.210.13:44585 (size: 13.4 KB, free: 909.3 MB)
20/04/13 15:27:26 INFO spark.SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1163
20/04/13 15:27:26 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[139] at flatMap at CountVectorizer.scala:205) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:27:26 INFO cluster.YarnClusterScheduler: Adding task set 30.0 with 6 tasks
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 30.0 (TID 131, 178.62.210.13, executor 2, partition 0, NODE_LOCAL, 8258 bytes)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 30.0 (TID 132, 178.62.200.211, executor 1, partition 1, NODE_LOCAL, 8258 bytes)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 30.0 (TID 133, 178.62.200.211, executor 3, partition 2, NODE_LOCAL, 8258 bytes)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 30.0 (TID 134, 178.62.210.13, executor 2, partition 3, NODE_LOCAL, 8258 bytes)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 30.0 (TID 135, 178.62.200.211, executor 1, partition 4, NODE_LOCAL, 8258 bytes)
20/04/13 15:27:26 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 30.0 (TID 136, 178.62.200.211, executor 3, partition 5, NODE_LOCAL, 8258 bytes)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 178.62.210.13:36375 (size: 13.4 KB, free: 1455.6 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 178.62.200.211:35537 (size: 13.4 KB, free: 1455.6 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 178.62.200.211:36927 (size: 13.4 KB, free: 1455.6 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1455.6 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1455.6 MB)
20/04/13 15:27:26 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1455.6 MB)
20/04/13 15:27:39 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 30.0 (TID 136) in 12687 ms on 178.62.200.211 (executor 3) (1/6)
20/04/13 15:27:47 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 30.0 (TID 134) in 20313 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:27:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 30.0 (TID 131) in 21884 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 15:27:54 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 30.0 (TID 135) in 27318 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:27:54 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 30.0 (TID 133) in 27473 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 15:27:55 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 30.0 (TID 132) in 28292 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 15:27:55 INFO cluster.YarnClusterScheduler: Removed TaskSet 30.0, whose tasks have all completed, from pool 
20/04/13 15:27:55 INFO scheduler.DAGScheduler: ShuffleMapStage 30 (flatMap at CountVectorizer.scala:205) finished in 28.309 s
20/04/13 15:27:55 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/04/13 15:27:55 INFO scheduler.DAGScheduler: running: Set()
20/04/13 15:27:55 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 31)
20/04/13 15:27:55 INFO scheduler.DAGScheduler: failed: Set()
20/04/13 15:27:55 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[141] at map at CountVectorizer.scala:223), which has no missing parents
20/04/13 15:27:55 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 3.3 KB, free 882.0 MB)
20/04/13 15:27:55 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 1977.0 B, free 882.0 MB)
20/04/13 15:27:55 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 178.62.210.13:44585 (size: 1977.0 B, free: 909.3 MB)
20/04/13 15:27:55 INFO spark.SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1163
20/04/13 15:27:55 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 31 (MapPartitionsRDD[141] at map at CountVectorizer.scala:223) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:27:55 INFO cluster.YarnClusterScheduler: Adding task set 31.0 with 6 tasks
20/04/13 15:27:55 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 137, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 7651 bytes)
20/04/13 15:27:55 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 31.0 (TID 138, 178.62.200.211, executor 3, partition 1, NODE_LOCAL, 7651 bytes)
20/04/13 15:27:55 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 31.0 (TID 139, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 7651 bytes)
20/04/13 15:27:55 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 31.0 (TID 140, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 7651 bytes)
20/04/13 15:27:55 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 31.0 (TID 141, 178.62.200.211, executor 3, partition 4, NODE_LOCAL, 7651 bytes)
20/04/13 15:27:55 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 31.0 (TID 142, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 7651 bytes)
20/04/13 15:27:55 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 178.62.210.13:36375 (size: 1977.0 B, free: 1455.6 MB)
20/04/13 15:27:55 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 178.62.200.211:35537 (size: 1977.0 B, free: 1455.6 MB)
20/04/13 15:27:55 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 178.62.200.211:36927 (size: 1977.0 B, free: 1455.6 MB)
20/04/13 15:27:55 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 178.62.210.13:38976
20/04/13 15:27:55 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 178.62.200.211:57160
20/04/13 15:27:55 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 178.62.200.211:57158
20/04/13 15:27:57 INFO storage.BlockManagerInfo: Added rdd_141_5 in memory on 178.62.210.13:36375 (size: 35.8 MB, free: 1419.8 MB)
20/04/13 15:27:57 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 31.0 (TID 142) in 2192 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:27:57 INFO storage.BlockManagerInfo: Added rdd_141_2 in memory on 178.62.210.13:36375 (size: 35.9 MB, free: 1383.9 MB)
20/04/13 15:27:57 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 31.0 (TID 139) in 2249 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:27:58 INFO storage.BlockManagerInfo: Added rdd_141_4 in memory on 178.62.200.211:35537 (size: 36.2 MB, free: 1419.4 MB)
20/04/13 15:27:58 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 31.0 (TID 141) in 3357 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 15:27:58 INFO storage.BlockManagerInfo: Added rdd_141_1 in memory on 178.62.200.211:35537 (size: 36.0 MB, free: 1383.4 MB)
20/04/13 15:27:58 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 31.0 (TID 138) in 3514 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 15:27:58 INFO storage.BlockManagerInfo: Added rdd_141_3 in memory on 178.62.200.211:36927 (size: 36.0 MB, free: 1419.6 MB)
20/04/13 15:27:58 INFO storage.BlockManagerInfo: Added rdd_141_0 in memory on 178.62.200.211:36927 (size: 35.9 MB, free: 1383.7 MB)
20/04/13 15:27:58 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 31.0 (TID 140) in 3900 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:27:58 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 137) in 3901 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 15:27:58 INFO cluster.YarnClusterScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool 
20/04/13 15:27:58 INFO scheduler.DAGScheduler: ResultStage 31 (count at CountVectorizer.scala:230) finished in 3.906 s
20/04/13 15:27:58 INFO scheduler.DAGScheduler: Job 22 finished: count at CountVectorizer.scala:230, took 32.220213 s
20/04/13 15:27:58 INFO spark.SparkContext: Starting job: top at CountVectorizer.scala:233
20/04/13 15:27:58 INFO scheduler.DAGScheduler: Got job 23 (top at CountVectorizer.scala:233) with 6 output partitions
20/04/13 15:27:58 INFO scheduler.DAGScheduler: Final stage: ResultStage 33 (top at CountVectorizer.scala:233)
20/04/13 15:27:58 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)
20/04/13 15:27:58 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:27:58 INFO scheduler.DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[142] at top at CountVectorizer.scala:233), which has no missing parents
20/04/13 15:27:58 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 4.2 KB, free 882.0 MB)
20/04/13 15:27:58 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 2.4 KB, free 882.0 MB)
20/04/13 15:27:58 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 178.62.210.13:44585 (size: 2.4 KB, free: 909.3 MB)
20/04/13 15:27:58 INFO spark.SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1163
20/04/13 15:27:58 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 33 (MapPartitionsRDD[142] at top at CountVectorizer.scala:233) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:27:58 INFO cluster.YarnClusterScheduler: Adding task set 33.0 with 6 tasks
20/04/13 15:27:58 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 33.0 (TID 143, 178.62.210.13, executor 2, partition 2, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:27:58 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 144, 178.62.200.211, executor 1, partition 0, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:27:58 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 33.0 (TID 145, 178.62.200.211, executor 3, partition 1, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:27:58 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 33.0 (TID 146, 178.62.210.13, executor 2, partition 5, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:27:58 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 33.0 (TID 147, 178.62.200.211, executor 1, partition 3, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:27:58 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 33.0 (TID 148, 178.62.200.211, executor 3, partition 4, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:27:58 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 178.62.210.13:36375 (size: 2.4 KB, free: 1383.9 MB)
20/04/13 15:27:58 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 178.62.200.211:35537 (size: 2.4 KB, free: 1383.4 MB)
20/04/13 15:27:58 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 178.62.200.211:36927 (size: 2.4 KB, free: 1383.7 MB)
20/04/13 15:27:59 INFO storage.BlockManagerInfo: Added taskresult_143 in memory on 178.62.210.13:36375 (size: 5.8 MB, free: 1378.1 MB)
20/04/13 15:27:59 INFO storage.BlockManagerInfo: Added taskresult_146 in memory on 178.62.210.13:36375 (size: 5.8 MB, free: 1372.3 MB)
20/04/13 15:27:59 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:36375 after 9 ms (0 ms spent in bootstraps)
20/04/13 15:27:59 INFO storage.BlockManagerInfo: Added taskresult_145 in memory on 178.62.200.211:35537 (size: 5.8 MB, free: 1377.6 MB)
20/04/13 15:27:59 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:35537 after 3 ms (0 ms spent in bootstraps)
20/04/13 15:27:59 INFO storage.BlockManagerInfo: Added taskresult_148 in memory on 178.62.200.211:35537 (size: 5.8 MB, free: 1371.9 MB)
20/04/13 15:27:59 INFO storage.BlockManagerInfo: Added taskresult_144 in memory on 178.62.200.211:36927 (size: 5.8 MB, free: 1377.9 MB)
20/04/13 15:27:59 INFO storage.BlockManagerInfo: Added taskresult_147 in memory on 178.62.200.211:36927 (size: 5.8 MB, free: 1372.1 MB)
20/04/13 15:27:59 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 33.0 (TID 143) in 1023 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:27:59 INFO storage.BlockManagerInfo: Removed taskresult_143 on 178.62.210.13:36375 in memory (size: 5.8 MB, free: 1378.1 MB)
20/04/13 15:28:00 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:36927 after 6 ms (0 ms spent in bootstraps)
20/04/13 15:28:01 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 33.0 (TID 146) in 2053 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1372.2 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1371.9 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1378.1 MB)
20/04/13 15:28:01 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 33.0 (TID 145) in 2062 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 909.3 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed taskresult_146 on 178.62.210.13:36375 in memory (size: 5.8 MB, free: 1383.9 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed taskresult_145 on 178.62.200.211:35537 in memory (size: 5.8 MB, free: 1377.7 MB)
20/04/13 15:28:01 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 33.0 (TID 148) in 2127 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed taskresult_148 on 178.62.200.211:35537 in memory (size: 5.8 MB, free: 1383.5 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1383.5 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 909.3 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 178.62.200.211:35537 in memory (size: 1062.2 KB, free: 1384.5 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 178.62.200.211:36927 in memory (size: 1062.2 KB, free: 1373.2 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 178.62.210.13:36375 in memory (size: 1062.2 KB, free: 1385.0 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 178.62.210.13:44585 in memory (size: 1062.2 KB, free: 910.4 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 178.62.200.211:35537 in memory (size: 8.0 KB, free: 1384.5 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 178.62.200.211:36927 in memory (size: 8.0 KB, free: 1373.2 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 178.62.210.13:44585 in memory (size: 8.0 KB, free: 910.4 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 178.62.210.13:36375 in memory (size: 8.0 KB, free: 1385.0 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1384.6 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1373.2 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1385.0 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 910.4 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 178.62.210.13:44585 in memory (size: 4.6 KB, free: 910.4 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 178.62.200.211:35537 in memory (size: 4.6 KB, free: 1384.6 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 178.62.200.211:36927 in memory (size: 13.4 KB, free: 1373.2 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 178.62.200.211:35537 in memory (size: 13.4 KB, free: 1384.6 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 178.62.210.13:36375 in memory (size: 13.4 KB, free: 1385.0 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 178.62.210.13:44585 in memory (size: 13.4 KB, free: 910.4 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 178.62.200.211:35537 in memory (size: 1977.0 B, free: 1384.6 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 178.62.200.211:36927 in memory (size: 1977.0 B, free: 1373.2 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 178.62.210.13:44585 in memory (size: 1977.0 B, free: 910.4 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 178.62.210.13:36375 in memory (size: 1977.0 B, free: 1385.0 MB)
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 767
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 824
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 747
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 820
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 794
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 748
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 768
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 818
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 765
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 739
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 783
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 791
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 678
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 788
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 737
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 721
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 761
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 782
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 745
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 740
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 682
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 744
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 711
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 819
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 789
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 735
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 730
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 778
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 705
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 758
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 804
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 811
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 743
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 741
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 742
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 725
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 674
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 684
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 723
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 797
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 793
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 760
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 694
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 780
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 816
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 708
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 732
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 33
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 677
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 759
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 32
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 728
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 722
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 718
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 726
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 800
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 713
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 784
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 790
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 701
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 675
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 776
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 707
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 681
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 697
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 755
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 757
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 813
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 738
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 709
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 752
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 763
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 686
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 785
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 803
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 823
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 817
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 693
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 691
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 680
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 821
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 753
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 683
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 695
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 798
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 736
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 716
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 751
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 729
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 749
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 812
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 805
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 700
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 764
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 679
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 807
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 810
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 781
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 746
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 714
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 703
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 689
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 676
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 688
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 766
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 799
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 762
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 715
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 704
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 814
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 692
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 825
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 756
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 696
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 796
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 720
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 792
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 750
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 34
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 690
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 787
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1373.3 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 910.5 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1384.6 MB)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1385.1 MB)
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 733
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 724
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 795
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 717
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 769
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 35
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 808
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 727
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 699
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 801
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 685
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 786
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 734
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 802
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 712
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 731
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 754
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 777
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 706
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 822
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 809
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 815
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 687
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 779
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 710
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 36
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 806
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 698
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 719
20/04/13 15:28:01 INFO spark.ContextCleaner: Cleaned accumulator 702
20/04/13 15:28:01 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 144) in 2295 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed taskresult_144 on 178.62.200.211:36927 in memory (size: 5.8 MB, free: 1379.1 MB)
20/04/13 15:28:01 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 33.0 (TID 147) in 2350 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 15:28:01 INFO cluster.YarnClusterScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool 
20/04/13 15:28:01 INFO storage.BlockManagerInfo: Removed taskresult_147 on 178.62.200.211:36927 in memory (size: 5.8 MB, free: 1384.8 MB)
20/04/13 15:28:01 INFO scheduler.DAGScheduler: ResultStage 33 (top at CountVectorizer.scala:233) finished in 2.693 s
20/04/13 15:28:01 INFO scheduler.DAGScheduler: Job 23 finished: top at CountVectorizer.scala:233, took 2.762771 s
20/04/13 15:28:03 INFO memory.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 37.7 MB, free 848.7 MB)
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 3.2 MB, free 845.4 MB)
20/04/13 15:28:04 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 178.62.210.13:44585 (size: 3.2 MB, free: 907.2 MB)
20/04/13 15:28:04 INFO spark.SparkContext: Created broadcast 52 from broadcast at CountVectorizer.scala:298
20/04/13 15:28:04 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:28:04 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:28:04 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Entry: string, Entry name: string, Sequence: string ... 1 more fields>
20/04/13 15:28:04 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:28:04 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 337.3 KB, free 845.1 MB)
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 31.5 KB, free 845.1 MB)
20/04/13 15:28:04 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 907.2 MB)
20/04/13 15:28:04 INFO spark.SparkContext: Created broadcast 53 from parquet at NativeMethodAccessorImpl.java:0
20/04/13 15:28:04 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:28:04 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
20/04/13 15:28:04 INFO scheduler.DAGScheduler: Got job 24 (parquet at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 15:28:04 INFO scheduler.DAGScheduler: Final stage: ResultStage 34 (parquet at NativeMethodAccessorImpl.java:0)
20/04/13 15:28:04 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:28:04 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:28:04 INFO scheduler.DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[147] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 3.3 MB, free 841.8 MB)
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 2.2 MB, free 839.6 MB)
20/04/13 15:28:04 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 178.62.210.13:44585 (size: 2.2 MB, free: 905.0 MB)
20/04/13 15:28:04 INFO spark.SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1163
20/04/13 15:28:04 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 34 (MapPartitionsRDD[147] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:28:04 INFO cluster.YarnClusterScheduler: Adding task set 34.0 with 6 tasks
20/04/13 15:28:04 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 34.0 (TID 149, 178.62.200.211, executor 3, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:04 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 34.0 (TID 150, 178.62.200.211, executor 1, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:04 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 34.0 (TID 151, 178.62.210.13, executor 2, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:04 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 34.0 (TID 152, 178.62.200.211, executor 3, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:04 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 34.0 (TID 153, 178.62.200.211, executor 1, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:04 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 34.0 (TID 154, 178.62.210.13, executor 2, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:04 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 178.62.210.13:36375 (size: 2.2 MB, free: 1382.9 MB)
20/04/13 15:28:04 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 178.62.200.211:36927 (size: 2.2 MB, free: 1382.7 MB)
20/04/13 15:28:04 INFO storage.BlockManagerInfo: Added broadcast_54_piece0 in memory on 178.62.200.211:35537 (size: 2.2 MB, free: 1382.5 MB)
20/04/13 15:28:04 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1382.9 MB)
20/04/13 15:28:04 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1382.4 MB)
20/04/13 15:28:04 INFO storage.BlockManagerInfo: Added broadcast_53_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1382.7 MB)
20/04/13 15:28:04 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 178.62.210.13:36375 (size: 3.2 MB, free: 1379.6 MB)
20/04/13 15:28:04 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 178.62.200.211:36927 (size: 3.2 MB, free: 1379.4 MB)
20/04/13 15:28:05 INFO storage.BlockManagerInfo: Added broadcast_52_piece0 in memory on 178.62.200.211:35537 (size: 3.2 MB, free: 1379.2 MB)
20/04/13 15:28:10 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 34.0 (TID 154) in 6135 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:28:15 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 34.0 (TID 151) in 11301 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:28:21 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 34.0 (TID 153) in 16587 ms on 178.62.200.211 (executor 1) (3/6)
20/04/13 15:28:21 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 34.0 (TID 150) in 17269 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:28:22 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 34.0 (TID 152) in 18316 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 34.0 (TID 149) in 18425 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:28:23 INFO cluster.YarnClusterScheduler: Removed TaskSet 34.0, whose tasks have all completed, from pool 
20/04/13 15:28:23 INFO scheduler.DAGScheduler: ResultStage 34 (parquet at NativeMethodAccessorImpl.java:0) finished in 18.568 s
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Job 24 finished: parquet at NativeMethodAccessorImpl.java:0, took 18.570447 s
20/04/13 15:28:23 INFO datasources.FileFormatWriter: Write Job fea0c38e-2511-4b4c-b4ab-66aa0f7f2eba committed.
20/04/13 15:28:23 INFO datasources.FileFormatWriter: Finished processing stats for write job fea0c38e-2511-4b4c-b4ab-66aa0f7f2eba.
20/04/13 15:28:23 INFO datasources.InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
20/04/13 15:28:23 INFO datasources.InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
20/04/13 15:28:23 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:28:23 INFO datasources.FileSourceStrategy: Post-Scan Filters: (length(trim(value#300, None)) > 0)
20/04/13 15:28:23 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 15:28:23 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_55 stored as values in memory (estimated size 337.3 KB, free 839.3 MB)
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 31.5 KB, free 839.3 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 905.0 MB)
20/04/13 15:28:23 INFO spark.SparkContext: Created broadcast 55 from csv at NativeMethodAccessorImpl.java:0
20/04/13 15:28:23 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:28:23 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Got job 25 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[153] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_56 stored as values in memory (estimated size 8.9 KB, free 839.3 MB)
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 4.6 KB, free 839.3 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on 178.62.210.13:44585 (size: 4.6 KB, free: 905.0 MB)
20/04/13 15:28:23 INFO spark.SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1163
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[153] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/04/13 15:28:23 INFO cluster.YarnClusterScheduler: Adding task set 35.0 with 1 tasks
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 155, 178.62.210.13, executor 2, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_56_piece0 in memory on 178.62.210.13:36375 (size: 4.6 KB, free: 1379.6 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_55_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1379.6 MB)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 155) in 34 ms on 178.62.210.13 (executor 2) (1/1)
20/04/13 15:28:23 INFO cluster.YarnClusterScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool 
20/04/13 15:28:23 INFO scheduler.DAGScheduler: ResultStage 35 (csv at NativeMethodAccessorImpl.java:0) finished in 0.038 s
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Job 25 finished: csv at NativeMethodAccessorImpl.java:0, took 0.041466 s
20/04/13 15:28:23 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:28:23 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:28:23 INFO datasources.FileSourceStrategy: Output Data Schema: struct<value: string>
20/04/13 15:28:23 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 337.3 KB, free 838.9 MB)
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 31.5 KB, free 838.9 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 905.0 MB)
20/04/13 15:28:23 INFO spark.SparkContext: Created broadcast 57 from csv at NativeMethodAccessorImpl.java:0
20/04/13 15:28:23 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:28:23 INFO spark.SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Got job 26 (csv at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Final stage: ResultStage 36 (csv at NativeMethodAccessorImpl.java:0)
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[159] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 15.0 KB, free 838.9 MB)
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 8.0 KB, free 838.9 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 178.62.210.13:44585 (size: 8.0 KB, free: 904.9 MB)
20/04/13 15:28:23 INFO spark.SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1163
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 36 (MapPartitionsRDD[159] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:28:23 INFO cluster.YarnClusterScheduler: Adding task set 36.0 with 6 tasks
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 156, 178.62.210.13, executor 2, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 36.0 (TID 157, 178.62.200.211, executor 1, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 36.0 (TID 158, 178.62.200.211, executor 3, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 36.0 (TID 159, 178.62.210.13, executor 2, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 36.0 (TID 160, 178.62.200.211, executor 1, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 36.0 (TID 161, 178.62.200.211, executor 3, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 178.62.200.211:35537 (size: 8.0 KB, free: 1379.2 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 178.62.210.13:36375 (size: 8.0 KB, free: 1379.6 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_58_piece0 in memory on 178.62.200.211:36927 (size: 8.0 KB, free: 1379.4 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1379.5 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1379.4 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_57_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1379.1 MB)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 36.0 (TID 161) in 137 ms on 178.62.200.211 (executor 3) (1/6)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 156) in 146 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 36.0 (TID 157) in 163 ms on 178.62.200.211 (executor 1) (3/6)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 36.0 (TID 159) in 164 ms on 178.62.210.13 (executor 2) (4/6)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 36.0 (TID 160) in 204 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 36.0 (TID 158) in 234 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:28:23 INFO cluster.YarnClusterScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool 
20/04/13 15:28:23 INFO scheduler.DAGScheduler: ResultStage 36 (csv at NativeMethodAccessorImpl.java:0) finished in 0.238 s
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Job 26 finished: csv at NativeMethodAccessorImpl.java:0, took 0.239473 s
20/04/13 15:28:23 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:28:23 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:28:23 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Sequence: string>
20/04/13 15:28:23 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 337.3 KB, free 838.6 MB)
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 31.5 KB, free 838.5 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 904.9 MB)
20/04/13 15:28:23 INFO spark.SparkContext: Created broadcast 59 from rdd at CountVectorizer.scala:187
20/04/13 15:28:23 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:28:23 INFO spark.SparkContext: Starting job: count at CountVectorizer.scala:230
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Registering RDD 169 (flatMap at CountVectorizer.scala:205) as input to shuffle 5
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Got job 27 (count at CountVectorizer.scala:230) with 6 output partitions
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Final stage: ResultStage 38 (count at CountVectorizer.scala:230)
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 37)
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[169] at flatMap at CountVectorizer.scala:205), which has no missing parents
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 27.4 KB, free 838.5 MB)
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 13.4 KB, free 838.5 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 178.62.210.13:44585 (size: 13.4 KB, free: 904.9 MB)
20/04/13 15:28:23 INFO spark.SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1163
20/04/13 15:28:23 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[169] at flatMap at CountVectorizer.scala:205) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:28:23 INFO cluster.YarnClusterScheduler: Adding task set 37.0 with 6 tasks
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 162, 178.62.200.211, executor 1, partition 0, NODE_LOCAL, 8258 bytes)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 37.0 (TID 163, 178.62.210.13, executor 2, partition 1, NODE_LOCAL, 8258 bytes)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 37.0 (TID 164, 178.62.200.211, executor 3, partition 2, NODE_LOCAL, 8258 bytes)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 37.0 (TID 165, 178.62.200.211, executor 1, partition 3, NODE_LOCAL, 8258 bytes)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 37.0 (TID 166, 178.62.210.13, executor 2, partition 4, NODE_LOCAL, 8258 bytes)
20/04/13 15:28:23 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 37.0 (TID 167, 178.62.200.211, executor 3, partition 5, NODE_LOCAL, 8258 bytes)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 178.62.200.211:35537 (size: 13.4 KB, free: 1379.1 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 178.62.200.211:36927 (size: 13.4 KB, free: 1379.4 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_60_piece0 in memory on 178.62.210.13:36375 (size: 13.4 KB, free: 1379.5 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1379.3 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1379.5 MB)
20/04/13 15:28:23 INFO storage.BlockManagerInfo: Added broadcast_59_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1379.1 MB)
20/04/13 15:28:35 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 37.0 (TID 167) in 11409 ms on 178.62.200.211 (executor 3) (1/6)
20/04/13 15:28:44 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 37.0 (TID 166) in 20645 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:28:45 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 37.0 (TID 163) in 22215 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 15:28:47 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 37.0 (TID 165) in 23972 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:28:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 162) in 25480 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:28:49 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 37.0 (TID 164) in 25954 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:28:49 INFO cluster.YarnClusterScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool 
20/04/13 15:28:49 INFO scheduler.DAGScheduler: ShuffleMapStage 37 (flatMap at CountVectorizer.scala:205) finished in 25.965 s
20/04/13 15:28:49 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/04/13 15:28:49 INFO scheduler.DAGScheduler: running: Set()
20/04/13 15:28:49 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 38)
20/04/13 15:28:49 INFO scheduler.DAGScheduler: failed: Set()
20/04/13 15:28:49 INFO scheduler.DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[171] at map at CountVectorizer.scala:223), which has no missing parents
20/04/13 15:28:49 INFO memory.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 3.3 KB, free 838.5 MB)
20/04/13 15:28:49 INFO memory.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 1975.0 B, free 838.5 MB)
20/04/13 15:28:49 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 178.62.210.13:44585 (size: 1975.0 B, free: 904.9 MB)
20/04/13 15:28:49 INFO spark.SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1163
20/04/13 15:28:49 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 38 (MapPartitionsRDD[171] at map at CountVectorizer.scala:223) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:28:49 INFO cluster.YarnClusterScheduler: Adding task set 38.0 with 6 tasks
20/04/13 15:28:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 168, 178.62.210.13, executor 2, partition 0, NODE_LOCAL, 7651 bytes)
20/04/13 15:28:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 38.0 (TID 169, 178.62.200.211, executor 1, partition 1, NODE_LOCAL, 7651 bytes)
20/04/13 15:28:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 38.0 (TID 170, 178.62.200.211, executor 3, partition 2, NODE_LOCAL, 7651 bytes)
20/04/13 15:28:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 38.0 (TID 171, 178.62.210.13, executor 2, partition 3, NODE_LOCAL, 7651 bytes)
20/04/13 15:28:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 38.0 (TID 172, 178.62.200.211, executor 1, partition 4, NODE_LOCAL, 7651 bytes)
20/04/13 15:28:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 38.0 (TID 173, 178.62.200.211, executor 3, partition 5, NODE_LOCAL, 7651 bytes)
20/04/13 15:28:49 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 178.62.200.211:36927 (size: 1975.0 B, free: 1379.3 MB)
20/04/13 15:28:49 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 178.62.200.211:35537 (size: 1975.0 B, free: 1379.1 MB)
20/04/13 15:28:49 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 178.62.200.211:57158
20/04/13 15:28:49 INFO storage.BlockManagerInfo: Added broadcast_61_piece0 in memory on 178.62.210.13:36375 (size: 1975.0 B, free: 1379.5 MB)
20/04/13 15:28:49 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 178.62.200.211:57160
20/04/13 15:28:49 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 178.62.210.13:38976
20/04/13 15:28:51 INFO storage.BlockManagerInfo: Added rdd_171_3 in memory on 178.62.210.13:36375 (size: 36.0 MB, free: 1343.5 MB)
20/04/13 15:28:51 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 38.0 (TID 171) in 2266 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:28:51 INFO storage.BlockManagerInfo: Added rdd_171_0 in memory on 178.62.210.13:36375 (size: 35.9 MB, free: 1307.6 MB)
20/04/13 15:28:51 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 168) in 2283 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:28:52 INFO storage.BlockManagerInfo: Added rdd_171_5 in memory on 178.62.200.211:35537 (size: 35.9 MB, free: 1343.2 MB)
20/04/13 15:28:52 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 38.0 (TID 173) in 3299 ms on 178.62.200.211 (executor 3) (3/6)
20/04/13 15:28:53 INFO storage.BlockManagerInfo: Added rdd_171_2 in memory on 178.62.200.211:35537 (size: 35.9 MB, free: 1307.3 MB)
20/04/13 15:28:53 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 38.0 (TID 170) in 3479 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Added rdd_171_1 in memory on 178.62.200.211:36927 (size: 36.6 MB, free: 1342.7 MB)
20/04/13 15:28:54 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 38.0 (TID 169) in 4439 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Added rdd_171_4 in memory on 178.62.200.211:36927 (size: 36.1 MB, free: 1306.6 MB)
20/04/13 15:28:54 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 38.0 (TID 172) in 4496 ms on 178.62.200.211 (executor 1) (6/6)
20/04/13 15:28:54 INFO cluster.YarnClusterScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool 
20/04/13 15:28:54 INFO scheduler.DAGScheduler: ResultStage 38 (count at CountVectorizer.scala:230) finished in 4.499 s
20/04/13 15:28:54 INFO scheduler.DAGScheduler: Job 27 finished: count at CountVectorizer.scala:230, took 30.469743 s
20/04/13 15:28:54 INFO spark.SparkContext: Starting job: top at CountVectorizer.scala:233
20/04/13 15:28:54 INFO scheduler.DAGScheduler: Got job 28 (top at CountVectorizer.scala:233) with 6 output partitions
20/04/13 15:28:54 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (top at CountVectorizer.scala:233)
20/04/13 15:28:54 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)
20/04/13 15:28:54 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:28:54 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[172] at top at CountVectorizer.scala:233), which has no missing parents
20/04/13 15:28:54 INFO memory.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 4.2 KB, free 838.5 MB)
20/04/13 15:28:54 INFO memory.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 2.4 KB, free 838.5 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 178.62.210.13:44585 (size: 2.4 KB, free: 904.9 MB)
20/04/13 15:28:54 INFO spark.SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1163
20/04/13 15:28:54 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 40 (MapPartitionsRDD[172] at top at CountVectorizer.scala:233) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:28:54 INFO cluster.YarnClusterScheduler: Adding task set 40.0 with 6 tasks
20/04/13 15:28:54 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 40.0 (TID 174, 178.62.200.211, executor 1, partition 1, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:28:54 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 40.0 (TID 175, 178.62.200.211, executor 3, partition 2, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:28:54 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 176, 178.62.210.13, executor 2, partition 0, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:28:54 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 40.0 (TID 177, 178.62.200.211, executor 1, partition 4, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:28:54 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 40.0 (TID 178, 178.62.200.211, executor 3, partition 5, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:28:54 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 40.0 (TID 179, 178.62.210.13, executor 2, partition 3, PROCESS_LOCAL, 7651 bytes)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 178.62.210.13:36375 (size: 2.4 KB, free: 1307.6 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 178.62.200.211:35537 (size: 2.4 KB, free: 1307.3 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Added broadcast_62_piece0 in memory on 178.62.200.211:36927 (size: 2.4 KB, free: 1306.6 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Added taskresult_176 in memory on 178.62.210.13:36375 (size: 5.8 MB, free: 1301.8 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Added taskresult_179 in memory on 178.62.210.13:36375 (size: 5.8 MB, free: 1296.0 MB)
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 961
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 865
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 854
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 960
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 864
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 954
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 989
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 911
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 862
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 971
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 991
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 858
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 940
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1296.0 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 904.9 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1307.3 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_53_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1306.7 MB)
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 987
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 880
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1296.1 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 905.0 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1306.7 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_57_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1307.3 MB)
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 905
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 916
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 902
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 882
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 922
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 881
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 981
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 873
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 973
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 926
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 923
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 988
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 934
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on 178.62.210.13:36375 in memory (size: 1975.0 B, free: 1296.1 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on 178.62.210.13:44585 in memory (size: 1975.0 B, free: 905.0 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on 178.62.200.211:35537 in memory (size: 1975.0 B, free: 1307.3 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_61_piece0 on 178.62.200.211:36927 in memory (size: 1975.0 B, free: 1306.7 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Added taskresult_177 in memory on 178.62.200.211:36927 (size: 5.8 MB, free: 1300.9 MB)
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 956
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 907
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 909
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 851
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 879
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 992
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 876
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 897
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 918
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 946
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 982
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 914
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 924
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 937
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 178.62.210.13:44585 in memory (size: 8.0 KB, free: 905.0 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 178.62.200.211:35537 in memory (size: 8.0 KB, free: 1307.4 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 178.62.200.211:36927 in memory (size: 8.0 KB, free: 1300.9 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_58_piece0 on 178.62.210.13:36375 in memory (size: 8.0 KB, free: 1296.1 MB)
20/04/13 15:28:54 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 176) in 764 ms on 178.62.210.13 (executor 2) (1/6)
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 968
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 976
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 932
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 994
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 856
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 906
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 890
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 878
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 896
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 974
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 983
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 962
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 980
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 978
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 905.0 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed taskresult_176 on 178.62.210.13:36375 in memory (size: 5.8 MB, free: 1301.9 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_55_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1301.9 MB)
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 995
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 917
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 959
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 1001
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 997
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 912
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 979
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 943
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 866
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 863
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 859
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 998
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 872
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 875
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 886
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 852
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 957
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 936
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 929
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 868
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 887
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 919
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 944
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 883
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 999
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 855
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 942
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 993
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 860
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 990
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 920
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 928
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 941
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 901
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 900
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 867
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 904
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 930
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 964
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 884
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 885
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 888
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 975
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 939
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 910
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 972
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 895
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 894
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 899
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 925
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 874
20/04/13 15:28:54 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 40.0 (TID 179) in 782 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on 178.62.200.211:35537 in memory (size: 13.4 KB, free: 1307.4 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed taskresult_179 on 178.62.210.13:36375 in memory (size: 5.8 MB, free: 1307.7 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on 178.62.200.211:36927 in memory (size: 13.4 KB, free: 1300.9 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Added taskresult_174 in memory on 178.62.200.211:36927 (size: 5.8 MB, free: 1295.1 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Added taskresult_178 in memory on 178.62.200.211:35537 (size: 5.8 MB, free: 1301.6 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on 178.62.210.13:36375 in memory (size: 13.4 KB, free: 1307.7 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_60_piece0 on 178.62.210.13:44585 in memory (size: 13.4 KB, free: 905.0 MB)
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 889
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on 178.62.210.13:44585 in memory (size: 4.6 KB, free: 905.0 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_56_piece0 on 178.62.210.13:36375 in memory (size: 4.6 KB, free: 1307.7 MB)
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 898
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 861
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 1002
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 869
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 892
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 984
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 857
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 903
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 958
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 927
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 967
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 853
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 985
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 908
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 965
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 933
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 913
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 931
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 969
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 963
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 986
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 871
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 870
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 935
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 891
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 970
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 877
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 915
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 955
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 996
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 945
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 977
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 1000
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 893
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 966
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 953
20/04/13 15:28:54 INFO spark.ContextCleaner: Cleaned accumulator 938
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on 178.62.200.211:35537 in memory (size: 2.2 MB, free: 1303.7 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on 178.62.200.211:36927 in memory (size: 2.2 MB, free: 1297.3 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on 178.62.210.13:36375 in memory (size: 2.2 MB, free: 1309.9 MB)
20/04/13 15:28:54 INFO storage.BlockManagerInfo: Removed broadcast_54_piece0 on 178.62.210.13:44585 in memory (size: 2.2 MB, free: 907.2 MB)
20/04/13 15:28:55 INFO spark.ContextCleaner: Cleaned accumulator 921
20/04/13 15:28:55 INFO storage.BlockManagerInfo: Added taskresult_175 in memory on 178.62.200.211:35537 (size: 5.8 MB, free: 1298.0 MB)
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 837
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 828
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 848
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 178.62.200.211:35537 in memory (size: 31.5 KB, free: 1298.0 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 178.62.200.211:36927 in memory (size: 31.5 KB, free: 1297.3 MB)
20/04/13 15:28:56 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 40.0 (TID 177) in 2094 ms on 178.62.200.211 (executor 1) (3/6)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 178.62.210.13:44585 in memory (size: 31.5 KB, free: 907.2 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed taskresult_177 on 178.62.200.211:36927 in memory (size: 5.8 MB, free: 1303.1 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 178.62.210.13:36375 in memory (size: 31.5 KB, free: 1309.9 MB)
20/04/13 15:28:56 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 40.0 (TID 178) in 2131 ms on 178.62.200.211 (executor 3) (4/6)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed taskresult_178 on 178.62.200.211:35537 in memory (size: 5.8 MB, free: 1303.8 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 178.62.210.13:44585 in memory (size: 2.4 KB, free: 907.2 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 178.62.200.211:35537 in memory (size: 2.4 KB, free: 1303.8 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 178.62.200.211:36927 in memory (size: 2.4 KB, free: 1303.1 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 178.62.210.13:36375 in memory (size: 2.4 KB, free: 1309.9 MB)
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 830
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 838
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 774
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 835
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 847
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 844
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 841
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 845
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 772
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 850
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 829
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 842
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 831
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 849
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 832
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 843
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 839
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 833
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 773
20/04/13 15:28:56 INFO storage.BlockManager: Removing RDD 141
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned RDD 141
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 840
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 826
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 178.62.200.211:35537 in memory (size: 1770.4 KB, free: 1377.7 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 178.62.200.211:36927 in memory (size: 1770.4 KB, free: 1376.8 MB)
20/04/13 15:28:56 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 40.0 (TID 174) in 2153 ms on 178.62.200.211 (executor 1) (5/6)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 178.62.210.13:44585 in memory (size: 1770.4 KB, free: 908.9 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 178.62.210.13:36375 in memory (size: 1770.4 KB, free: 1383.3 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed taskresult_174 on 178.62.200.211:36927 in memory (size: 5.8 MB, free: 1382.5 MB)
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 770
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 771
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 178.62.210.13:44585 in memory (size: 77.2 KB, free: 909.0 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 178.62.210.13:36375 in memory (size: 77.2 KB, free: 1383.4 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 178.62.200.211:35537 in memory (size: 77.2 KB, free: 1377.7 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 178.62.200.211:36927 in memory (size: 77.2 KB, free: 1382.6 MB)
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 846
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 827
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 836
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on 178.62.210.13:44585 in memory (size: 3.2 MB, free: 912.3 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on 178.62.200.211:35537 in memory (size: 3.2 MB, free: 1381.0 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on 178.62.200.211:36927 in memory (size: 3.2 MB, free: 1385.9 MB)
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed broadcast_52_piece0 on 178.62.210.13:36375 in memory (size: 3.2 MB, free: 1386.7 MB)
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned shuffle 4
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 834
20/04/13 15:28:56 INFO spark.ContextCleaner: Cleaned accumulator 775
20/04/13 15:28:56 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 40.0 (TID 175) in 2222 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:28:56 INFO cluster.YarnClusterScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool 
20/04/13 15:28:56 INFO storage.BlockManagerInfo: Removed taskresult_175 on 178.62.200.211:35537 in memory (size: 5.8 MB, free: 1386.8 MB)
20/04/13 15:28:56 INFO scheduler.DAGScheduler: ResultStage 40 (top at CountVectorizer.scala:233) finished in 2.494 s
20/04/13 15:28:56 INFO scheduler.DAGScheduler: Job 28 finished: top at CountVectorizer.scala:233, took 2.561736 s
20/04/13 15:28:58 INFO memory.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 37.7 MB, free 874.2 MB)
20/04/13 15:28:58 INFO memory.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 3.3 MB, free 871.0 MB)
20/04/13 15:28:58 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 178.62.210.13:44585 (size: 3.3 MB, free: 909.0 MB)
20/04/13 15:28:58 INFO spark.SparkContext: Created broadcast 63 from broadcast at CountVectorizer.scala:298
20/04/13 15:28:59 INFO datasources.FileSourceStrategy: Pruning directories with: 
20/04/13 15:28:59 INFO datasources.FileSourceStrategy: Post-Scan Filters: 
20/04/13 15:28:59 INFO datasources.FileSourceStrategy: Output Data Schema: struct<Entry: string, Entry name: string, Sequence: string ... 1 more fields>
20/04/13 15:28:59 INFO execution.FileSourceScanExec: Pushed Filters: 
20/04/13 15:28:59 INFO parquet.ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_64 stored as values in memory (estimated size 337.3 KB, free 870.7 MB)
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 31.5 KB, free 870.6 MB)
20/04/13 15:28:59 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 178.62.210.13:44585 (size: 31.5 KB, free: 909.0 MB)
20/04/13 15:28:59 INFO spark.SparkContext: Created broadcast 64 from parquet at NativeMethodAccessorImpl.java:0
20/04/13 15:28:59 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6543671 bytes, open cost is considered as scanning 4194304 bytes.
20/04/13 15:28:59 INFO spark.SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
20/04/13 15:28:59 INFO scheduler.DAGScheduler: Got job 29 (parquet at NativeMethodAccessorImpl.java:0) with 6 output partitions
20/04/13 15:28:59 INFO scheduler.DAGScheduler: Final stage: ResultStage 41 (parquet at NativeMethodAccessorImpl.java:0)
20/04/13 15:28:59 INFO scheduler.DAGScheduler: Parents of final stage: List()
20/04/13 15:28:59 INFO scheduler.DAGScheduler: Missing parents: List()
20/04/13 15:28:59 INFO scheduler.DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[177] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_65 stored as values in memory (estimated size 3.3 MB, free 867.4 MB)
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 2.2 MB, free 865.2 MB)
20/04/13 15:28:59 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 178.62.210.13:44585 (size: 2.2 MB, free: 906.8 MB)
20/04/13 15:28:59 INFO spark.SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1163
20/04/13 15:28:59 INFO scheduler.DAGScheduler: Submitting 6 missing tasks from ResultStage 41 (MapPartitionsRDD[177] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
20/04/13 15:28:59 INFO cluster.YarnClusterScheduler: Adding task set 41.0 with 6 tasks
20/04/13 15:28:59 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 180, 178.62.210.13, executor 2, partition 0, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:59 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 41.0 (TID 181, 178.62.200.211, executor 3, partition 1, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:59 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 41.0 (TID 182, 178.62.200.211, executor 1, partition 2, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:59 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 41.0 (TID 183, 178.62.210.13, executor 2, partition 3, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:59 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 41.0 (TID 184, 178.62.200.211, executor 3, partition 4, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:59 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 41.0 (TID 185, 178.62.200.211, executor 1, partition 5, NODE_LOCAL, 8269 bytes)
20/04/13 15:28:59 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 178.62.210.13:36375 (size: 2.2 MB, free: 1384.5 MB)
20/04/13 15:28:59 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 178.62.200.211:35537 (size: 2.2 MB, free: 1384.6 MB)
20/04/13 15:28:59 INFO storage.BlockManagerInfo: Added broadcast_65_piece0 in memory on 178.62.200.211:36927 (size: 2.2 MB, free: 1383.7 MB)
20/04/13 15:28:59 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 178.62.210.13:36375 (size: 31.5 KB, free: 1384.5 MB)
20/04/13 15:28:59 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 178.62.200.211:35537 (size: 31.5 KB, free: 1384.6 MB)
20/04/13 15:28:59 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 178.62.210.13:36375 (size: 3.3 MB, free: 1381.2 MB)
20/04/13 15:28:59 INFO storage.BlockManagerInfo: Added broadcast_64_piece0 in memory on 178.62.200.211:36927 (size: 31.5 KB, free: 1383.7 MB)
20/04/13 15:28:59 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 178.62.200.211:36927 (size: 3.3 MB, free: 1380.4 MB)
20/04/13 15:28:59 INFO storage.BlockManagerInfo: Added broadcast_63_piece0 in memory on 178.62.200.211:35537 (size: 3.3 MB, free: 1381.3 MB)
20/04/13 15:29:06 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 41.0 (TID 185) in 7373 ms on 178.62.200.211 (executor 1) (1/6)
20/04/13 15:29:11 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 41.0 (TID 183) in 11808 ms on 178.62.210.13 (executor 2) (2/6)
20/04/13 15:29:11 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 180) in 12063 ms on 178.62.210.13 (executor 2) (3/6)
20/04/13 15:29:13 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 41.0 (TID 182) in 14073 ms on 178.62.200.211 (executor 1) (4/6)
20/04/13 15:29:15 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 41.0 (TID 184) in 15485 ms on 178.62.200.211 (executor 3) (5/6)
20/04/13 15:29:15 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 41.0 (TID 181) in 15672 ms on 178.62.200.211 (executor 3) (6/6)
20/04/13 15:29:15 INFO cluster.YarnClusterScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool 
20/04/13 15:29:15 INFO scheduler.DAGScheduler: ResultStage 41 (parquet at NativeMethodAccessorImpl.java:0) finished in 15.906 s
20/04/13 15:29:15 INFO scheduler.DAGScheduler: Job 29 finished: parquet at NativeMethodAccessorImpl.java:0, took 15.909431 s
20/04/13 15:29:15 INFO datasources.FileFormatWriter: Write Job 430c935a-dcb7-464b-aed2-c8a1dcfd5ea4 committed.
20/04/13 15:29:15 INFO datasources.FileFormatWriter: Finished processing stats for write job 430c935a-dcb7-464b-aed2-c8a1dcfd5ea4.
20/04/13 15:29:15 INFO server.AbstractConnector: Stopped Spark@1c5dc510{HTTP/1.1,[http/1.1]}{0.0.0.0:0}
20/04/13 15:29:15 INFO ui.SparkUI: Stopped Spark web UI at http://178.62.210.13:33879
20/04/13 15:29:15 INFO yarn.YarnAllocator: Driver requested a total number of 0 executor(s).
20/04/13 15:29:15 INFO cluster.YarnClusterSchedulerBackend: Shutting down all executors
20/04/13 15:29:15 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/04/13 15:29:15 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/04/13 15:29:15 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/04/13 15:29:15 INFO memory.MemoryStore: MemoryStore cleared
20/04/13 15:29:15 INFO storage.BlockManager: BlockManager stopped
20/04/13 15:29:15 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
20/04/13 15:29:15 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/04/13 15:29:15 INFO spark.SparkContext: Successfully stopped SparkContext
20/04/13 15:29:16 INFO yarn.ApplicationMaster: Final app status: SUCCEEDED, exitCode: 0
20/04/13 15:29:16 INFO yarn.ApplicationMaster: Unregistering ApplicationMaster with SUCCEEDED
20/04/13 15:29:16 INFO impl.AMRMClientImpl: Waiting for application to be successfully unregistered.
20/04/13 15:29:16 INFO yarn.ApplicationMaster: Deleting staging directory hdfs://178.62.208.209:9000/user/root/.sparkStaging/application_1586780871303_0007
20/04/13 15:29:16 INFO util.ShutdownHookManager: Shutdown hook called
20/04/13 15:29:16 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0007/spark-0128c65e-e176-4bbe-b12c-078c5723c2d5
20/04/13 15:29:16 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0007/spark-0128c65e-e176-4bbe-b12c-078c5723c2d5/pyspark-4d758d2e-fb32-4620-b7e4-fa88715dd50e
End of LogType:stderr

LogType:stdout
Log Upload Time:Mon Apr 13 15:29:17 +0000 2020
LogLength:0
Log Contents:
End of LogType:stdout



Container: container_1586780871303_0007_01_000003 on 178.62.210.13_35211
==========================================================================
LogType:stderr
Log Upload Time:Mon Apr 13 15:29:17 +0000 2020
LogLength:117749
Log Contents:
20/04/13 15:24:39 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 20283@178.62.210.13
20/04/13 15:24:39 INFO util.SignalUtils: Registered signal handler for TERM
20/04/13 15:24:39 INFO util.SignalUtils: Registered signal handler for HUP
20/04/13 15:24:39 INFO util.SignalUtils: Registered signal handler for INT
20/04/13 15:24:40 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 15:24:40 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 15:24:40 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 15:24:40 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 15:24:40 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 15:24:40 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:39481 after 64 ms (0 ms spent in bootstraps)
20/04/13 15:24:41 INFO spark.SecurityManager: Changing view acls to: root
20/04/13 15:24:41 INFO spark.SecurityManager: Changing modify acls to: root
20/04/13 15:24:41 INFO spark.SecurityManager: Changing view acls groups to: 
20/04/13 15:24:41 INFO spark.SecurityManager: Changing modify acls groups to: 
20/04/13 15:24:41 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
20/04/13 15:24:41 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:39481 after 5 ms (0 ms spent in bootstraps)
20/04/13 15:24:41 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0007/blockmgr-78e7ce3e-2db9-4212-a002-bb56a844e4c6
20/04/13 15:24:41 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MB
20/04/13 15:24:41 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@178.62.210.13:39481
20/04/13 15:24:41 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver
20/04/13 15:24:41 INFO executor.Executor: Starting executor ID 2 on host 178.62.210.13
20/04/13 15:24:41 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36375.
20/04/13 15:24:41 INFO netty.NettyBlockTransferService: Server created on 178.62.210.13:36375
20/04/13 15:24:41 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/04/13 15:24:41 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(2, 178.62.210.13, 36375, None)
20/04/13 15:24:41 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(2, 178.62.210.13, 36375, None)
20/04/13 15:24:41 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(2, 178.62.210.13, 36375, None)
20/04/13 15:24:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0
20/04/13 15:24:47 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
20/04/13 15:24:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1
20/04/13 15:24:47 INFO client.TransportClientFactory: Successfully created connection to /178.62.210.13:44585 after 5 ms (0 ms spent in bootstraps)
20/04/13 15:24:47 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1458.6 MB)
20/04/13 15:24:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 119 ms
20/04/13 15:24:47 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.9 KB, free 1458.6 MB)
20/04/13 15:24:48 INFO codegen.CodeGenerator: Code generated in 323.483468 ms
20/04/13 15:24:48 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:24:48 INFO codegen.CodeGenerator: Code generated in 15.890235 ms
20/04/13 15:24:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0
20/04/13 15:24:48 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.6 MB)
20/04/13 15:24:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 14 ms
20/04/13 15:24:48 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 536.2 KB, free 1458.0 MB)
20/04/13 15:24:49 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1375 bytes result sent to driver
20/04/13 15:24:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3
20/04/13 15:24:49 INFO executor.Executor: Running task 2.0 in stage 1.0 (TID 3)
20/04/13 15:24:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6
20/04/13 15:24:49 INFO executor.Executor: Running task 5.0 in stage 1.0 (TID 6)
20/04/13 15:24:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3
20/04/13 15:24:49 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1458.6 MB)
20/04/13 15:24:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 12 ms
20/04/13 15:24:49 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.0 KB, free 1458.6 MB)
20/04/13 15:24:49 INFO codegen.CodeGenerator: Code generated in 12.491713 ms
20/04/13 15:24:50 INFO codegen.CodeGenerator: Code generated in 14.256342 ms
20/04/13 15:24:50 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:24:50 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:24:50 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2
20/04/13 15:24:50 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.5 MB)
20/04/13 15:24:50 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 14 ms
20/04/13 15:24:50 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 536.2 KB, free 1458.0 MB)
20/04/13 15:24:51 INFO executor.Executor: Finished task 5.0 in stage 1.0 (TID 6). 1640 bytes result sent to driver
20/04/13 15:24:51 INFO executor.Executor: Finished task 2.0 in stage 1.0 (TID 3). 1597 bytes result sent to driver
20/04/13 15:24:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9
20/04/13 15:24:56 INFO executor.Executor: Running task 2.0 in stage 2.0 (TID 9)
20/04/13 15:24:56 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12
20/04/13 15:24:56 INFO executor.Executor: Running task 5.0 in stage 2.0 (TID 12)
20/04/13 15:24:56 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5
20/04/13 15:24:56 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1458.0 MB)
20/04/13 15:24:56 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 26 ms
20/04/13 15:24:56 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 27.4 KB, free 1458.0 MB)
20/04/13 15:24:56 INFO codegen.CodeGenerator: Code generated in 15.01235 ms
20/04/13 15:24:56 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:24:56 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:24:56 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4
20/04/13 15:24:57 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1458.0 MB)
20/04/13 15:24:57 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 26 ms
20/04/13 15:24:57 INFO codegen.CodeGenerator: Code generated in 27.816313 ms
20/04/13 15:24:57 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 536.2 KB, free 1457.4 MB)
20/04/13 15:24:57 INFO codegen.CodeGenerator: Code generated in 25.154408 ms
20/04/13 15:24:57 INFO codegen.CodeGenerator: Code generated in 45.457131 ms
20/04/13 15:25:01 INFO python.PythonUDFRunner: Times: total = 811, boot = 293, init = 206, finish = 312
20/04/13 15:25:01 INFO executor.Executor: Finished task 5.0 in stage 2.0 (TID 12). 2463 bytes result sent to driver
20/04/13 15:25:05 INFO python.PythonUDFRunner: Times: total = 1973, boot = 289, init = 201, finish = 1483
20/04/13 15:25:05 INFO executor.Executor: Finished task 2.0 in stage 2.0 (TID 9). 2420 bytes result sent to driver
20/04/13 15:25:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14
20/04/13 15:25:13 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17
20/04/13 15:25:13 INFO executor.Executor: Running task 4.0 in stage 3.0 (TID 17)
20/04/13 15:25:13 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 14)
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
20/04/13 15:25:13 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6
20/04/13 15:25:13 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1976.0 B, free 1457.4 MB)
20/04/13 15:25:13 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 25 ms
20/04/13 15:25:13 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 3.3 KB, free 1457.4 MB)
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:25:13 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:25:13 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:25:13 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:25:13 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:36927 after 5 ms (0 ms spent in bootstraps)
20/04/13 15:25:13 INFO client.TransportClientFactory: Successfully created connection to /178.62.200.211:35537 after 7 ms (0 ms spent in bootstraps)
20/04/13 15:25:13 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 23 ms
20/04/13 15:25:13 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 23 ms
20/04/13 15:25:14 INFO memory.MemoryStore: Block rdd_21_1 stored as values in memory (estimated size 147.4 KB, free 1457.3 MB)
20/04/13 15:25:14 INFO memory.MemoryStore: Block rdd_21_4 stored as values in memory (estimated size 147.4 KB, free 1457.1 MB)
20/04/13 15:25:14 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 14). 1176 bytes result sent to driver
20/04/13 15:25:14 INFO executor.Executor: Finished task 4.0 in stage 3.0 (TID 17). 1176 bytes result sent to driver
20/04/13 15:25:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 21
20/04/13 15:25:14 INFO executor.Executor: Running task 1.0 in stage 5.0 (TID 21)
20/04/13 15:25:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 24
20/04/13 15:25:14 INFO executor.Executor: Running task 4.0 in stage 5.0 (TID 24)
20/04/13 15:25:14 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7
20/04/13 15:25:14 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1457.1 MB)
20/04/13 15:25:14 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 20 ms
20/04/13 15:25:14 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 4.2 KB, free 1457.1 MB)
20/04/13 15:25:14 INFO storage.BlockManager: Found block rdd_21_4 locally
20/04/13 15:25:14 INFO storage.BlockManager: Found block rdd_21_1 locally
20/04/13 15:25:14 INFO executor.Executor: Finished task 1.0 in stage 5.0 (TID 21). 40772 bytes result sent to driver
20/04/13 15:25:14 INFO executor.Executor: Finished task 4.0 in stage 5.0 (TID 24). 40923 bytes result sent to driver
20/04/13 15:25:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 26
20/04/13 15:25:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 29
20/04/13 15:25:15 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 26)
20/04/13 15:25:15 INFO executor.Executor: Running task 4.0 in stage 6.0 (TID 29)
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 124.8 KB, free 1457.0 MB)
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 19 ms
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 347.5 KB, free 1456.7 MB)
20/04/13 15:25:15 INFO codegen.CodeGenerator: Code generated in 42.01873 ms
20/04/13 15:25:15 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:25:15 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:25:15 INFO codegen.CodeGenerator: Code generated in 25.631222 ms
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.6 MB)
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 19 ms
20/04/13 15:25:15 INFO codegen.CodeGenerator: Code generated in 62.76173 ms
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 536.2 KB, free 1456.1 MB)
20/04/13 15:25:15 INFO codegen.CodeGenerator: Code generated in 55.649347 ms
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:15 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:15 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 77.2 KB, free 1392.0 MB)
20/04/13 15:25:15 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 28 ms
20/04/13 15:25:15 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 987.3 KB, free 1391.1 MB)
20/04/13 15:25:16 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:16 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:16 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:16 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:25:16 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:25:16 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:25:16 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:25:16 INFO compress.CodecPool: Got brand-new compressor [.snappy]
20/04/13 15:25:16 INFO compress.CodecPool: Got brand-new compressor [.snappy]
20/04/13 15:25:27 INFO python.PythonUDFRunner: Times: total = 4307, boot = -18034, init = 18231, finish = 4110
20/04/13 15:25:27 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8811881
20/04/13 15:25:27 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152515_0006_m_000004_29' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152515_0006_m_000004
20/04/13 15:25:27 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152515_0006_m_000004_29: Committed
20/04/13 15:25:27 INFO executor.Executor: Finished task 4.0 in stage 6.0 (TID 29). 3241 bytes result sent to driver
20/04/13 15:25:27 INFO python.PythonUDFRunner: Times: total = 3334, boot = -16901, init = 17093, finish = 3142
20/04/13 15:25:27 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 7707840
20/04/13 15:25:27 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152515_0006_m_000001_26' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152515_0006_m_000001
20/04/13 15:25:27 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152515_0006_m_000001_26: Committed
20/04/13 15:25:27 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 26). 3198 bytes result sent to driver
20/04/13 15:25:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 33
20/04/13 15:25:34 INFO executor.Executor: Running task 1.0 in stage 8.0 (TID 33)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14
20/04/13 15:25:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 36
20/04/13 15:25:34 INFO executor.Executor: Running task 4.0 in stage 8.0 (TID 36)
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1456.1 MB)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 23 ms
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 15.0 KB, free 1456.1 MB)
20/04/13 15:25:34 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:25:34 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.1 MB)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 14 ms
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 536.2 KB, free 1455.6 MB)
20/04/13 15:25:34 INFO executor.Executor: Finished task 1.0 in stage 8.0 (TID 33). 1554 bytes result sent to driver
20/04/13 15:25:34 INFO executor.Executor: Finished task 4.0 in stage 8.0 (TID 36). 1554 bytes result sent to driver
20/04/13 15:25:34 INFO storage.BlockManager: Removing RDD 21
20/04/13 15:25:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 40
20/04/13 15:25:34 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 43
20/04/13 15:25:34 INFO executor.Executor: Running task 2.0 in stage 9.0 (TID 40)
20/04/13 15:25:34 INFO executor.Executor: Running task 5.0 in stage 9.0 (TID 43)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1456.4 MB)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 15 ms
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 27.4 KB, free 1456.4 MB)
20/04/13 15:25:34 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15
20/04/13 15:25:34 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1456.4 MB)
20/04/13 15:25:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 22 ms
20/04/13 15:25:34 INFO codegen.CodeGenerator: Code generated in 35.828345 ms
20/04/13 15:25:34 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 536.2 KB, free 1455.8 MB)
20/04/13 15:25:37 INFO python.PythonUDFRunner: Times: total = 317, boot = -15099, init = 15172, finish = 244
20/04/13 15:25:37 INFO executor.Executor: Finished task 5.0 in stage 9.0 (TID 43). 2420 bytes result sent to driver
20/04/13 15:25:41 INFO python.PythonUDFRunner: Times: total = 536, boot = -16067, init = 16127, finish = 476
20/04/13 15:25:41 INFO executor.Executor: Finished task 2.0 in stage 9.0 (TID 40). 2420 bytes result sent to driver
20/04/13 15:25:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 46
20/04/13 15:25:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 49
20/04/13 15:25:48 INFO executor.Executor: Running task 5.0 in stage 10.0 (TID 49)
20/04/13 15:25:48 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
20/04/13 15:25:48 INFO executor.Executor: Running task 2.0 in stage 10.0 (TID 46)
20/04/13 15:25:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17
20/04/13 15:25:48 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 1976.0 B, free 1455.8 MB)
20/04/13 15:25:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 9 ms
20/04/13 15:25:48 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 3.3 KB, free 1455.8 MB)
20/04/13 15:25:48 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
20/04/13 15:25:48 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:25:49 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
20/04/13 15:25:49 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:25:49 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:25:49 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:25:49 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 15:25:49 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 3 ms
20/04/13 15:25:49 INFO memory.MemoryStore: Block rdd_51_5 stored as values in memory (estimated size 147.2 KB, free 1455.7 MB)
20/04/13 15:25:49 INFO executor.Executor: Finished task 5.0 in stage 10.0 (TID 49). 1176 bytes result sent to driver
20/04/13 15:25:49 INFO memory.MemoryStore: Block rdd_51_2 stored as values in memory (estimated size 146.8 KB, free 1455.5 MB)
20/04/13 15:25:49 INFO executor.Executor: Finished task 2.0 in stage 10.0 (TID 46). 1176 bytes result sent to driver
20/04/13 15:25:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 50
20/04/13 15:25:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 53
20/04/13 15:25:49 INFO executor.Executor: Running task 2.0 in stage 12.0 (TID 50)
20/04/13 15:25:49 INFO executor.Executor: Running task 5.0 in stage 12.0 (TID 53)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1455.5 MB)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 13 ms
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 4.2 KB, free 1455.5 MB)
20/04/13 15:25:49 INFO storage.BlockManager: Found block rdd_51_2 locally
20/04/13 15:25:49 INFO storage.BlockManager: Found block rdd_51_5 locally
20/04/13 15:25:49 INFO executor.Executor: Finished task 2.0 in stage 12.0 (TID 50). 40807 bytes result sent to driver
20/04/13 15:25:49 INFO executor.Executor: Finished task 5.0 in stage 12.0 (TID 53). 41280 bytes result sent to driver
20/04/13 15:25:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 58
20/04/13 15:25:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 61
20/04/13 15:25:49 INFO executor.Executor: Running task 2.0 in stage 13.0 (TID 58)
20/04/13 15:25:49 INFO executor.Executor: Running task 5.0 in stage 13.0 (TID 61)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 124.7 KB, free 1455.4 MB)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 16 ms
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 347.5 KB, free 1455.1 MB)
20/04/13 15:25:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:25:49 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:25:49 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1455.0 MB)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 25 ms
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 536.2 KB, free 1454.5 MB)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 77.2 KB, free 1390.4 MB)
20/04/13 15:25:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 25 ms
20/04/13 15:25:49 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 987.3 KB, free 1389.5 MB)
20/04/13 15:25:49 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:49 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:25:49 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:49 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:25:49 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:25:49 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:25:49 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 8502
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:25:54 INFO python.PythonUDFRunner: Times: total = 270, boot = -14492, init = 14565, finish = 197
20/04/13 15:25:54 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 3465941
20/04/13 15:25:54 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152549_0013_m_000005_61' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152549_0013_m_000005
20/04/13 15:25:54 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152549_0013_m_000005_61: Committed
20/04/13 15:25:54 INFO executor.Executor: Finished task 5.0 in stage 13.0 (TID 61). 3198 bytes result sent to driver
20/04/13 15:25:59 INFO python.PythonUDFRunner: Times: total = 428, boot = -14280, init = 14329, finish = 379
20/04/13 15:25:59 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9210790
20/04/13 15:25:59 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152549_0013_m_000002_58' to hdfs://178.62.208.209:9000/data/df_3-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152549_0013_m_000002
20/04/13 15:25:59 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152549_0013_m_000002_58: Committed
20/04/13 15:25:59 INFO executor.Executor: Finished task 2.0 in stage 13.0 (TID 58). 3198 bytes result sent to driver
20/04/13 15:26:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 63
20/04/13 15:26:07 INFO executor.Executor: Running task 0.0 in stage 15.0 (TID 63)
20/04/13 15:26:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 66
20/04/13 15:26:07 INFO executor.Executor: Running task 3.0 in stage 15.0 (TID 66)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1453.5 MB)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 13 ms
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 15.0 KB, free 1453.5 MB)
20/04/13 15:26:07 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1453.4 MB)
20/04/13 15:26:07 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 13 ms
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 536.2 KB, free 1452.9 MB)
20/04/13 15:26:07 INFO executor.Executor: Finished task 0.0 in stage 15.0 (TID 63). 1554 bytes result sent to driver
20/04/13 15:26:07 INFO executor.Executor: Finished task 3.0 in stage 15.0 (TID 66). 1554 bytes result sent to driver
20/04/13 15:26:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 69
20/04/13 15:26:07 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 72
20/04/13 15:26:07 INFO executor.Executor: Running task 0.0 in stage 16.0 (TID 69)
20/04/13 15:26:07 INFO executor.Executor: Running task 3.0 in stage 16.0 (TID 72)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1452.9 MB)
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 9 ms
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 27.4 KB, free 1452.9 MB)
20/04/13 15:26:07 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1452.8 MB)
20/04/13 15:26:07 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:26:07 INFO broadcast.TorrentBroadcast: Reading broadcast variable 26 took 22 ms
20/04/13 15:26:07 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 536.2 KB, free 1452.3 MB)
20/04/13 15:26:08 INFO codegen.CodeGenerator: Code generated in 44.288078 ms
20/04/13 15:26:17 INFO python.PythonUDFRunner: Times: total = 3567, boot = -17919, init = 17978, finish = 3508
20/04/13 15:26:17 INFO executor.Executor: Finished task 3.0 in stage 16.0 (TID 72). 2420 bytes result sent to driver
20/04/13 15:26:18 INFO python.PythonUDFRunner: Times: total = 1967, boot = -17761, init = 17827, finish = 1901
20/04/13 15:26:18 INFO executor.Executor: Finished task 0.0 in stage 16.0 (TID 69). 2420 bytes result sent to driver
20/04/13 15:26:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 77
20/04/13 15:26:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 80
20/04/13 15:26:24 INFO executor.Executor: Running task 2.0 in stage 17.0 (TID 77)
20/04/13 15:26:24 INFO executor.Executor: Running task 5.0 in stage 17.0 (TID 80)
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache
20/04/13 15:26:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 28
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 1977.0 B, free 1452.3 MB)
20/04/13 15:26:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 28 took 9 ms
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 3.3 KB, free 1452.3 MB)
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:26:24 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:26:24 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:26:24 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 15:26:24 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:26:24 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 3 ms
20/04/13 15:26:24 INFO memory.MemoryStore: Block rdd_81_2 stored as values in memory (estimated size 2.6 MB, free 1449.7 MB)
20/04/13 15:26:24 INFO memory.MemoryStore: Block rdd_81_5 stored as values in memory (estimated size 2.6 MB, free 1447.1 MB)
20/04/13 15:26:24 INFO executor.Executor: Finished task 2.0 in stage 17.0 (TID 77). 1176 bytes result sent to driver
20/04/13 15:26:24 INFO executor.Executor: Finished task 5.0 in stage 17.0 (TID 80). 1176 bytes result sent to driver
20/04/13 15:26:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 81
20/04/13 15:26:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 84
20/04/13 15:26:24 INFO executor.Executor: Running task 2.0 in stage 19.0 (TID 81)
20/04/13 15:26:24 INFO executor.Executor: Running task 5.0 in stage 19.0 (TID 84)
20/04/13 15:26:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 29
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1447.1 MB)
20/04/13 15:26:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 29 took 10 ms
20/04/13 15:26:24 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 4.2 KB, free 1447.1 MB)
20/04/13 15:26:24 INFO storage.BlockManager: Found block rdd_81_5 locally
20/04/13 15:26:24 INFO storage.BlockManager: Found block rdd_81_2 locally
20/04/13 15:26:25 INFO executor.Executor: Finished task 5.0 in stage 19.0 (TID 84). 675274 bytes result sent to driver
20/04/13 15:26:25 INFO executor.Executor: Finished task 2.0 in stage 19.0 (TID 81). 677986 bytes result sent to driver
20/04/13 15:26:25 INFO storage.BlockManager: Removing RDD 51
20/04/13 15:26:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 88
20/04/13 15:26:26 INFO executor.Executor: Running task 1.0 in stage 20.0 (TID 88)
20/04/13 15:26:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 91
20/04/13 15:26:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 32
20/04/13 15:26:26 INFO executor.Executor: Running task 4.0 in stage 20.0 (TID 91)
20/04/13 15:26:26 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 1062.5 KB, free 1450.2 MB)
20/04/13 15:26:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 32 took 12 ms
20/04/13 15:26:26 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 1883.3 KB, free 1448.3 MB)
20/04/13 15:26:27 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:26:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 31
20/04/13 15:26:27 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1448.3 MB)
20/04/13 15:26:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:27 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:27 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 31 took 8 ms
20/04/13 15:26:27 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 536.2 KB, free 1447.8 MB)
20/04/13 15:26:27 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:26:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:27 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:27 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:26:27 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:26:27 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:26:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 30
20/04/13 15:26:27 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 1770.3 KB, free 1382.1 MB)
20/04/13 15:26:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 30 took 22 ms
20/04/13 15:26:28 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 18.8 MB, free 1363.2 MB)
20/04/13 15:26:28 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:26:28 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:26:28 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:26:28 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:26:28 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:26:28 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 164062
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:26:28 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 164062
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:26:39 INFO python.PythonUDFRunner: Times: total = 3147, boot = -15524, init = 15574, finish = 3097
20/04/13 15:26:39 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12131844
20/04/13 15:26:39 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152626_0020_m_000004_91' to hdfs://178.62.208.209:9000/data/df_4-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152626_0020_m_000004
20/04/13 15:26:39 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152626_0020_m_000004_91: Committed
20/04/13 15:26:39 INFO executor.Executor: Finished task 4.0 in stage 20.0 (TID 91). 3198 bytes result sent to driver
20/04/13 15:26:40 INFO python.PythonUDFRunner: Times: total = 3265, boot = -17151, init = 17173, finish = 3243
20/04/13 15:26:40 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12307986
20/04/13 15:26:40 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152626_0020_m_000001_88' to hdfs://178.62.208.209:9000/data/df_4-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152626_0020_m_000001
20/04/13 15:26:40 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152626_0020_m_000001_88: Committed
20/04/13 15:26:40 INFO executor.Executor: Finished task 1.0 in stage 20.0 (TID 88). 3198 bytes result sent to driver
20/04/13 15:26:46 INFO storage.BlockManager: Removing RDD 81
20/04/13 15:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 95
20/04/13 15:26:46 INFO executor.Executor: Running task 1.0 in stage 22.0 (TID 95)
20/04/13 15:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 98
20/04/13 15:26:46 INFO executor.Executor: Running task 4.0 in stage 22.0 (TID 98)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 36
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1436.4 MB)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 36 took 8 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 15.0 KB, free 1436.4 MB)
20/04/13 15:26:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:26:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 35
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1436.4 MB)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 35 took 7 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 536.2 KB, free 1435.9 MB)
20/04/13 15:26:46 INFO executor.Executor: Finished task 4.0 in stage 22.0 (TID 98). 1554 bytes result sent to driver
20/04/13 15:26:46 INFO executor.Executor: Finished task 1.0 in stage 22.0 (TID 95). 1554 bytes result sent to driver
20/04/13 15:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 102
20/04/13 15:26:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 105
20/04/13 15:26:46 INFO executor.Executor: Running task 2.0 in stage 23.0 (TID 102)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 38
20/04/13 15:26:46 INFO executor.Executor: Running task 5.0 in stage 23.0 (TID 105)
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1435.9 MB)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 38 took 10 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 27.4 KB, free 1435.8 MB)
20/04/13 15:26:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 37
20/04/13 15:26:46 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1435.8 MB)
20/04/13 15:26:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 37 took 26 ms
20/04/13 15:26:46 INFO codegen.CodeGenerator: Code generated in 30.172884 ms
20/04/13 15:26:46 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 536.2 KB, free 1435.3 MB)
20/04/13 15:26:51 INFO python.PythonUDFRunner: Times: total = 236, boot = -16496, init = 16568, finish = 164
20/04/13 15:26:51 INFO executor.Executor: Finished task 5.0 in stage 23.0 (TID 105). 2420 bytes result sent to driver
20/04/13 15:26:57 INFO python.PythonUDFRunner: Times: total = 2443, boot = -16648, init = 16724, finish = 2367
20/04/13 15:26:57 INFO executor.Executor: Finished task 2.0 in stage 23.0 (TID 102). 2420 bytes result sent to driver
20/04/13 15:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 107
20/04/13 15:27:03 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 110
20/04/13 15:27:03 INFO executor.Executor: Running task 4.0 in stage 24.0 (TID 110)
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache
20/04/13 15:27:03 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 39
20/04/13 15:27:03 INFO executor.Executor: Running task 1.0 in stage 24.0 (TID 107)
20/04/13 15:27:03 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 1977.0 B, free 1435.3 MB)
20/04/13 15:27:03 INFO broadcast.TorrentBroadcast: Reading broadcast variable 39 took 8 ms
20/04/13 15:27:03 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 3.3 KB, free 1435.3 MB)
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:27:03 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:27:03 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:27:03 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 15:27:03 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:27:03 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 1 ms
20/04/13 15:27:04 INFO memory.MemoryStore: Block rdd_111_4 stored as values in memory (estimated size 2.6 MB, free 1432.6 MB)
20/04/13 15:27:04 INFO executor.Executor: Finished task 4.0 in stage 24.0 (TID 110). 1176 bytes result sent to driver
20/04/13 15:27:04 INFO memory.MemoryStore: Block rdd_111_1 stored as values in memory (estimated size 2.6 MB, free 1430.0 MB)
20/04/13 15:27:04 INFO executor.Executor: Finished task 1.0 in stage 24.0 (TID 107). 1176 bytes result sent to driver
20/04/13 15:27:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 112
20/04/13 15:27:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 115
20/04/13 15:27:04 INFO executor.Executor: Running task 1.0 in stage 26.0 (TID 112)
20/04/13 15:27:04 INFO executor.Executor: Running task 4.0 in stage 26.0 (TID 115)
20/04/13 15:27:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 40
20/04/13 15:27:04 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1430.0 MB)
20/04/13 15:27:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 40 took 9 ms
20/04/13 15:27:04 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 4.2 KB, free 1430.0 MB)
20/04/13 15:27:04 INFO storage.BlockManager: Found block rdd_111_4 locally
20/04/13 15:27:04 INFO storage.BlockManager: Found block rdd_111_1 locally
20/04/13 15:27:04 INFO executor.Executor: Finished task 1.0 in stage 26.0 (TID 112). 677902 bytes result sent to driver
20/04/13 15:27:04 INFO executor.Executor: Finished task 4.0 in stage 26.0 (TID 115). 677387 bytes result sent to driver
20/04/13 15:27:06 INFO storage.BlockManager: Removing RDD 111
20/04/13 15:27:06 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 119
20/04/13 15:27:06 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 122
20/04/13 15:27:06 INFO executor.Executor: Running task 1.0 in stage 27.0 (TID 119)
20/04/13 15:27:06 INFO executor.Executor: Running task 4.0 in stage 27.0 (TID 122)
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 43
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 1062.2 KB, free 1456.0 MB)
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Reading broadcast variable 43 took 12 ms
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 1883.3 KB, free 1454.1 MB)
20/04/13 15:27:06 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 42
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1454.1 MB)
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Reading broadcast variable 42 took 13 ms
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:27:06 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:27:06 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 536.2 KB, free 1453.6 MB)
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 41
20/04/13 15:27:06 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 1770.4 KB, free 1387.8 MB)
20/04/13 15:27:06 INFO broadcast.TorrentBroadcast: Reading broadcast variable 41 took 12 ms
20/04/13 15:27:07 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 18.8 MB, free 1369.0 MB)
20/04/13 15:27:07 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:27:07 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:27:07 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:27:07 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:27:07 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:27:07 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 164062
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:27:07 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 164062
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:27:19 INFO python.PythonUDFRunner: Times: total = 391, boot = -19454, init = 19495, finish = 350
20/04/13 15:27:19 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12316404
20/04/13 15:27:19 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152706_0027_m_000004_122' to hdfs://178.62.208.209:9000/data/df_4-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152706_0027_m_000004
20/04/13 15:27:19 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152706_0027_m_000004_122: Committed
20/04/13 15:27:19 INFO executor.Executor: Finished task 4.0 in stage 27.0 (TID 122). 3198 bytes result sent to driver
20/04/13 15:27:19 INFO python.PythonUDFRunner: Times: total = 3188, boot = -17255, init = 17288, finish = 3155
20/04/13 15:27:19 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 12561524
20/04/13 15:27:19 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152706_0027_m_000001_119' to hdfs://178.62.208.209:9000/data/df_4-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152706_0027_m_000001
20/04/13 15:27:19 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152706_0027_m_000001_119: Committed
20/04/13 15:27:19 INFO executor.Executor: Finished task 1.0 in stage 27.0 (TID 119). 3198 bytes result sent to driver
20/04/13 15:27:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 126
20/04/13 15:27:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 129
20/04/13 15:27:26 INFO executor.Executor: Running task 1.0 in stage 29.0 (TID 126)
20/04/13 15:27:26 INFO executor.Executor: Running task 4.0 in stage 29.0 (TID 129)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 47
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1433.0 MB)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 47 took 5 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 15.0 KB, free 1433.0 MB)
20/04/13 15:27:26 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 46
20/04/13 15:27:26 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1433.0 MB)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 46 took 5 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 536.2 KB, free 1432.4 MB)
20/04/13 15:27:26 INFO executor.Executor: Finished task 1.0 in stage 29.0 (TID 126). 1597 bytes result sent to driver
20/04/13 15:27:26 INFO executor.Executor: Finished task 4.0 in stage 29.0 (TID 129). 1597 bytes result sent to driver
20/04/13 15:27:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 131
20/04/13 15:27:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 134
20/04/13 15:27:26 INFO executor.Executor: Running task 0.0 in stage 30.0 (TID 131)
20/04/13 15:27:26 INFO executor.Executor: Running task 3.0 in stage 30.0 (TID 134)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 49
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1432.4 MB)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 49 took 5 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 27.4 KB, free 1432.4 MB)
20/04/13 15:27:26 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 48
20/04/13 15:27:26 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1432.4 MB)
20/04/13 15:27:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 48 took 13 ms
20/04/13 15:27:26 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 536.2 KB, free 1431.8 MB)
20/04/13 15:27:26 INFO codegen.CodeGenerator: Code generated in 38.175193 ms
20/04/13 15:27:44 INFO python.PythonUDFRunner: Times: total = 3672, boot = -17023, init = 17072, finish = 3623
20/04/13 15:27:45 INFO python.PythonUDFRunner: Times: total = 2523, boot = -19816, init = 19895, finish = 2444
20/04/13 15:27:47 INFO executor.Executor: Finished task 3.0 in stage 30.0 (TID 134). 2420 bytes result sent to driver
20/04/13 15:27:48 INFO executor.Executor: Finished task 0.0 in stage 30.0 (TID 131). 2420 bytes result sent to driver
20/04/13 15:27:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 139
20/04/13 15:27:55 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 142
20/04/13 15:27:55 INFO executor.Executor: Running task 2.0 in stage 31.0 (TID 139)
20/04/13 15:27:55 INFO executor.Executor: Running task 5.0 in stage 31.0 (TID 142)
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache
20/04/13 15:27:55 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 50
20/04/13 15:27:55 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 1977.0 B, free 1431.8 MB)
20/04/13 15:27:55 INFO broadcast.TorrentBroadcast: Reading broadcast variable 50 took 8 ms
20/04/13 15:27:55 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 3.3 KB, free 1431.8 MB)
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them
20/04/13 15:27:55 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:27:55 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:27:55 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 2 ms
20/04/13 15:27:55 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:27:55 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 5 ms
20/04/13 15:27:57 INFO memory.MemoryStore: Block rdd_141_5 stored as values in memory (estimated size 35.8 MB, free 1357.2 MB)
20/04/13 15:27:57 INFO executor.Executor: Finished task 5.0 in stage 31.0 (TID 142). 1219 bytes result sent to driver
20/04/13 15:27:57 INFO memory.MemoryStore: Block rdd_141_2 stored as values in memory (estimated size 35.9 MB, free 1360.1 MB)
20/04/13 15:27:57 INFO executor.Executor: Finished task 2.0 in stage 31.0 (TID 139). 1219 bytes result sent to driver
20/04/13 15:27:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 143
20/04/13 15:27:58 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 146
20/04/13 15:27:58 INFO executor.Executor: Running task 2.0 in stage 33.0 (TID 143)
20/04/13 15:27:58 INFO executor.Executor: Running task 5.0 in stage 33.0 (TID 146)
20/04/13 15:27:58 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 51
20/04/13 15:27:58 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1360.1 MB)
20/04/13 15:27:58 INFO broadcast.TorrentBroadcast: Reading broadcast variable 51 took 7 ms
20/04/13 15:27:58 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 4.2 KB, free 1360.1 MB)
20/04/13 15:27:58 INFO storage.BlockManager: Found block rdd_141_5 locally
20/04/13 15:27:58 INFO storage.BlockManager: Found block rdd_141_2 locally
20/04/13 15:27:59 INFO memory.MemoryStore: Block taskresult_143 stored as bytes in memory (estimated size 5.8 MB, free 1354.3 MB)
20/04/13 15:27:59 INFO executor.Executor: Finished task 2.0 in stage 33.0 (TID 143). 6067337 bytes result sent via BlockManager)
20/04/13 15:27:59 INFO memory.MemoryStore: Block taskresult_146 stored as bytes in memory (estimated size 5.8 MB, free 1348.5 MB)
20/04/13 15:27:59 INFO executor.Executor: Finished task 5.0 in stage 33.0 (TID 146). 6068098 bytes result sent via BlockManager)
20/04/13 15:28:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 151
20/04/13 15:28:04 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 154
20/04/13 15:28:04 INFO executor.Executor: Running task 2.0 in stage 34.0 (TID 151)
20/04/13 15:28:04 INFO executor.Executor: Running task 5.0 in stage 34.0 (TID 154)
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 54
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 2.2 MB, free 1362.6 MB)
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 54 took 25 ms
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_54 stored as values in memory (estimated size 3.3 MB, free 1359.3 MB)
20/04/13 15:28:04 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 13087342-19631013, partition values: [empty row]
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 32718355-35067723, partition values: [empty row]
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:04 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:04 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 53
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1359.3 MB)
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 53 took 8 ms
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_53 stored as values in memory (estimated size 536.2 KB, free 1358.7 MB)
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 52
20/04/13 15:28:04 INFO memory.MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 3.2 MB, free 1291.5 MB)
20/04/13 15:28:04 INFO broadcast.TorrentBroadcast: Reading broadcast variable 52 took 20 ms
20/04/13 15:28:05 INFO memory.MemoryStore: Block broadcast_52 stored as values in memory (estimated size 31.7 MB, free 1259.8 MB)
20/04/13 15:28:05 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:05 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:28:05 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 262144
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:28:05 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:05 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:28:05 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:28:05 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 262144
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:28:10 INFO python.PythonUDFRunner: Times: total = 161, boot = -35447, init = 35488, finish = 120
20/04/13 15:28:10 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 3687942
20/04/13 15:28:10 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152804_0034_m_000005_154' to hdfs://178.62.208.209:9000/data/df_5-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152804_0034_m_000005
20/04/13 15:28:10 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152804_0034_m_000005_154: Committed
20/04/13 15:28:10 INFO executor.Executor: Finished task 5.0 in stage 34.0 (TID 154). 3241 bytes result sent to driver
20/04/13 15:28:15 INFO python.PythonUDFRunner: Times: total = 3716, boot = -34281, init = 34332, finish = 3665
20/04/13 15:28:15 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9242659
20/04/13 15:28:15 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152804_0034_m_000002_151' to hdfs://178.62.208.209:9000/data/df_5-shingles_sparse-binary-vectors.parquet/_temporary/0/task_20200413152804_0034_m_000002
20/04/13 15:28:15 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152804_0034_m_000002_151: Committed
20/04/13 15:28:15 INFO executor.Executor: Finished task 2.0 in stage 34.0 (TID 151). 3198 bytes result sent to driver
20/04/13 15:28:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 155
20/04/13 15:28:23 INFO executor.Executor: Running task 0.0 in stage 35.0 (TID 155)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 56
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 4.6 KB, free 1323.8 MB)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 56 took 7 ms
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_56 stored as values in memory (estimated size 8.9 KB, free 1323.8 MB)
20/04/13 15:28:23 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 55
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1323.7 MB)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 55 took 5 ms
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_55 stored as values in memory (estimated size 536.2 KB, free 1323.2 MB)
20/04/13 15:28:23 INFO executor.Executor: Finished task 0.0 in stage 35.0 (TID 155). 1332 bytes result sent to driver
20/04/13 15:28:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 156
20/04/13 15:28:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 159
20/04/13 15:28:23 INFO executor.Executor: Running task 3.0 in stage 36.0 (TID 159)
20/04/13 15:28:23 INFO executor.Executor: Running task 0.0 in stage 36.0 (TID 156)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 58
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 8.0 KB, free 1323.2 MB)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 58 took 7 ms
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_58 stored as values in memory (estimated size 15.0 KB, free 1323.2 MB)
20/04/13 15:28:23 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 57
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1323.2 MB)
20/04/13 15:28:23 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 57 took 7 ms
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_57 stored as values in memory (estimated size 536.2 KB, free 1322.6 MB)
20/04/13 15:28:23 INFO executor.Executor: Finished task 0.0 in stage 36.0 (TID 156). 1554 bytes result sent to driver
20/04/13 15:28:23 INFO executor.Executor: Finished task 3.0 in stage 36.0 (TID 159). 1554 bytes result sent to driver
20/04/13 15:28:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 163
20/04/13 15:28:23 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 166
20/04/13 15:28:23 INFO executor.Executor: Running task 1.0 in stage 37.0 (TID 163)
20/04/13 15:28:23 INFO executor.Executor: Running task 4.0 in stage 37.0 (TID 166)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 60
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 13.4 KB, free 1322.6 MB)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 60 took 10 ms
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_60 stored as values in memory (estimated size 27.4 KB, free 1322.6 MB)
20/04/13 15:28:23 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 6543671-13087342, partition values: [empty row]
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 59
20/04/13 15:28:23 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 26174684-32718355, partition values: [empty row]
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1322.6 MB)
20/04/13 15:28:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 59 took 15 ms
20/04/13 15:28:23 INFO memory.MemoryStore: Block broadcast_59 stored as values in memory (estimated size 536.2 KB, free 1322.1 MB)
20/04/13 15:28:23 INFO codegen.CodeGenerator: Code generated in 29.534939 ms
20/04/13 15:28:38 INFO python.PythonUDFRunner: Times: total = 2118, boot = -15278, init = 15319, finish = 2077
20/04/13 15:28:39 INFO python.PythonUDFRunner: Times: total = 2074, boot = -18815, init = 18885, finish = 2004
20/04/13 15:28:44 INFO executor.Executor: Finished task 4.0 in stage 37.0 (TID 166). 2420 bytes result sent to driver
20/04/13 15:28:45 INFO executor.Executor: Finished task 1.0 in stage 37.0 (TID 163). 2420 bytes result sent to driver
20/04/13 15:28:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 168
20/04/13 15:28:49 INFO executor.Executor: Running task 0.0 in stage 38.0 (TID 168)
20/04/13 15:28:49 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 171
20/04/13 15:28:49 INFO executor.Executor: Running task 3.0 in stage 38.0 (TID 171)
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache
20/04/13 15:28:49 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 61
20/04/13 15:28:49 INFO memory.MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 1975.0 B, free 1322.0 MB)
20/04/13 15:28:49 INFO broadcast.TorrentBroadcast: Reading broadcast variable 61 took 18 ms
20/04/13 15:28:49 INFO memory.MemoryStore: Block broadcast_61 stored as values in memory (estimated size 3.3 KB, free 1322.0 MB)
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@178.62.210.13:39481)
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them
20/04/13 15:28:49 INFO spark.MapOutputTrackerWorker: Got the output locations
20/04/13 15:28:49 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:28:49 INFO storage.ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 2 local blocks and 4 remote blocks
20/04/13 15:28:49 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 0 ms
20/04/13 15:28:49 INFO storage.ShuffleBlockFetcherIterator: Started 2 remote fetches in 10 ms
20/04/13 15:28:51 INFO memory.MemoryStore: Block rdd_171_3 stored as values in memory (estimated size 36.0 MB, free 1209.8 MB)
20/04/13 15:28:51 INFO executor.Executor: Finished task 3.0 in stage 38.0 (TID 171). 1219 bytes result sent to driver
20/04/13 15:28:51 INFO memory.MemoryStore: Block rdd_171_0 stored as values in memory (estimated size 35.9 MB, free 1250.1 MB)
20/04/13 15:28:51 INFO executor.Executor: Finished task 0.0 in stage 38.0 (TID 168). 1219 bytes result sent to driver
20/04/13 15:28:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 176
20/04/13 15:28:54 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 179
20/04/13 15:28:54 INFO executor.Executor: Running task 3.0 in stage 40.0 (TID 179)
20/04/13 15:28:54 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 62
20/04/13 15:28:54 INFO executor.Executor: Running task 0.0 in stage 40.0 (TID 176)
20/04/13 15:28:54 INFO memory.MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1250.1 MB)
20/04/13 15:28:54 INFO broadcast.TorrentBroadcast: Reading broadcast variable 62 took 6 ms
20/04/13 15:28:54 INFO memory.MemoryStore: Block broadcast_62 stored as values in memory (estimated size 4.2 KB, free 1250.1 MB)
20/04/13 15:28:54 INFO storage.BlockManager: Found block rdd_171_3 locally
20/04/13 15:28:54 INFO storage.BlockManager: Found block rdd_171_0 locally
20/04/13 15:28:54 INFO memory.MemoryStore: Block taskresult_176 stored as bytes in memory (estimated size 5.8 MB, free 1244.3 MB)
20/04/13 15:28:54 INFO memory.MemoryStore: Block taskresult_179 stored as bytes in memory (estimated size 5.8 MB, free 1238.6 MB)
20/04/13 15:28:54 INFO executor.Executor: Finished task 0.0 in stage 40.0 (TID 176). 6067753 bytes result sent via BlockManager)
20/04/13 15:28:54 INFO executor.Executor: Finished task 3.0 in stage 40.0 (TID 179). 6068046 bytes result sent via BlockManager)
20/04/13 15:28:56 INFO storage.BlockManager: Removing RDD 141
20/04/13 15:28:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 180
20/04/13 15:28:59 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 183
20/04/13 15:28:59 INFO executor.Executor: Running task 0.0 in stage 41.0 (TID 180)
20/04/13 15:28:59 INFO executor.Executor: Running task 3.0 in stage 41.0 (TID 183)
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 65
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 2.2 MB, free 1384.0 MB)
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 65 took 9 ms
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_65 stored as values in memory (estimated size 3.3 MB, free 1380.7 MB)
20/04/13 15:28:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 0-6543671, partition values: [empty row]
20/04/13 15:28:59 INFO datasources.FileScanRDD: Reading File path: hdfs://178.62.208.209:9000/data/uniprot-proteome_UP000005640.tab, range: 19631013-26174684, partition values: [empty row]
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 64
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 31.5 KB, free 1380.7 MB)
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 64 took 10 ms
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/04/13 15:28:59 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
20/04/13 15:28:59 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
20/04/13 15:28:59 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:59 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:28:59 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:28:59 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:28:59 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:28:59 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:28:59 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:28:59 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:28:59 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:28:59 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:28:59 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:28:59 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:28:59 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 262144
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_64 stored as values in memory (estimated size 536.2 KB, free 1380.1 MB)
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 63
20/04/13 15:28:59 INFO memory.MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 3.3 MB, free 1312.9 MB)
20/04/13 15:28:59 INFO broadcast.TorrentBroadcast: Reading broadcast variable 63 took 16 ms
20/04/13 15:29:00 INFO memory.MemoryStore: Block broadcast_63 stored as values in memory (estimated size 31.7 MB, free 1281.2 MB)
20/04/13 15:29:00 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:29:00 INFO codec.CodecConfig: Compression: SNAPPY
20/04/13 15:29:00 INFO hadoop.ParquetOutputFormat: Parquet block size to 134217728
20/04/13 15:29:00 INFO hadoop.ParquetOutputFormat: Parquet page size to 1048576
20/04/13 15:29:00 INFO hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
20/04/13 15:29:00 INFO hadoop.ParquetOutputFormat: Dictionary is on
20/04/13 15:29:00 INFO hadoop.ParquetOutputFormat: Validation is off
20/04/13 15:29:00 INFO hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
20/04/13 15:29:00 INFO hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
20/04/13 15:29:00 INFO hadoop.ParquetOutputFormat: Page size checking is: estimated
20/04/13 15:29:00 INFO hadoop.ParquetOutputFormat: Min row count for page size check is: 100
20/04/13 15:29:00 INFO hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
20/04/13 15:29:00 INFO parquet.ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "entry",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "entry_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "features",
    "type" : {
      "type" : "udt",
      "class" : "org.apache.spark.ml.linalg.VectorUDT",
      "pyClass" : "pyspark.ml.linalg.VectorUDT",
      "sqlType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "type",
          "type" : "byte",
          "nullable" : false,
          "metadata" : { }
        }, {
          "name" : "size",
          "type" : "integer",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "indices",
          "type" : {
            "type" : "array",
            "elementType" : "integer",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "values",
          "type" : {
            "type" : "array",
            "elementType" : "double",
            "containsNull" : false
          },
          "nullable" : true,
          "metadata" : { }
        } ]
      }
    },
    "nullable" : true,
    "metadata" : {
      "ml_attr" : {
        "attrs" : { },
        "num_attrs" : 262144
      }
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary entry (UTF8);
  optional binary entry_name (UTF8);
  optional group features {
    required int32 type (INT_8);
    optional int32 size;
    optional group indices (LIST) {
      repeated group list {
        required int32 element;
      }
    }
    optional group values (LIST) {
      repeated group list {
        required double element;
      }
    }
  }
}

       
20/04/13 15:29:11 INFO python.PythonUDFRunner: Times: total = 389, boot = -33848, init = 33886, finish = 351
20/04/13 15:29:11 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9750809
20/04/13 15:29:11 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152859_0041_m_000003_183' to hdfs://178.62.208.209:9000/data/df_5-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152859_0041_m_000003
20/04/13 15:29:11 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152859_0041_m_000003_183: Committed
20/04/13 15:29:11 INFO executor.Executor: Finished task 3.0 in stage 41.0 (TID 183). 3198 bytes result sent to driver
20/04/13 15:29:11 INFO python.PythonUDFRunner: Times: total = 2962, boot = -33794, init = 33832, finish = 2924
20/04/13 15:29:11 INFO hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8592885
20/04/13 15:29:11 INFO output.FileOutputCommitter: Saved output of task 'attempt_20200413152859_0041_m_000000_180' to hdfs://178.62.208.209:9000/data/df_5-shingles_sparse-count-vectors.parquet/_temporary/0/task_20200413152859_0041_m_000000
20/04/13 15:29:11 INFO mapred.SparkHadoopMapRedUtil: attempt_20200413152859_0041_m_000000_180: Committed
20/04/13 15:29:11 INFO executor.Executor: Finished task 0.0 in stage 41.0 (TID 180). 3198 bytes result sent to driver
20/04/13 15:29:15 INFO executor.CoarseGrainedExecutorBackend: Driver commanded a shutdown
20/04/13 15:29:15 INFO memory.MemoryStore: MemoryStore cleared
20/04/13 15:29:15 INFO storage.BlockManager: BlockManager stopped
20/04/13 15:29:15 INFO util.ShutdownHookManager: Shutdown hook called
20/04/13 15:29:15 INFO util.ShutdownHookManager: Deleting directory /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1586780871303_0007/spark-fccd5a0d-9ef2-4368-8b29-513540ec04cd
End of LogType:stderr

LogType:stdout
Log Upload Time:Mon Apr 13 15:29:17 +0000 2020
LogLength:0
Log Contents:
End of LogType:stdout

